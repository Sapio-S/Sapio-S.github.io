<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.2.1" /><meta property="og:title" content="从围棋到矩阵乘" /><meta name="author" content="Sapio-S" /><meta property="og:locale" content="en" /><meta name="description" content="算法发展" /><meta property="og:description" content="算法发展" /><link rel="canonical" href="https://sapio-s.github.io/posts/AlphaGo/" /><meta property="og:url" content="https://sapio-s.github.io/posts/AlphaGo/" /><meta property="og:site_name" content="Sapio-S" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-10-20T00:00:00+08:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="从围棋到矩阵乘" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"author":{"@type":"Person","name":"Sapio-S"},"description":"算法发展","url":"https://sapio-s.github.io/posts/AlphaGo/","@type":"BlogPosting","headline":"从围棋到矩阵乘","dateModified":"2022-10-20T04:31:36+08:00","datePublished":"2022-10-20T00:00:00+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://sapio-s.github.io/posts/AlphaGo/"},"@context":"https://schema.org"}</script><title>从围棋到矩阵乘 | Sapio-S</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/favicon.jpg"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon.jpg"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon.jpg"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.jpg"><meta name="apple-mobile-web-app-title" content="Sapio-S"><meta name="application-name" content="Sapio-S"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/favicons/favicon.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Sapio-S</a></div><div class="site-subtitle font-italic">Interested in almost everything.</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/Sapio-S" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/" aria-label="twitter" class="order-4" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['1205402283','qq.com'].join('@')" aria-label="email" class="order-5" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" class="order-6" > <i class="fas fa-rss"></i> </a> <span class="icon-border order-2"></span> <span id="mode-toggle-wrapper" class="order-1"> <i class="mode-toggle fas fa-adjust"></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.updateMermaid(); }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer) ) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } updateMermaid() { if (typeof mermaid !== "undefined") { let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } this.updateMermaid(); } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>从围棋到矩阵乘</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>从围棋到矩阵乘</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> Sapio-S </span> on <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Thu, Oct 20, 2022, 12:00 AM +0800" >Oct 20<i class="unloaded">2022-10-20T00:00:00+08:00</i> </span></div><div> <span> Updated <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Thu, Oct 20, 2022, 4:31 AM +0800" >Oct 20<i class="unloaded">2022-10-20T04:31:36+08:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="2291 words">22 min read</span></div></div><div class="post-content"><h2 id="算法发展">算法发展</h2><h3 id="overview">Overview</h3><p>本部分涉及到的主要算法发展流程。来自DeepMind。</p><p>几种算法的表现如下。</p><h3 id="monte-carlo-tree-search-mcts">Monte-Carlo Tree Search (MCTS)</h3><p>在《人工智能导论》一课我们就学过这一算法，并且用它训练了一个玩四子棋的AI。让我们简单回顾一下这个算法。</p><p>算法主要分为四个部分：通过一定的方式选择一个可扩展节点$n_0$；执行行动，拓展$n_0$得到$n_1$；使用蒙特卡洛的方式（即随机落子）模拟$n_1$状态的棋局，得到$n_1$对应的奖励；回溯更新各祖先节点的价值。多次重复上述过程后，AI选择胜率（或者其他指标）最大的节点落子即可。</p><p>上述算法中，最重要的是如何<strong>选择</strong>节点扩展，以及如何<strong>更新</strong>节点的价值。使用UCB (Upper Confidence Bound)算法的UCT (Upper Confidence Bounds for Trees)即是目前最常见的MCTS变体。在UCT中，根据$\argmax_{v’\in children}(\frac{Q(v’)}{N(v’)}+c\sqrt{\frac{2\ln(N(v))}{N(v’)}})$选择要拓展的子节点；更新时，从子节点向父节点回溯，$N(v)\leftarrow N(v)+1$， $Q(v)\leftarrow Q(v)+\Delta$，其中$\Delta\leftarrow 1-\Delta$。简单来说，$N(v)$代表节点被访问的次数，$Q(v)$表示己方胜利的次数（这就是为什么要用交替变换的$\Delta$更新父节点），选择子节点扩展时，既要考虑到这些节点被访问的次数，尽量访问未被充分探索的子节点，同时也要考虑到子节点价值的高低，尽量寻找价值高的子节点。UCB的推导原理可以用regret、多臂老虎机模型等方式解释，此处不多做介绍。</p><h3 id="alphago">AlphaGo</h3><h4 id="rl训练">RL训练</h4><p>在前期训练阶段，AlphaGo的主要流程如下。</p><ol><li><p>从人类专家数据集(30 million!)中，使用supervised learning学出两种策略（即，给定棋局，选择下一步落子的位置）：一个是简单的fast rollout policy $\pi_\delta$，一个是复杂的SL policy $\pi_\sigma$。在SL时，输入除了棋局之外，还有围棋相关的指标(liberties, ladder status等)。rollout policy $\pi_\rho$的准确度低于SL policy $\pi_\sigma$，但是速度非常快。</p><li><p>$\pi_\sigma$通过self-play（即让模型自己同自己过去的任一历史策略对弈）和policy gradient，进化为一个更好的策略，即RL policy $\pi_\rho$。</p><li><p>用$\pi_\rho$self-play，可以得到多组棋局及其对应的胜率数据(30 million!)。用神经网络$V_\theta$拟合这些数据，即可快速评估某一棋局的价值。针对统一棋局，用$V_\theta$直接预测的结果，与执行100次$\pi_\rho$的结果基本相当，但是$V_\theta$更快。</p></ol><p>其中，后两步合在一起可以视作一次approximate policy iteration（即一次policy improvement与一次policy evaluation）的RL。</p><h4 id="mcts对弈">MCTS对弈</h4><p>在对弈阶段，AlphaGo的使用一种改进版的MCTS选择落子策略。</p><ol><li><div class="table-wrapper"><table><tbody><tr><td>selection: 在选择节点扩展时，选择的标准改为$\argmax_{a}(Q(s,a)+u(s,a))$，其中$u(s,a) \propto \frac{\pi_\sigma(a<td>s)}{1+N(s,a)}$，$\pi_\sigma(a<td>s)$即为$\pi_\sigma$给出的先验概率。</table></div><li><p>expansion &amp; simulation: 计算子节点的价值时，结合了两种方式：使用$V_\theta$直接估算价值，以及使用$\pi_\delta$运行一次rollout。子节点的价值为$V(s)=(1-\lambda)V_\theta(s)+\lambda z$。</p><li><p>backpropagation: 更新每个祖先节点，重新计算子节点的价值均值即可。</p><li>落子时，选择访问次数最高的行为。</ol><h3 id="alphazero">AlphaZero</h3><p>AlphaZero不需要围棋相关的知识与评判标准作为状态输入，也不需要专家数据，因此可以迁移至象棋与Shogi等其他棋盘游戏上。</p><h4 id="rl训练-1">RL训练</h4><p>policy网络和value网络使用了同一个残差网络，只是加了不同的head（即额外几层简单网络），使得网络可以同时给出策略和价值。</p><p>AlphaZero没有专家数据，使用iterative self-play training scheme直接从0学起，进行了多次approximate policy iteration。</p><h4 id="mcts对弈-1">MCTS对弈</h4><p>在MCTS时，AlphaZero只使用$V_\theta$判断棋局的价值，不再使用fast rollout policy$\pi_\delta$进行rollout。</p><h3 id="muzero">MuZero</h3><p>不用是棋局，是啥都行！</p><h4 id="rl训练-2">RL训练</h4><h4 id="mcts对弈-2">MCTS对弈</h4><h3 id="问题与思考">问题与思考</h3><h4 id="计算开销">计算开销</h4><p>AlphaGo用了1202 CPUs和176 GPUs执行MCTS；AlphaZero在RL阶段用5000 TPUs训了14天。</p><h4 id="self-play">self-play</h4><ul><li><p>好处：self-play构成了一组natural curriculum，每次都可以和自己水平相当的对手（即自己近期的历史模型）切磋。</p><li><p>坏处：策略未必收敛到纳什均衡，存在很多漏洞。固定AlphaZero的策略，可以用标准RL训练出一个能（且几乎只能）打败AlphaZero的AI。</p></ul><h2 id="alphago应用">AlphaGo应用</h2><h3 id="alphatensor">AlphaTensor</h3><p>用AlphaZero发现了矩阵乘的快速计算方式，优化了矩阵乘法的计算速度。Nature 2022.10的封面文章。</p><h4 id="背景">背景</h4><p>以2x2的矩阵乘为例，它的计算可以由以下方式表示。例如，用Strass’s方式计算C=AB的矩阵乘，可以用图b的方式算得。将图b的计算方式转换为向量表示，则可以得到c。</p><p>图a中的三位tensor记为$\Tau_n$，对于一个特定的维度n是确定的；c中的三个矩阵分别记为$U$, $V$, $W$。寻找矩阵乘的算法，即寻找一组$U,V,W$，使得$\Tau_n=\Sigma_{r=1}^R u^{(r)}\otimes v^{(r)}\otimes w^{(r)}$，其中$R$大于等于张量的rank，代表着计算矩阵乘法时运算的次数。</p><h4 id="问题建模">问题建模</h4><p>基于上述原理，可以将矩阵乘的过程抽象为一个TensorGame。</p><ul><li><p>初始“棋局”：$\Tau_n$。</p><li><p>终止状态：$\Tau_t = 0$。</p><li><p>“下棋”action：选择一组$(u^{(t)},v^{(t)},w^{(t)})$，其中每个元素只能在人为设定的集合$F={-2,-1,0,1,2}$中选择。</p><li><p>状态转移：$\Tau_t \leftarrow \Tau_{t-1} -u^{(t)}\otimes v^{(t)}\otimes w^{(t)}$。</p><li><p>奖励：每个时间步给-1的惩罚；规定时间内没有完成棋局，则根据最后状态的rank计算惩罚。</p></ul><h4 id="alphazero的应用">AlphaZero的应用</h4><p>AlphaTensor基于AlphaZero进行了一些修改：</p><ul><li><p>MCST扩展节点时，因为子节点的可能性过大，因此不枚举全部子节点比较其UCB（或者UCB相关的值），而是<strong>抽取</strong>部分子节点比较后扩展。</p><li><p>RL过程中，policy和value网络的底层框架改为Transformer式结构。价值函数$V_\theta$ 给出的结果是对步数的预期（对张量的rank的预期）。</p><li><p>RL训练的数据包括两部分：AI玩过的游戏回放，以及提前合成好的数据（随机生成$(u^{(t)},v^{(t)},w^{(t)})$，得到$\Tau=\Sigma_{r=1}^R u^{(r)}\otimes v^{(r)}\otimes w^{(r)}$，不管$\Tau$是不是目标张量）。</p></ul><p>一些任务相关的trick：</p><ul><li><p>数据增强：同一场棋局中，action的出现次序并不影响结果，所以可以交换action的次序，构建更多数据。</p><li><p>对张量进行三次cyclic transposition卷积，因为矩阵交换行或列后rank不变。</p></ul><h4 id="结果">结果</h4><ul><li><p>发现了更多种矩阵乘组合方式，其中部分方式的乘法运算次数比已知的任何算法都少（意味着在硬件上实现时可以更快，因为乘法器运算的时间远大于加法器）。</p><li><p>发现了更高维度的矩阵乘法分解方式。</p><li><p>针对特定的硬件，在reward中引入该硬件执行矩阵乘的时间作为惩罚，可以得到独特的矩阵乘法方式。</p></ul><h3 id="chemical-syntheses">chemical syntheses</h3><p>发表在nature上。使用AlphaGo的变体寻找分子合成的方式。</p><h4 id="问题建模-1">问题建模</h4><p>图a为逆向合成的反应思路，图b为对应的搜索树。terminal state中的分子都是工业上可用的材料。每一个状态下，可能action分为两种：在rollout阶段（即通过MC随机采样/rollout policy的方式估算子节点价值）使用的是小范围的rules，在MCTS节点扩展阶段使用的是大范围的rules（包括出现频率较少的rules，只关注反应中心）。</p><h4 id="alphago应用-1">AlphaGo应用</h4><p>基于AlphaGo的实现。从专家数据（这里是已知的化学反应原理）中学习fast rollout policy与expansion policy；用value网络预测反应的可行度。</p><p>注意这里没有self-play的参与，几乎全程使用MCST。（AlphaTensor应该也没有）</p><h3 id="quantum-physics">quantum physics</h3><p>发表在nature子刊npjqi上。使用AlphaZero优化动态量子代价函数(dynamical quantum cost functionals)。</p><p>这篇文章结合了Quantum Optimal Control Theory (QOCT) 与AlphaZero，但是因为笔者对于相应领域实在是不太理解，文章里也没有介绍，所以就不详细展开了。</p><h2 id="拓展阅读">拓展阅读</h2><h2 id="quiz">Quiz</h2><ul><li><p>In AlphaGo, how is the RL policy network $p_{\rho}$ trained and what role does it play in the final program?</p><li><p>What is the RL method used in AlphaZero?</p><li><p>Does the “emergent complexity/tool use” in the Bansal et al and Baker et al papers really require an adversarial setting?  If so, why? If not, explain how it could arise otherwise.</p><li><p>Explain the primary difference between the ASP and PAIRED approaches to auto-curriculum generation.</p></ul><h2 id="references">References</h2><p>课件参考：<a href="https://sites.google.com/view/berkeley-cs294-190-fa21">CS294-190 – Fa21</a></p><h3 id="papers">Papers</h3><h3 id="blogs">Blogs</h3></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/%E7%A7%91%E7%A0%94/'>科研</a>, <a href='/categories/paper/'>paper</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/rl/" class="post-tag no-text-decoration" >RL</a> <a href="/tags/%E7%AC%94%E8%AE%B0/" class="post-tag no-text-decoration" >笔记</a> <a href="/tags/%E6%8A%80%E6%9C%AF/" class="post-tag no-text-decoration" >技术</a> <a href="/tags/%E8%AE%BA%E6%96%87/" class="post-tag no-text-decoration" >论文</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=从围棋到矩阵乘 - Sapio-S&url=https://sapio-s.github.io/posts/AlphaGo/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=从围棋到矩阵乘 - Sapio-S&u=https://sapio-s.github.io/posts/AlphaGo/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=从围棋到矩阵乘 - Sapio-S&url=https://sapio-s.github.io/posts/AlphaGo/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/CSthesis-LR/">从0开始看懂PPO</a><li><a href="/posts/AlphaGo/">从围棋到矩阵乘</a><li><a href="/posts/RL-remake-1/">RL基础（精简版）part 1：介绍</a><li><a href="/posts/RL-remake-2/">RL基础（精简版）part 2：穷举小模型</a><li><a href="/posts/RL-remake-3/">RL基础（精简版）part 3：大模型近似</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/%E7%AC%94%E8%AE%B0/">笔记</a> <a class="post-tag" href="/tags/%E6%8A%80%E6%9C%AF/">技术</a> <a class="post-tag" href="/tags/rl/">RL</a> <a class="post-tag" href="/tags/%E6%9D%82%E8%B0%88/">杂谈</a> <a class="post-tag" href="/tags/%E8%AE%BA%E6%96%87/">论文</a> <a class="post-tag" href="/tags/%E8%B8%A9%E5%9D%91/">踩坑</a> <a class="post-tag" href="/tags/%E5%88%9B%E4%BD%9C/">创作</a> <a class="post-tag" href="/tags/%E5%89%A7%E6%9C%AC/">剧本</a> <a class="post-tag" href="/tags/%E7%A2%8E%E7%A2%8E%E5%BF%B5/">碎碎念</a> <a class="post-tag" href="/tags/%E8%89%BA%E6%9C%AF/">艺术</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/MARL-sequential-update/"><div class="card-body"> <span class="timeago small" >Sep 7<i class="unloaded">2022-09-07T00:00:00+08:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>MARL之sequential update</h3><div class="text-muted small"><p> 合作的MARL 问题描述 多机的POMDP（Partially Observable Markov Decision Process）可以用$&lt;N, S, A, P, r, \gamma&gt;$ 描述。其中，$N$是智能体个数，$S$是智能体的联合状态State，$A$是智能体的联合行为Action，$P$是状态转移函数Transition Probability functio...</p></div></div></a></div><div class="card"> <a href="/posts/RL-remake-1/"><div class="card-body"> <span class="timeago small" >Oct 20<i class="unloaded">2022-10-20T00:00:00+08:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>RL基础（精简版）part 1：介绍</h3><div class="text-muted small"><p> Overview RL并不要求一定有神经网络的参与，比如传统RL就可以视作一种纯粹的计算问题，至多是用了不同的近似方法去求解奖励，或者梯度式更新某一参数。但是随着问题越来越复杂，我们不得不开始考虑使用更强大的方式优化运算，于是出现了神经网络，出现了现在的Deep RL。 我按照自己的理解重新整理了David Silver在UCL讲授的RL内容。简单来说，前五讲解决的是小model的问题，...</p></div></div></a></div><div class="card"> <a href="/posts/RL-remake-2/"><div class="card-body"> <span class="timeago small" >Oct 20<i class="unloaded">2022-10-20T00:00:00+08:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>RL基础（精简版）part 2：穷举小模型</h3><div class="text-muted small"><p> 已知model：planning 用动态规划的思路对已知的MDP求解，主要有两种方式：1）policy evaluation+policy iteration；2）value iteration。 policy evaluation 本质上来说，每一个状态的价值是下一个状态的价值的期望: $V(S_t)\leftarrow E_\pi [R_{t+1}+\gamma V(S_{t+1}...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/MARL-sequential-update/" class="btn btn-outline-primary" prompt="Older"><p>MARL之sequential update</p></a> <a href="/posts/RL-remake-1/" class="btn btn-outline-primary" prompt="Newer"><p>RL基础（精简版）part 1：介绍</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2022 <a href="https://github.com/Sapio-S">Sapio-S</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/%E7%AC%94%E8%AE%B0/">笔记</a> <a class="post-tag" href="/tags/%E6%8A%80%E6%9C%AF/">技术</a> <a class="post-tag" href="/tags/rl/">RL</a> <a class="post-tag" href="/tags/%E6%9D%82%E8%B0%88/">杂谈</a> <a class="post-tag" href="/tags/%E8%AE%BA%E6%96%87/">论文</a> <a class="post-tag" href="/tags/%E8%B8%A9%E5%9D%91/">踩坑</a> <a class="post-tag" href="/tags/%E5%88%9B%E4%BD%9C/">创作</a> <a class="post-tag" href="/tags/%E5%89%A7%E6%9C%AC/">剧本</a> <a class="post-tag" href="/tags/%E7%A2%8E%E7%A2%8E%E5%BF%B5/">碎碎念</a> <a class="post-tag" href="/tags/%E8%89%BA%E6%9C%AF/">艺术</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id="></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); }); </script>
