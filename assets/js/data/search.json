[ { "title": "RL基础（精简版）part 3：大模型近似", "url": "/posts/RL-remake-3/", "categories": "科研, RL基础", "tags": "RL, 笔记, 技术", "date": "2022-10-20 00:00:00 +0800", "snippet": "Overview当状态空间/动作空间非常大或者连续的时候，我们没有办法遍历所有的state/action，只能通过estimate近似求解。前面讲到的针对小model的RL方法都可以看作是通过lookup table实现的，即，对于每一个state或者action，我们都记录了一个对应的$V(s)$或者$Q(s,a)$；现在我们使用各种函数逼近function approximation（比如linear combination，神经网络，决策树等）替代之前的lookup table，预测没有见过的状态的价值。value-based: Value Function Approximation我们希望agent可以从有限的经验中学习value function，预测没有见过的状态的价值。普遍使用$w$表示价值函数的参数。Gradient DescentGradient Descent快速回顾假设我们的目标函数是$J(w)$，其梯度是$\\nabla_wJ(w)$。为了求$J(w)$的（局部）最小值，我们只需要沿着这个方向更新$J(w)$的值：$\\Delta w=-frac{1}{2}\\alpha \\nabla_wJ(w)$。Prediction: 逼近V目标是$\\hat v(s,w) \\approx v_\\pi(s)$。此时可知： 目标函数：$J(w)=E_\\pi[(v_\\pi(S)-\\hat v(S,w))^2]$，即均方差MSE的期望。 梯度：$\\Delta w=\\alpha(v_\\pi(S)-\\hat v(S,w)) \\nabla_w \\hat v(S,w)$。这里使用单样本对gradient进行了采样，所以不用求期望。 使用一组线性函数(Linear Function Approximation)近似，即，令$\\hat v(S,w)=x(S)^Tw=\\sum x_j(S)w_j$。这种情况下，$\\nabla_w \\hat v(S,w)=x(S)$，因此梯度可表示为$\\Delta w=\\alpha(v_\\pi(S)-\\hat v(S,w)) x(S)$。但是，在训练时我们得不到价值函数的真值$v_\\pi (S)$，需要近似表示： MC: $G_t$。此时梯度为$\\Delta w=\\alpha(G_t-\\hat v(S_t,w)) x(S_t)$。收敛至局部最优。 TD(0): $R_{t+1}+\\gamma \\hat v(S_{t+1},w)$。收敛至全局最优。 TD(λ): $G^\\lambda _t$。收敛至全局最优。 Control: 逼近Q在policy evaluation时预估Q，之后使用$\\epsilon$-greedy完成policy improvement。拟合的思路与上一小节基本完全一致。目标是$\\hat v(s,w) \\approx v_\\pi(s)$，当使用Linear Function近似时，令$\\hat v(S,w)=x(S)^Tw=\\sum x_j(S)w_j$，有： MC: $G_t$。此时梯度为$\\Delta w=\\alpha(G_t-\\hat v(S_t,w)) x(S_t)$。 TD(0): $R_{t+1}+\\gamma \\hat v(S_{t+1},w)$。此时梯度为$\\Delta w=\\alpha \\delta x(S)$。 TD(λ): $G^\\lambda _t$。此时梯度为$\\Delta w=\\alpha \\delta_t E_t$。 batch RLGradient Descent使用的是流式的数据，更新参数后sample即废弃。更加sample efficient的方式是收集一段时间内的数据，直接在这个数据集上拟合。这时我们的优化目标转换为求Least Squares，即求$LS(w)=\\sum (v_t^\\pi-\\hat v(s_t,w))^2$的最小值。令其导数为0，可以直接求出结果$w=(\\sum x(s_t)x(s_t)^T)^{-1}\\sum x(s_t)v_t^\\pi$。同样的，这里的$v_t^\\pi$在实际求解时未知，需要通过MC/TD等方式近似。解决control问题时，我们使用least squares Q-learning进行policy evaluation，用greedy policy进行policy improvement。拟合Q时我们需要将上述的v换为Q。注意因为数据集包含多个历史policy的experience，所以必须off-policy式更新： 从旧policy的experience中取得sample$S_t, A_t, R_{t+1}, S_{t+1}$。 用新policy选择action$A’=\\pi_{new}(S_{t+1})$。 更新：$\\hat q(S_t,A_t,w) \\leftarrow R_{t+1}+\\gamma \\hat q(S_{t+1},A’,w)$。 【和Q-learning的关系？】policy-based: Policy Gradient我们希望agent可以从有限的经验中学习policy。直接学习policy的好处在于：更好的收敛性；可以有效用于高维/连续空间；可以学到随机策略（比如石头剪刀布场景下，随机策略才是最优解）。坏处在于：通常只能达到局部最优；很难直接评估策略。普遍使用$\\theta$表示policy的参数。优化目标我们可以用不同的方式衡量策略$\\pi_\\theta$的好坏，比如： start value: $J_1(\\theta)=V^{\\pi_theta}(s_1)$. average value: $J_{avV}(\\theta)=\\sum_{s} d^{\\pi_theta}(s) V^{\\pi_theta}(s)$. average reward per time-step: $J_{avV}(\\theta)=\\sum_{s} d^{\\pi_theta}(s) \\sum_a \\pi_theta(s,a)R^a_s$. 我们希望最大化$J(\\theta)$。有很多可选的方式，这里我们使用Gradient Descent，此时$\\Delta \\theta=\\alpha \\nabla_\\theta J(\\theta)$。policy gradientFinite Differences使用导数/微分的定义。对于$\\theta$的每一维k，计算$J(\\theta)$的偏导，从而得到近似的梯度：$\\frac {\\partial J(\\theta)}{\\partial \\theta_k} \\approx \\frac {J(\\theta+\\epsilon u_k)-J(\\theta)}{\\epsilon}$。score function根据公式$d \\ln(y)=\\frac {dy}y$，可以得到$\\nabla \\theta \\pi\\theta(s,a)=\\pi_\\theta(s,a) \\nabla_\\theta \\ln \\pi_\\theta(s,a)$。我们定义score function为$\\nabla \\theta log\\pi\\theta(s,a)$。score function其实就是gradient的估计器score function是可以根据policy的类别直接计算的，比如： softmax policy：针对离散行为，将行为表示为$\\phi(s,a)^T \\theta$，则采取行为的概率为$\\pi_\\theta(s,a) \\propto e^{\\phi(s,a)^T\\theta}$，此时score function为$\\nabla \\theta \\log \\pi\\theta(s,a)=\\phi(s,a)-E_{\\pi\\theta}[\\phi(s,·)]$。 Gauss policy：针对连续行为，将行为表示为$\\phi(s)^T \\theta$，采取行为的概率为$a \\sim \\mathcal{N}(\\mu(s),\\sigma^2)$，score function为$\\nabla \\theta \\log \\pi\\theta(s,a)=\\frac{(a-\\mu(s))\\phi(s)}{\\sigma^2}$。 policy gradient theoremFor any differentiable policy $\\pi_\\theta(s,a)$, for any of the policy objective functions $J = J_1, J_{avR},\\frac1{1-\\gamma}J_{avV}$, the policy gradient is $\\nabla_\\theta J(\\theta)=E_{\\pi\\theta}[\\nabla_\\theta \\log \\pi_\\theta(s,a)Q^{\\pi_\\theta}(s,a)]$.和之前一样，我们遇到了这个问题：$Q^{\\pi_\\theta}(s,a)]$未知，需要近似。想必大家都熟悉这个近似方式了。比如，用MC近似，则 可以视$v_t$为无偏样本，此时更新方式为$\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta \\log \\pi_\\theta(s_t,a_t)v_t$。value + policy: Actor-Critic这一类算法需要同时预估policy和value，维护两组参数。其中，actor为policy，critic为value，用$Q_w(s,a)$估算$Q^{\\pi_\\theta}(s,a)$。此时，policy gradient的梯度可以写作$\\nabla_\\theta J(\\theta)=E_{\\pi\\theta}[\\nabla_\\theta \\log \\pi_\\theta(s,a)Q_w(s,a)]$。这样一来，我们可以降低policy gradient的variance。critic的任务就是之前的policy evaluation。因此，我们可以用之前提过的任何方式更新critic和actor，把比如用TD(0)更新critic，用policy gradient更新actor。advantage function在不改变期望的情况下，我们可以引入baseline来降低算法的方差。常用的baseline就是state value function$B(s)=V^{\\pi_\\theta}(s)$。我们定义advantage function为$A^{\\pi_\\theta}(s,a)=Q^{\\pi_\\theta}(s,a)-V^{\\pi_\\theta}(s)$，此时梯度变为$\\nabla_\\theta J(\\theta)=E_{\\pi\\theta}[\\nabla_\\theta \\log \\pi_\\theta(s,a)A^{\\pi_\\theta}(s,a)]$。当然，$A^{\\pi_\\theta}(s,a)$的值我们得不到，所以同样使用近似：$A(s,a)=Q_w(s,a)-V_v(s)$。两个近似量可以用之前的任意方式求解。注意，由于$Q(s,a)=r+\\gamma V(s)$，因此我们实际可以用一套参数更新，比如TD error是$\\delta_v=r+\\gamma V_v(s’)-V_v(s)$。总结除了MC是policy-based之外，其余的算法都是Actor-Critic结构。【TBC】model-basedagent从经验中学习model，自行预测环境可能给出的reward和状态转移方式，进而以此构建value function/policy。具体来说，我们使用$\\eta$表示model的参数，要预测的model即为$&amp;lt;P_\\eta,R_\\eta&amp;gt;$，从$(s,a)\\rightarrow r$数据集中学习$R_\\eta$是regression问题，从$(s,a)\\rightarrow s’$数据集中学习$P_\\eta$是density estimation问题，使用MSE，KL divergence等loss求解即可。model可以有table lookup（记录全部(s,a)对），Gaussian Process等多种形式。 有了这个model之后，我们就可以得到下一步的状态分布$S_{t+1}~P_\\eta(S_{t+1} S_t,A_t)$以及奖励分布$R_{t+1}~R_\\eta(R_{t+1} S_t,A_t)$。之后，我们根据这个model求解MDP。 一个典型的求解思路是sample-based planning。这意味着学习到的model只负责产生samples，得到samples之后使用model-free的方式求解MDP（可以选用MC control, SARSA, Q-learning等方式）。我们也可以将model与其他方式结合。一个典型的例子就是Dyna算法，agent学到model之后，会混合真实的和模拟的数据，在此基础上learn+plan。另外，model也可以用在搜索中，辅助构建搜索树。在model-based search中，一般会用到simulation policy $\\pi$，model负责simulation，确定节点的下一个状态，并估计某一状态的价值。这类算法在更新Q函数的时候可以用MC control或者SARSA，其对应的应用即为Monte Carlo Tree Search和TD Tree Search。MCTS在另一篇blog中已经有详细介绍了，这里就不展开了。Exploration &amp;amp; ExploitationSelf-Play" }, { "title": "RL基础（精简版）part 2：穷举小模型", "url": "/posts/RL-remake-2/", "categories": "科研, RL基础", "tags": "RL, 笔记, 技术", "date": "2022-10-20 00:00:00 +0800", "snippet": "已知model：planning用动态规划的思路对已知的MDP求解，主要有两种方式：1）policy evaluation+policy iteration；2）value iteration。policy evaluation本质上来说，每一个状态的价值是下一个状态的价值的期望: $V(S_t)\\leftarrow E_\\pi [R_{t+1}+\\gamma V(S_{t+1})]$。用$v$表示价值矩阵，可以得到Bellman Expectation Equation $v=R+\\gamma Pv$。现在我们要迭代求解Bellman Expectation Equation（当然也可以直接矩阵求逆，但是维数大的时候计算量太大了）。这里循环迭代$v^{k+1}=R+\\gamma Pv^k$即可求得policy \\pi 的value，注意，这里每次迭代都会更新全部状态的价值。可以证明这样的迭代最后一定会收敛得到真正的$v$。policy improvement根据policy evaluation的结果，贪心求解即可。每次选择收益最大的行为$\\pi’(s)=\\argmax a q\\pi (s,a)$即为最优策略。一般来讲，我们先进行policy evaluation再进行policy improvement，二者交替进行。value iteration思路1：Principle of Optimality将v(s)逐步拆解，可以从末态出发逐步计算。状态s下的最优策略可以被分解为两部分：从状态s到下一个状态s’采取了最优行为；之后，在状态s’时，遵循s’状态下的最优策略。即，$v_(s) \\leftarrow max_a (R^a_s+\\gamma \\sum_{s’\\in S}{P^a_{ss’}v_*(s’)})$。思路2：迭代求解对Bellman Optimality Equation求解。循环迭代$v^{k+1}=\\max _a (R^a+\\gamma P^a v^k)$。可以证明最终一定会收敛到$v*$。DP优化(asynchronous DP)每次更新只对部分state有效（这些state提前backup），可以降低computation。in-place dynamic programming: 真·动态规划prioritised sweeping：对Bellman误差最大的state backupreal-time DP：只backup S_tsample backup：随机抽样一些state收敛性证明构造operator，使用Contraction mapping定理即可证明。未知model：model-free从这里起我们引入一个新的概念：episode。从某一个状态开始，Agent与Environment交互直到进入终止状态。完整的Episode对起始状态不做要求，但是要求个体必须达到某一个终止状态。提示：model指MDP的transition probability和reward。prediction当model未知时，估算value function，对应于planning中的第一步，policy evalution。迭代求解框架我们仍然想使用迭代的方式求出value function，但是现在$v^{k+1}=R+\\gamma Pv^k$式中的$R$，$P$未知，需要换个思路：用“牛顿下山法”等数值分析技巧直接迭代求解。最终，我们可以得到这样的迭代框架：$V(S_t)\\leftarrow V(S_t)+\\alpha ( X -V(S_t))$。其中，$X$代表着当前状态价值的真值（或估计的真值），有以下几种常见的形式。MCMonte Carlo方式中，我们认为value的估计值可以用样本中的return均值替代。此时，$X=G_t$，即有$V(S_t)\\leftarrow V(S_t)+\\alpha ( G_t -V(S_t))$。由于必须等到episode结束才能计算出$G_t$，所以MC方式必须在完整episode结束之后再进行更新。TD(0)全称为Temporal-Difference。每次更新时仅考虑下一个状态的reward，因此可以使用不完整的episode即时更新，更好地体现了Markov property（即无后效性）。此时的$X$称作TD target, $X=R_{t+1}+\\gamma V(S_{t+1})$，更新公式为$V(S_t)\\leftarrow V(S_t)+\\alpha ( R_{t+1}+\\gamma V(S_{t+1}) -V(S_t))$。其中$\\delta_t = R_{t+1}+\\gamma V(S_{t+1}) -V(S_t)$称为TD error。由于迭代时使用的$V(S_{t+1})$并不是准确的值（也是一个估计值），所以进行TD运算时，对$X$的估计是有偏的。如果我们能在迭代时得到准确真值，即$X=R_{t+1}+\\gamma V_\\pi(S_{t+1})$，那就可以得到无偏估计。TD(λ): Forward-View在TD(0)中，我们每次仅考虑了下一个状态的reward。我们当然可以综合更多步的价值再更新！从当前时刻起，第n步的reward可以表示为$G^{(n)}t=R{t+1}+\\gamma R_{t+2}+…+\\gamma ^n V(S_{t+n})$。加权结合n步内所有的reward，给n-步收获增加一个权重$(1-\\lambda)\\lambda^{n-1}$，我们就得到了$G^\\lambda_t=(1-\\lambda) \\sum \\lambda^{n-1}G^{(n)}_t$。令$X=G^\\lambda_t$，更新公式即为$V(S_t)\\leftarrow V(S_t)+\\alpha (G^\\lambda_t -V(S_t))$。显然，标准的TD(λ)需要完整的episode才可以计算出$X$并更新价值。Backward-View TD(λ)另一种思路是从后向前迭代更新，当第n步的reward出现之后，更新这一步之前的全部状态的价值。这样一来，更新方式是online的，但是结果与forward-view一样。我们仍然使用TD error的定义$\\delta_t = R_{t+1}+\\gamma V(S_{t+1}) -V(S_t)$，每次更新的时候，同时更新前面出现过的全部state。这样一来，迭代公式为$v(s) \\leftarrow v(s)+\\alpha \\delta_t E_t(s)$，其中$E_t(s)$ eligibility trace定义为$E_0(s)=0, E_t(s)=\\gamma \\lambda E_{t-1}(s)+1(s_t=s)$。注意这里的更新公式是矩阵形式的。对比【TBC】control这部分需要解决的问题是：当model未知时，如何选择最优策略。类似于planning，我们可以使用policy iteration（交替进行policy evaluation与policy improvement），或者直接进行value iteration。但是，与planning不同的是： 使用$Q(s,a)$而不是$v(s)$进行policy evaluation或者value iteration。这样我们可以直接通过$\\pi(s)=\\argmax_a Q(s,a)$得到最优策略，跳过model中$P$的参与。 进行policy improvement的时候，对greedy贪心算法进行轻微的调整，改为使用$\\epsilon$-greedy选择动作。即，以$1-\\epsilon$的概率选择当前认为最好的行为，以$\\epsilon$的概率随机选择一个行为。这主要是为了保证算法可以充分地探索环境。 这里引入了两个新的概念，on-policy和off-policy。简单而言，on-policy用策略$\\pi$产生的行为学习、优化$\\pi$，off-policy用别的策略$\\mu$产生的行为学习、优化$\\pi$，类似于“站在别人的肩膀上可以看得更远”。on-policy policy iteration下面简单说明如何得到$Q(s,a)$。与prediction一样，我们可以得到迭代更新的框架$Q(S_t, A_t)\\leftarrow Q(S_t, A_t)+\\alpha ( X -Q(S_t,A_t))$。其中，$X$有几种常见的表示形式： MC: $X=G_t$. TD(0), or SARSA: $X=R+\\gamma Q(S’,A’)$. 这个方法称为SARSA的原因是更新公式中含有SARSA: $Q(S,A)\\leftarrow Q(S,A)+\\alpha (R+\\gamma Q(S’,A’)-Q(S,A))$. Forward-View TD(λ), or SARSA(λ): $X=q_t^\\lambda$. 第n步的Q-return: $q_t^{(n)}=R_{t+1}+…+\\gamma^{n-1}R_{t+n}+\\gamma^nQ(S_{t+n},A_{t+n})$. $q_t^\\lambda=(1-\\lambda)\\Sigma_{n=1}^\\inf \\lambda^{n-1}q_t^{(n)}$, $q_t^{(n)}$. Backward-View SARSA(λ): $Q(s,a) \\leftarrow Q(s,a)+\\alpha \\delta_t E_t(s,a)$. $\\delta_t=R_{t+1}+\\gamma Q(S_{t+1}, A_{t+1})-Q(S_t,A_t)$. $E_0(s,a)=0, E_t(s,a)= \\gamma \\lambda E_{t-1}(s,a)+1(S_t=s,A_t=a)$. off-policy policy iteration我们希望用$\\mu$的行为优化$\\pi$，但是二者的策略分布并不相同，因此需要先通过importance sampling转换不同分布下的期望：$E_{X~P}[f(x)]=E_{X~Q}[\\frac{P(X)}{Q(X)}f(x)]$。之后，我们就可以直接将on-policy下推导的结果挪用过来继续使用了。 MC: $X=G_t^{\\pi / \\mu}=frac{\\pi(A_{t} S_{t})}{\\mu(A_{t} S_{t})} frac{\\pi(A_{t+1} S_{t+1})}{\\mu(A_{t+1} S_{t+1})} … frac{\\pi(A_{T} S_{T})}{\\mu(A_{T} S_{T})} G_t$. TD(0): $X=frac{\\pi(A_{t} S_{t})}{\\mu(A_{t} S_{t})}(R_{t+1}+\\gamma Q(S’,A’))$. Q-learning: value iteration$Q(S,A)\\leftarrow Q(S,A)+\\alpha (R+\\gamma \\max_{a’}Q(S’,a’)-Q(S,A))$.可以证明收敛到$q*(s,a)$。对比【TBC】总结【TBC】" }, { "title": "RL基础（精简版）part 1：介绍", "url": "/posts/RL-remake-1/", "categories": "科研, RL基础", "tags": "RL, 笔记, 技术", "date": "2022-10-20 00:00:00 +0800", "snippet": "OverviewRL并不要求一定有神经网络的参与，比如传统RL就可以视作一种纯粹的计算问题，至多是用了不同的近似方法去求解奖励，或者梯度式更新某一参数。但是随着问题越来越复杂，我们不得不开始考虑使用更强大的方式优化运算，于是出现了神经网络，出现了现在的Deep RL。我按照自己的理解重新整理了David Silver在UCL讲授的RL内容。简单来说，前五讲解决的是小model的问题，后五讲解决的是大model的问题。对于小model来说，若model已知，那么就可以直接用规划的方式得到最优解；若model未知，那么我们就需要使用model-free的方式，从prediction和control两个角度切入求解。对于大model来说，我们已经不能遍历全部的动作/状态空间，因此只能近似求解RL问题中最重要的三个组件：value，policy和model。基本概念agent &amp;amp; environmentt时刻，个体(agent)观测到observation $O_t$，执行action $A_t$，收到环境(environment)的reward $R_{t+1}$。环境接收个体的$A_t$，给出$O_{t+1}$与$R_{t+1}$。History &amp;amp; State History: $H_t=O_1,R_1,R_1,…,O_t,R_t,A_t$ State: $S_t=f(H_t)$。决定将来的已知信息，与历史有关。 Markov Property：A state $S_t$ is Markov if and only if $P[S_{t+1} S_t]=P[S_{t+1} S_1,S_2,…,S_t]$。历史信息不起作用，仅当前状态的信息足够推断将来。 Fully Observable Environments：即Markov Decision Process（MDP）。个体状态与环境状态一致。 Partially Observable Environments：个体状态 != 环境状态。个体需要考虑如何给出状态。个体的三个组成部分 policy：$\\pi$，从状态到行为的概率分布。给定状态s，$\\pi(a s)$为个体采取行为a的概率，即$\\pi(a s)=P[A_t=a S_t=s]$。策略仅和当前的状态有关，与历史信息无关；某一确定的Policy是静态的，与时间无关，但是个体可以随着时间更新策略。 Value Function：对未来将来的预测，评价当前状态（不是策略）的好坏。注意，价值函数基于特定的策略，不同策略下同一状态的价值可能不同。某一策略下的价值函数为$v_\\pi(s)=E_\\pi[R_{t+1}+\\gamma R_{t+2}+\\gamma ^2 R_{t+3}+… S_t=s]$。 model：个体对环境的建模，试图预测环境给出的状态和奖励。做出决策后，抵达下一个特定状态的概率为$P^a_{ss’} = P[S_{t+1}=s’ S_t=s,A_t=a]$，可能收获的奖励为$R^a_s=E[R_{t+1} S_t=s,A_t=a]$。 Markov Decision ProcessesMDP通常表示为 $M=&amp;lt;S,P,R,\\gamma,A&amp;gt;$。其中各项的含义如下： 状态集S：所有可能状态的集合。 状态转移概率矩阵P：表示所有状态的转移概率。$P^a_{ss’} = P[S_{t+1}=s’ S_t=s,A_t=a]$ 奖励函数$R$：给定状态(s)，时间(t)，下一时刻(t+1)可以获得的奖励期望，$R_s=E[R_{t+1} S_t=s]$。 衰减系数 Discount Factor $\\gamma$：在0到1之间。值越大，意味着agent越倾向于考虑长期的利益；值越小，agent越倾向于考虑短期的利益。 收获/收益/回报 return $G_t$：在Markov Reward Process上，从t时刻开始往后所有的奖励的总和，$G_t=R_{t+1}+\\gamma R_{t+2}+\\gamma ^2 R_{t+3}+…$。 value function：给定状态(s)，时间(t)，从该状态开始return的期望，即$v(s)=E[G_t S_t=s]$。给出某一状态的长期价值。 对于个体而言，我们可以定义策略policy和两种价值函数value function： policy：在特定状态下，agent采取各行为的概率。一般用$\\pi(a s)$表示。 状态价值函数V：给定状态$s$与policy $\\pi$，此时的价值函数为状态价值函数，记为$v_\\pi(s)$，表示从状态$s$开始，遵循当前策略可获得的return的期望。$v_\\pi(s)=E_\\pi[G_t S_t=s]$。 行为价值函数Q：给定状态$s$与policy $\\pi$，执行具体行为$a$时可以得到的return的期望，一般使用状态行为对进行描述，$q_\\pi(s,a)=E_\\pi[G_t S_t=s,A_t=a]$。 Bellman EquationBellman Expectation Equation在上述定义的基础上，可以得到MDP的两种价值函数方程，基本与MRP的价值函数方程相同，仅下标不同。\\(v_\\pi(s)=E_\\pi[R_{t+1}+\\gamma v_\\pi(S_{t+1})|S_t=s]\\\\q_\\pi(s,a)=E_\\pi[R_{t+1}+\\gamma q_\\pi(S_{t+1},A_{t+1})|S_t=s,A_t=a]\\)简单推导，就可以得到Bellman Expectation Equation。这个方程可以用来求解$v_\\pi(s)$，$q_\\pi(s,a)$。\\[v_\\pi(s)=\\sum_{a\\in A}{\\pi(a|s){(R^a_s+\\gamma \\sum_{s&#39;\\in S}{P^a_{ss&#39;}v_\\pi(s&#39;)})}} \\\\q_\\pi(s,a)=R^a_s+\\gamma \\sum_{s&#39;\\in S}{P^a_{ss&#39;} \\sum_{a&#39;\\in A}{\\pi(a&#39;|s&#39;)q_\\pi(s&#39;,a&#39;)} }\\]简单来说，Bellman Expectation Equation可以记作$v=R+\\gamma Pv$。Bellman Optimality Equation最优价值函数 最优状态价值函数：$v_*(s)=max_\\pi v_\\pi(s)$，给定状态s，在所有可能的策略中，选择最大的状态价值函数。（reminder：一个策略对应一种行为的概率分布。） 最优行为价值函数：$q_*(s,a)=max_\\pi q_\\pi(s,a)$，给定状态s和行为a，在所有可能的策略中，选择最大的行为价值函数。 根据定义和式(1)(2)，我们可以建立起$v_(s)$与$q_(s,a)$的关系。\\[v_*(s)=max_a q_*(s,a) \\\\q_*(s,a)=R^a_s+\\gamma \\sum_{s&#39;\\in S}{P^a_{ss&#39;} v_*(s&#39;)}\\] Bellman Optimality Equation 迭代上面出现的两个式子，即可得到这组方程。它有两种表述方式。这组方程可以用来求解$v_(s)$，$q_(s,a)$，进一步地，可以计算出最优策略。\\[v_*(s)=max_a (R^a_s+\\gamma \\sum_{s&#39;\\in S}{P^a_{ss&#39;}v_*(s&#39;)}) \\\\q_*(s,a)=R^a_s+\\gamma \\sum_{s&#39;\\in S}{P^a_{ss&#39;} max_{a&#39;} q_*(s&#39;,a&#39;)}\\] 此方程是非线性的，没有快速便捷的解决方案，一般通过迭代解决。 小结之后的全部内容基本都是在讲如何求解Bellman Equation。只要解出了Bellman Equation，我们就可以认为得到了最优策略。 policy $\\pi$ 优于policy $\\pi’$ ：$\\pi \\geq \\pi’$，若$v_\\pi(s) \\geq v_{\\pi’}(s), \\forall s$ 定理：对于任何MDP，下面几点成立： 存在一个最优策略； 所有的最优策略有相同的最优价值函数； 所有的最优策略具有相同的行为价值函数。 通过最大化$q_(s,a)$找最优策略。仅当$a=argmax_{a\\in A}{q_(s,a)}$（即，当前行为a是最优的行为）时，$\\pi_*{(a s)}=1$，其他情况下，$\\pi_*{(a s)}=0$。 对于任何MDP问题，总存在一个确定性的最优策略；如果我们知道最优行为价值函数，那么我们就找到了最优策略。当我们知道了最优价值函数，也就知道了每个状态的最优价值，这时便认为这个MDP获得了解决。" }, { "title": "从围棋到矩阵乘", "url": "/posts/AlphaGo/", "categories": "科研, paper", "tags": "RL, 笔记, 技术, 论文", "date": "2022-10-20 00:00:00 +0800", "snippet": "算法发展Overview本部分涉及到的主要算法发展流程。来自DeepMind。几种算法的表现如下。Monte-Carlo Tree Search (MCTS)在《人工智能导论》一课我们就学过这一算法，并且用它训练了一个玩四子棋的AI。让我们简单回顾一下这个算法。算法主要分为四个部分：通过一定的方式选择一个可扩展节点$n_0$；执行行动，拓展$n_0$得到$n_1$；使用蒙特卡洛的方式（即随机落子）模拟$n_1$状态的棋局，得到$n_1$对应的奖励；回溯更新各祖先节点的价值。多次重复上述过程后，AI选择胜率（或者其他指标）最大的节点落子即可。上述算法中，最重要的是如何选择节点扩展，以及如何更新节点的价值。使用UCB (Upper Confidence Bound)算法的UCT (Upper Confidence Bounds for Trees)即是目前最常见的MCTS变体。在UCT中，根据$\\argmax_{v’\\in children}(\\frac{Q(v’)}{N(v’)}+c\\sqrt{\\frac{2\\ln(N(v))}{N(v’)}})$选择要拓展的子节点；更新时，从子节点向父节点回溯，$N(v)\\leftarrow N(v)+1$， $Q(v)\\leftarrow Q(v)+\\Delta$，其中$\\Delta\\leftarrow 1-\\Delta$。简单来说，$N(v)$代表节点被访问的次数，$Q(v)$表示己方胜利的次数（这就是为什么要用交替变换的$\\Delta$更新父节点），选择子节点扩展时，既要考虑到这些节点被访问的次数，尽量访问未被充分探索的子节点，同时也要考虑到子节点价值的高低，尽量寻找价值高的子节点。UCB的推导原理可以用regret、多臂老虎机模型等方式解释，此处不多做介绍。AlphaGoRL训练在前期训练阶段，AlphaGo的主要流程如下。 从人类专家数据集(30 million!)中，使用supervised learning学出两种策略（即，给定棋局，选择下一步落子的位置）：一个是简单的fast rollout policy $\\pi_\\delta$，一个是复杂的SL policy $\\pi_\\sigma$。在SL时，输入除了棋局之外，还有围棋相关的指标(liberties, ladder status等)。rollout policy $\\pi_\\rho$的准确度低于SL policy $\\pi_\\sigma$，但是速度非常快。 $\\pi_\\sigma$通过self-play（即让模型自己同自己过去的任一历史策略对弈）和policy gradient，进化为一个更好的策略，即RL policy $\\pi_\\rho$。 用$\\pi_\\rho$self-play，可以得到多组棋局及其对应的胜率数据(30 million!)。用神经网络$V_\\theta$拟合这些数据，即可快速评估某一棋局的价值。针对统一棋局，用$V_\\theta$直接预测的结果，与执行100次$\\pi_\\rho$的结果基本相当，但是$V_\\theta$更快。 其中，后两步合在一起可以视作一次approximate policy iteration（即一次policy improvement与一次policy evaluation）的RL。MCTS对弈在对弈阶段，AlphaGo的使用一种改进版的MCTS选择落子策略。 selection: 在选择节点扩展时，选择的标准改为$\\argmax_{a}(Q(s,a)+u(s,a))$，其中$u(s,a) \\propto \\frac{\\pi_\\sigma(a s)}{1+N(s,a)}$，$\\pi_\\sigma(a s)$即为$\\pi_\\sigma$给出的先验概率。 expansion &amp;amp; simulation: 计算子节点的价值时，结合了两种方式：使用$V_\\theta$直接估算价值，以及使用$\\pi_\\delta$运行一次rollout。子节点的价值为$V(s)=(1-\\lambda)V_\\theta(s)+\\lambda z$。 backpropagation: 更新每个祖先节点，重新计算子节点的价值均值即可。 落子时，选择访问次数最高的行为。AlphaZeroAlphaZero不需要围棋相关的知识与评判标准作为状态输入，也不需要专家数据，因此可以迁移至象棋与Shogi等其他棋盘游戏上。RL训练policy网络和value网络使用了同一个残差网络，只是加了不同的head（即额外几层简单网络），使得网络可以同时给出策略和价值。AlphaZero没有专家数据，使用iterative self-play training scheme直接从0学起，进行了多次approximate policy iteration。MCTS对弈在MCTS时，AlphaZero只使用$V_\\theta$判断棋局的价值，不再使用fast rollout policy$\\pi_\\delta$进行rollout。MuZero不用是棋局，是啥都行！RL训练MCTS对弈问题与思考计算开销AlphaGo用了1202 CPUs和176 GPUs执行MCTS；AlphaZero在RL阶段用5000 TPUs训了14天。self-play 好处：self-play构成了一组natural curriculum，每次都可以和自己水平相当的对手（即自己近期的历史模型）切磋。 坏处：策略未必收敛到纳什均衡，存在很多漏洞。固定AlphaZero的策略，可以用标准RL训练出一个能（且几乎只能）打败AlphaZero的AI。 AlphaGo应用AlphaTensor用AlphaZero发现了矩阵乘的快速计算方式，优化了矩阵乘法的计算速度。Nature 2022.10的封面文章。背景以2x2的矩阵乘为例，它的计算可以由以下方式表示。例如，用Strass’s方式计算C=AB的矩阵乘，可以用图b的方式算得。将图b的计算方式转换为向量表示，则可以得到c。图a中的三位tensor记为$\\Tau_n$，对于一个特定的维度n是确定的；c中的三个矩阵分别记为$U$, $V$, $W$。寻找矩阵乘的算法，即寻找一组$U,V,W$，使得$\\Tau_n=\\Sigma_{r=1}^R u^{(r)}\\otimes v^{(r)}\\otimes w^{(r)}$，其中$R$大于等于张量的rank，代表着计算矩阵乘法时运算的次数。问题建模基于上述原理，可以将矩阵乘的过程抽象为一个TensorGame。 初始“棋局”：$\\Tau_n$。 终止状态：$\\Tau_t = 0$。 “下棋”action：选择一组$(u^{(t)},v^{(t)},w^{(t)})$，其中每个元素只能在人为设定的集合$F={-2,-1,0,1,2}$中选择。 状态转移：$\\Tau_t \\leftarrow \\Tau_{t-1} -u^{(t)}\\otimes v^{(t)}\\otimes w^{(t)}$。 奖励：每个时间步给-1的惩罚；规定时间内没有完成棋局，则根据最后状态的rank计算惩罚。 AlphaZero的应用AlphaTensor基于AlphaZero进行了一些修改： MCST扩展节点时，因为子节点的可能性过大，因此不枚举全部子节点比较其UCB（或者UCB相关的值），而是抽取部分子节点比较后扩展。 RL过程中，policy和value网络的底层框架改为Transformer式结构。价值函数$V_\\theta$ 给出的结果是对步数的预期（对张量的rank的预期）。 RL训练的数据包括两部分：AI玩过的游戏回放，以及提前合成好的数据（随机生成$(u^{(t)},v^{(t)},w^{(t)})$，得到$\\Tau=\\Sigma_{r=1}^R u^{(r)}\\otimes v^{(r)}\\otimes w^{(r)}$，不管$\\Tau$是不是目标张量）。 一些任务相关的trick： 数据增强：同一场棋局中，action的出现次序并不影响结果，所以可以交换action的次序，构建更多数据。 对张量进行三次cyclic transposition卷积，因为矩阵交换行或列后rank不变。 结果 发现了更多种矩阵乘组合方式，其中部分方式的乘法运算次数比已知的任何算法都少（意味着在硬件上实现时可以更快，因为乘法器运算的时间远大于加法器）。 发现了更高维度的矩阵乘法分解方式。 针对特定的硬件，在reward中引入该硬件执行矩阵乘的时间作为惩罚，可以得到独特的矩阵乘法方式。 chemical syntheses发表在nature上。使用AlphaGo的变体寻找分子合成的方式。问题建模图a为逆向合成的反应思路，图b为对应的搜索树。terminal state中的分子都是工业上可用的材料。每一个状态下，可能action分为两种：在rollout阶段（即通过MC随机采样/rollout policy的方式估算子节点价值）使用的是小范围的rules，在MCTS节点扩展阶段使用的是大范围的rules（包括出现频率较少的rules，只关注反应中心）。AlphaGo应用基于AlphaGo的实现。从专家数据（这里是已知的化学反应原理）中学习fast rollout policy与expansion policy；用value网络预测反应的可行度。注意这里没有self-play的参与，几乎全程使用MCST。（AlphaTensor应该也没有）quantum physics发表在nature子刊npjqi上。使用AlphaZero优化动态量子代价函数(dynamical quantum cost functionals)。这篇文章结合了Quantum Optimal Control Theory (QOCT) 与AlphaZero，但是因为笔者对于相应领域实在是不太理解，文章里也没有介绍，所以就不详细展开了。拓展阅读Quiz In AlphaGo, how is the RL policy network $p_{\\rho}$ trained and what role does it play in the final program? What is the RL method used in AlphaZero? Does the “emergent complexity/tool use” in the Bansal et al and Baker et al papers really require an adversarial setting?  If so, why? If not, explain how it could arise otherwise. Explain the primary difference between the ASP and PAIRED approaches to auto-curriculum generation. References课件参考：CS294-190 – Fa21PapersBlogs" }, { "title": "MARL之sequential update", "url": "/posts/MARL-sequential-update/", "categories": "科研, paper", "tags": "RL, 笔记, 技术, 论文", "date": "2022-09-07 00:00:00 +0800", "snippet": "合作的MARL问题描述多机的POMDP（Partially Observable Markov Decision Process）可以用$&amp;lt;N, S, A, P, r, \\gamma&amp;gt;$ 描述。其中，$N$是智能体个数，$S$是智能体的联合状态State，$A$是智能体的联合行为Action，$P$是状态转移函数Transition Probability function，$r$是多机共享的joint reward function，$\\gamma$是折扣因子discount factor。联合策略joint policy由各个agent的独立策略组成，我们需要找到一个最优的policy从而maximize the expected total reward:常见处理方式同单机一样，多机RL也可以大致分为value-based和policy-based两种。Value Decomposition这一派算法的思路是把global Q function拆分为多个local Q functions，【图片：/2022-09-05-10-57-56-image.png” title=”” alt=”” width=”386”&amp;gt;，让每个agent学习自己的Q function。决策时，每个agent选择让自己的local Q最大的动作，就可以使得整体最优。显然，不是任意一种拆分方式（$f_{mix}$）都可以满足要求。我们需要拆分方式满足Individual Global Max（IGM）。即：【图片：/2022-09-05-10-52-41-image.png” alt=”” width=”333”&amp;gt;显然，Value Decomposition需要对global Q function的拆分方式进行特殊设计，使得它满足上述的IGM条件。常见的local Q function构造包括： VDN（Additivity，简单加和）【图片：/2022-09-05-10-53-22-image.png” title=”” alt=”” width=”173”&amp;gt; QMIX（Monotonicity，单调性条件）【图片：/2022-09-05-10-55-10-image.png” title=”” alt=”” width=”100”&amp;gt; 注意，这些拆分方式与IGM是不等价的。它们都是IGM的充分不必要条件，限制条件更为严格。Policy Gradient这一类方法直接通过梯度下降对策略进行优化。MAPPO就是这一类的典型算法。常用术语parameter sharing如果每个agent学习自己单独的Q函数/策略，那么参数大小将随着agent的数量相应成倍增加（十个agent就有十个不同的网络需要训练！）。为了减少参数，同时增加训练的速度和稳定性，Value Decomposition和Policy Gradient都使用了parameter sharing的技巧，让各个agent共享相同的参数。CTDECentralized框架把所有智能体联合建模来学习一个联合的策略，该策略的输入是所有智能体的联合观测，输出所有智能体的联合动作。但是中心化训练框架的问题在于输入和输出空间巨大，难以适应较大规模的多智能体系统。Decentralized结构中，每个智能体的训练独立于其他智能体，其策略网络根据局部观测输出要采取的动作。去中心化的训练框架会面临环境不平稳的问题，这是因为在训练过程中，某一智能体将其他智能体看成环境的一部分，然而其他智能体的训练将导致环境中的状态转移函数发生变化，从而破坏强化学习算法遵循的马尔可夫假设。Centralized Training and Decentralized Execution (CTDE)是当前MARL非常常见的运算模式。在训练时采用中心化的模式，而在训练结束之后智能体仅仅根据自身的局部观测，利用训练好的策略网络进行决策。从而在一定程度上同时克服环境不平稳和大规模智能体的问题。在training阶段，各个agent可以获取到global信息，从而选择全局最优的策略；在inference阶段，各个agent只能根据自己的观测选择动作。【图片：/2022-09-05-14-20-24-image.png” title=”” alt=”” width=”379”&amp;gt;【图片：/2022-09-05-14-21-07-image.png” title=”” alt=”” width=”368”&amp;gt;在MAPPO里，CTDE体现在Actor-Critic的结构里。Actor仅通过agent的local observation选择动作，而critic则会通过整体的global observation（或者多agent local observation concat出来的信息）评判动作的好坏。training阶段，actor &amp;amp; critic都会不断迭代，但是在inference阶段，agent仅通过actor进行决策，不需要全局观测的指导。sequential / auto-regressive update从简单的例子看share policy但是，上述的share policy/share parameter存在着一些问题： 要求agent必须同质化（拥有相同的action space &amp;amp; observation space）； 不能保证收敛到全局最优。 针对第二点，我们考虑2-agent的XOR问题：两个agent必须一个选择1一个选择0才可以得分，否则不得分。在这个情境中，value decomposition无法得到满足IGM的Q function拆分方式；share parameter的policy gradient也无法获得最优解。而不进行parameter sharing的policy gradient则可以取得最优解。扩展至n-agent的XOR问题，不难求出，share policy的optimal joint return ($J^_{share}$) 与individual policy的optimal joint return ($J^$)满足下式：【图片：/2022-09-05-13-52-53-image.png” alt=”” width=”87”&amp;gt;单调性问题但是，仅仅取消parameter sharing也不能保证使用policy gradient的joint policy可以收敛到全局最优解，因为我们很难保证每一步更新都是单调的。考虑2-agent XOR的升级问题：$r(a_1,a_2)=a_1*a_2$，且初始点位于第四象限。如果agent根据各自策略决策，它们将倾向于将点向左上方优化，这很有可能会让reward进一步降低。而一个更为合理的更新方式则应该让点落入第一象限，取得更大值。【图片：/2022-09-05-13-46-08-image.png” alt=”” width=”355”&amp;gt;sequential / auto-regressive方式有没有办法可以解决上述的问题呢？考虑agent不再同时选择行动，而是遵从一定的顺序先后进行决策。换言之，假如之前智能体的joint policy是【图片：/2022-09-05-14-35-12-image.png” title=”” alt=”” width=”164”&amp;gt;，那么现在就变为【图片：/2022-09-05-14-35-36-image.png” title=”” alt=”” width=”210”&amp;gt;。此时，我们可以这样定义优势函数：【图片：/2022-09-05-15-58-16-image.png” title=”” alt=”” width=”575”&amp;gt;这意味着h到m这几个agent采取的动作对整体价值的影响。根据Multi-Agent Advantage Decomposition理论，这时我们将不再需要精心假设价值/优势函数的拆分方式。对于任何合作式MDP，下式都成立：【图片：/2022-09-05-14-30-45-image.png” alt=”” width=”317”&amp;gt;与value decomposition中的QPLEX（拆分优势函数的一种变体）对比：【图片：/2022-09-05-14-31-46-image.png” title=”” alt=”” width=”482”&amp;gt;注意，在上述的sequential update中，智能体可以以任意顺序排列，不局限于特定顺序。这种更新方式意味着各个agent只需要最大化自己的local advantage，即可使整体最优。单调性保证有了上述的Multi-Agent Advantage Decomposition，我们就可以推出多机情况下的单调更新算法。遵照这种方式，我们可以保证joint reward在更新过程中单调不减。【图片：/2022-09-05-14-17-13-image.png” title=”” alt=”” width=”505”&amp;gt;其中，【图片：/2022-09-05-14-45-13-image.png” title=”” alt=”” width=”545”&amp;gt;，是优势函数的期望。如果推过TRPO的单调性的话，上述内容会更好理解。【图片：\\2022-09-05-18-13-22-image.png)具体更新算法如下：【图片：\\2022-09-05-14-51-25-image.png)multi-modality除了单调性之外，auto-regressive/sequential update还有一个好处，就是可以让policy足够diverse，一次学到多个可能的最优策略。在XOR问题的基础上，可以引申出permutation game的概念。【图片：/2022-09-05-15-02-06-image.png” alt=”” width=”400”&amp;gt;对于independent policy来说，policy是确定性的，熵为0；而对于auto-regressive policy而言，熵为【图片：/2022-09-05-15-03-43-image.png” title=”” alt=”” width=”117”&amp;gt;。可视化4-agent的permutation game结果，如下。【图片：/2022-09-05-15-06-03-image.png” title=”” alt=”” width=”649”&amp;gt;HATRPO/HAPPOHATRPOHeterogeneous-Agent Trust Region Policy Optimisation。融合了sequential update+TRPO。【图片：/2022-09-05-15-12-42-image.png” title=”” alt=”” width=”644”&amp;gt;与TRPO对比：【图片：/D:/Desktop/MAT/TRPO.jpg” title=”” alt=”” width=”632”&amp;gt;主要的区别在于对优势函数进行了序列化拆分，引入了$M$这一项。HAPPOHeterogeneous-Agent Proximal Policy Optimisation。为了避免计算KL-divergence的Hessian矩阵【图片：/2022-09-05-15-08-25-image.png” alt=”” width=”50”&amp;gt;，减少运算量，这里使用PPO的clip思想对运算进行近似。此时，优化的目标为：【图片：/2022-09-05-14-55-58-image.png” title=”” alt=”” width=”646”&amp;gt;其中，【图片：/2022-09-05-14-56-42-image.png” title=”” alt=”” width=”248”&amp;gt;。与MAPPO对比：【图片：/2022-09-05-14-55-39-image.png” title=”” alt=”” width=”556”&amp;gt;可以看到，HAPPO在计算期望时，需要考虑到最近更新的agent的策略。实验结果SMAC &amp;amp; MuJoCo域上表现都非常好，但是SMAC不够难，所以与其他算法差距不大。MuJoCo中多个agent是异质的，所以HATRPO/HAPPO表现显著更好，而且agent数量越多，差距越大。【图片：/2022-09-05-16-49-02-image.png” title=”” alt=”” width=”645”&amp;gt;【图片：/2022-09-05-16-50-54-image.png” title=”” alt=”” width=”648”&amp;gt;revisiting MARLattention机制每个agent需要得到前面agent的动作序列以及自己的local observation。这里引入attention机制处理action序列，可以更好地提取动作序列的特征以及agent之间动作的关系。同时，运用attention可以让表征的维数不随着动作序列的长短改变而改变，从而可以运用parameter sharing。attention：常用于NLP相关工作，如翻译。在encoder-decoder的模型中，我们将一个序列编码为一个表征，再通过解码器转换为一组序列。为了帮助模型掌握序列中的重要信息，解码器在解码的每一步都评价序列中的各部分的重要性，从而使解码器可以关注到序列的各个部分。实验结果auto-regressive模型可以更好地促进agent间合作。下图为bridge游戏结果。红块、蓝块一起过桥。auto-regressive policy会产生多种过桥方式，但是independent policy总是让红块先走。【图片：/2022-09-05-16-54-43-image.png” title=”” alt=”” width=”400”&amp;gt;在SMAC上，两个agent轮流向敌人射击，让敌人不停在二者间徘徊；使用independent policy的agent则会到处跑，保持与敌人的距离。面对多个敌人，auto-regressive policy让agents散开，从而分散敌军的攻击；independent policy则无法学会分而治之。【图片：/2022-09-05-17-02-43-image.png” title=”” alt=”” width=”378”&amp;gt;【图片：/2022-09-05-17-04-25-image.png” alt=”” width=”381”&amp;gt;在GRF上，auto-regressive policy可以让agent配合传球进攻，但是independent policy中是一个agent直接射门。【图片：/2022-09-05-17-06-50-image.png” title=”” alt=”” width=”433”&amp;gt;MATMulti-Agent Transformer。其优势在于： 与MAPPO相比，可以训练heterogeneous agents； 与HAPPO相比，可以并行训练，效率更高。 Transformer模型结构Transformer自从诞生起来就引发了广泛的关注，性能非常卓越。主要包括多头自注意力机制、残差连接、层归一化、前馈全连接模块等基本单元。【图片：/2022-09-05-15-50-18-image.png” alt=”” width=”317”&amp;gt;MAT结构observation到action的映射很像语言翻译！这里将joint observation视作input，序列化encode后放入decoder得到序列化action作为output。【图片：\\2022-09-05-16-14-18-image.png)值得注意的是，在训练阶段，所有agent的数值运算与更新可以并行进行（全部action早已存在buffer之中，只需要加mask即可呈现出auto-regressive效果），所以运算速度会更快。为了更好地满足CTDE的要求，与先前的算法作fair comparison，这里引入了MAT-dec，即把decoder换成各agent share-parameter的简单mlp。但是，MAT-dec模型仍然与CTDE不那么相似，毕竟encoder已经捕捉到了各个agent之间的互动和联系。实验结果更优的策略SMAC，MuJoCo，Bi-DexHand和GRF上均可以取得比HAPPO/MAPPO更好的结果，意味着对于homogeneous和heterogeneous的任务，MAT都可以学到更好的策略。【图片：/2022-09-05-17-08-55-image.png” title=”” alt=”” width=”649”&amp;gt;few-shot learning在SMAC简单任务上训练，直接放到难任务上evaluate；在MuJoCo环境中训练好之后让部分关节坏掉，再进行evaluate。这意味着MAT模型有更好的泛化能力。【图片：/2022-09-05-17-15-23-image.png” title=”” alt=”” width=”647”&amp;gt;T-PPOTransformed PPO。加入distill【图片：\\2022-09-05-16-25-05-image.png)在training阶段，策略【图片：/2022-09-05-16-42-38-image.png” alt=”” width=”50”&amp;gt;主要包含两部分，一部分是从多头注意力机制这里引入的前面几个agent的动作，另一部分是从自己的trajectory里面学到的策略。在训练的时候，用自己的策略算KL，可以减少其他agent的行为对于这个agent的影响。在inference阶段，为了保证真·decentralized的执行方式，即agent只根据自己的observation决定行为，改为使用【图片：/2022-09-05-16-46-58-image.png” title=”” alt=”” width=”50”&amp;gt;推断下一步动作。这个策略是通过behavior cloning得到的。实验结果在SMAC与GRF上进行了测试。证明：sequential update有用，可以学到最优策略；distill不影响inference的performance。【图片：/2022-09-05-17-18-40-image.png” title=”” alt=”” width=”648”&amp;gt;【图片：/2022-09-05-17-21-29-image.png” title=”” alt=”” width=”648”&amp;gt;" }, { "title": "从0开始看懂PPO", "url": "/posts/CSthesis-LR/", "categories": "科研, 创作", "tags": "RL, 技术", "date": "2022-06-10 00:00:00 +0800", "snippet": "Overview本工作的两项研究内容均与强化学习（Reinforcement Learning，简称RL）紧密相关，运用了当前流行的近端策略优化算法（Proximal Policy Optimization，简称PPO）与衍生算法，多机近端策略优化算法（Multi-Agent Proximal Policy Optimization，简称MAPPO）。本章将介绍的强化学习主要原理与算法，并对所使用的算法思路逐一予以介绍，希望可以帮助读者理解后文的模型结构与训练方式。强化学习简介强化学习（RL）通常将问题建模为两个部分，即智能体（agent）与环境（environment），智能体通过与环境交互，学习如何进行决策。在每一时刻，智能体可以进行环境观测（Observation），采取行为（Action），并从环境中获取奖励（Reward）；环境则基于智能体采取的行为进行更新，准备好下一时刻反馈给智能体的观测以及奖励。【图】这一过程通常被建模为马尔可夫决策过程（Markov Decision Process，简称MDP），记为$M(S,A,P,R,\\gamma)$。其中$S$代表状态（State）空间，$A$代表行为（Action）空间，$P$代表状态转移函数（Probability），表示选择某一动作的可能性，$R$代表在特定状态下选取某动作的奖励（Reward），$\\gamma\\in(0,1)$是奖励的折扣因子(discount factor)。将奖励按照一定衰减比例计算叠加起来，即可得到收益（Return）：$G_t=\\sum_{i=0}^{\\infty} \\gamma ^{i} R_{t+i+1}$。RL的目的是选择一个最优的策略$\\pi$，而对策略好坏的评判标准则是收益的大小。智能体需要根据自己的观测调整行为，从而最大化收益。在RL中，智能体通常由策略（Policy），价值函数（Value Function）与模型（Model）中的一种或多种组成。其中，策略是状态（Status，即智能体观测到的全部信息）与行为的映射，描述某一状态下执行不同行为的概率；价值函数用于预测奖励，判断当前所处状态的好坏，好的状态或行为对应着更高的收益；模型则是智能体对环境的建模，包括预测下一个状态的概率，以及预测环境可能给出的奖励。为了更好地描述奖励与收益，我们定义了两个价值函数，即行为价值函数与行为-状态价值函数。 行为价值函数：从状态s开始，遵循当前策略$\\pi$可获得的收益的期望。$v(s)= \\mathbb{E} [G_t S_t=s]$. 行为-状态价值函数：给定状态与策略 ，执行具体行为时可以得到的收益的期望，一般使用状态行为对进行描述。$q_\\pi(s,a)=\\mathbb{E}_\\pi[G_t S_t=s,A_t=a]$. 根据定义，我们可以推出行为价值函数与行为-状态价值函数的关系。式2.3表明，给定状态$s$与策略$\\pi$，则$s$的价值为$\\pi$采取的全部可能行为的期望；式2.4表明，给定状态$s$与策略$\\pi$，行为$a$的价值可以分为两部分，其一是通过这个状态获得的价值，其二是下一步状态价值的期望。\\(v_\\pi(s)=\\sum_{a\\in A}{\\pi(a|s)q_\\pi(s,a)} \\\\ q_\\pi(s,a)=R^a_s+\\gamma \\sum_{s&#39;\\in S}{P^a_{ss&#39;}v_\\pi(s&#39;)}\\)我们想要求解的是最优策略，即在所有可能的策略中，选择使价值函数最大策略。这也就是说，我们求解的最优策略对应的最优价值函数需要满足下式：\\(v_*(s)=max_\\pi v_\\pi(s) \\\\ q_*(s,a)=max_\\pi q_\\pi(s,a)\\)简单代换，即可得到贝尔曼最优方程（Bellman Optimality Equation），也被称为Q函数。对其求解，即可得到最优策略。$q_(s,a)=R^a_s+\\gamma \\sum_{s’\\in S}{P^a_{ss’} max_{a’} q_(s’,a’)}$策略梯度然而，由于算力及内存的限制，我们很难针对最优方程直接求解从而得到最优的策略。尤其对于较为复杂的任务，他们涉及到庞大或连续的状态空间与动作空间，连状态-行为的表征都很难实现。因此，我们需要对价值函数或策略函数进行估算，从而在有限的资源限制下逐步逼近最优解。策略梯度（Policy Gradient）方法便是一种常用的估算方法。这个算法将策略参数化，以$\\theta$表示，计算过程中需要根据收益的梯度确定策略参数的更新幅度。通过定义，我们需要求取最大值的目标函数是$L(\\theta)=\\mathbb{E}[\\sum_{i=0} \\gamma ^{i}r_{i+1} | \\pi_{\\theta}]$，即$L(\\theta)=v_{\\pi_\\theta}(s)$。一般而言，我们可以直接采用$\\theta_{t+1} \\leftarrow \\theta_t+\\alpha \\nabla L(\\theta)$的算法对参数$\\theta$进行更新，但是对于上式而言，直接求解梯度是很难实现的，所以我们通过蒙特卡洛策略梯度（Monte Carlo Policy Gradient）对其进行无偏估计，之后根据估计值更新参数。因此，重点在于如何求取梯度。这里的梯度可以表示为：$\\nabla L(\\theta) = \\mathbb{E}_\\pi[G_t \\frac{\\nabla \\pi(a_t|s_t,\\theta_t)}{\\pi(a_t|s_t,\\theta_t)}]$但是，这样的方式仍然存在着一定的问题。对策略梯度优化会导致收益波动大，即样本方差较大，因此收敛速度慢。另外，在更新策略时，策略梯度倾向于增加收益大的动作出现的概率。考虑如下情况：不论行为的好坏，其带来的收益始终为正，而没有采样到的行为的收益始终是0，那么，好的行为如果没有被采样到，它出现的可能性同样会越来越低。因此，这里引入一个简单的值函数$b(S)$作为基线，在计算收益时，减去均值（或加权后的均值），从而使得收益有正有负，减少样本方差，并且保证策略更新的公平性和合理性：$\\nabla L(\\theta) = \\mathbb{E}_\\pi[ (G_t-b(S_t)) \\frac{\\nabla \\pi(a_t|s_t,\\theta_t)}{\\pi(a_t|s_t,\\theta_t)}]$进一步的，我们可以更换基线，使用状态价值函数作为基线函数。在近似求解过程中，价值函数需要自身迭代（bootstrap），并且对策略函数给出的结果进行评估。这便是演员-评判家（Actor-Critic）算法，评判家指值函数，演员指策略函数。为了表达方便起见，我们定义优势函数（advantage function），$A(s,a)=G-b(S)$，Actor-Critic算法中，优势函数即为$A(s,a)=G-V(s)$。PPO算法PPO算法对于策略梯度算法进行了一定的修改与提升。上述算法的问题在于难以确定合适的步长，以及更新策略的样本利用率太低。为了利用旧策略的数据，我们使用重要性采样（Importance Sampling）的思想，将需要求取梯度的目标函数改写为：$L_{\\theta_{\\text{old}}}(\\theta)=\\mathbb{E}[\\frac{ \\pi_{\\theta}(a_t|s_t,\\theta_t)}{\\pi_{\\theta_{\\text{old}}}(a_t|s_t,\\theta_t)}A_t]$当新旧策略相差太大，这样的更新方式将使参数剧烈波动。因此，我们引入KL散度衡量两个策略的差异，当差异过大时相应地减小更新幅度，从而避免策略变化过大带来的波动，并且保证策略在迭代中逐渐趋近最优解。将KL散度的限制引入式中作为惩罚，即可得到新的待优化的目标函数： $L(\\theta)=\\mathbb{E}[\\frac{ \\pi_{\\theta}(a_t|s_t,\\theta_t)}{\\pi_{\\theta_{\\text{old}}}(a_t|s_t,\\theta_t)}A_t-\\beta \\text{KL}[\\pi_{\\theta_{\\text{old}}}(\\cdot|s_t),\\pi_\\theta (\\cdot|s_t)]]$其中，KL散度函数为：$\\text{KL}(P|Q)=\\int P(x)\\log \\frac{P(x)}{Q(x)}dx$然而，KL散度惩罚的更新梯度不是很好求解。为此，PPO算法提出了一种实现起来更为简便、快速的算法，直接通过计算两个策略的比例衡量策略的不同，即计算$r_t(\\theta)=\\frac{ \\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta_k}(a_t|s_t)}$如果这个比值超过预先设置的范围，那么价值函数将被“修剪”（clipped），从而避免出现过大的波动。Clip函数定义为：\\begin{equation} \\text{clip}(f(x))= \\begin{cases} f(1-\\epsilon) &amp;amp; x &amp;lt; 1-\\epsilon f(x) &amp;amp; 1-\\epsilon &amp;lt; x \\leq 1+\\epsilon f(1+\\epsilon) &amp;amp; x&amp;lt;1+\\epsilon \\end{cases}\\end{equation}此时待更新的函数为：$L_{\\text{clip}}(\\theta)=\\mathbb{E}_t[\\min(r_t(\\theta)A_t, \\text{clip}(r_t(\\theta),1-\\epsilon,1+\\epsilon)A_t)]$PPO完整算法流程如下所示。PPO易于实现与调参，结果胜过当前最优算法，目前是强化学习领域里非常主流的算法。\\begin{algorithm}[!ht] \\caption{PPO算法} \\label{PPO} \\small \\begin{algorithmic}[!ht] \\STATE 初始化Actor参数$\\theta$，Critic参数$\\phi$ \\FOR{k=0,1,2,$\\dots$} \\STATE 执行策略$\\pi_{\\theta_{\\text{old}}}$，收集轨迹$D_k$ \\STATE 估算优势函数$A_t$ \\STATE 使用Adam优化器更新策略，$\\theta_{k+1}=\\mathop{\\arg\\max}\\limits_{\\theta} L_{\\text{clip}}(\\theta)$ \\STATE 使用Adam优化器更新价值函数，$\\phi_{k+1}=\\mathop{\\arg\\min}\\limits_{\\phi} \\mathbb{E}[(V_{\\phi_{k}}(s_t)-R_t)^2]$ \\ENDFOR \\end{algorithmic}\\end{algorithm}" }, { "title": "Romeo and Juliet改编剧本", "url": "/posts/Romeo-Juliet/", "categories": "杂谈, 创作", "tags": "创作, 剧本", "date": "2021-11-22 00:00:00 +0800", "snippet": "Romeo &amp;amp; Juliet说明整部剧采取倒叙+插叙的手法。scene0是原剧的最后一幕，人们发现了罗朱之死，唤神父前来叙述完整的故事。在实际表演时，scene0一直在台上的一个角落，与其他幕切换主要靠灯光。scene 0Prince Bring forth the parties of suspicion.Friar I am the greatest, able to do least, Yet most suspected, as the time and place Doth make against me, of this direful murder; And here I stand, both to impeach and purge Myself condemned and myself excused.Prince Then say at once what thou dost know in this.Friar I will be brief, for my short date of breath Is not so long as is a tedious tale. Romeo, there dead, was husband to that Juliet; And she, there dead, that Romeo’s faithful wife.Friar （转场词）These violent delights have violent ends And in their triumph die, like fire and powder, Which, as they kiss, consume. Therefore love moderately: long love doth so; Too swift arrives as tardy as too slow.scene 1 舞会Capulet You are welcome, gentlemen! A hall, a hall! Give room. And foot it, girls.(音乐)More light, you knaves, and turn the tables up.Romeo看到Juliet，对视Romeo What lady’s that, which doth enrich the hand Of yonder knight?Benvolio I know not.Romeo O, she doth teach the torches to burn bright! Beauty too rich for use, for earth too dear! Did my heart love till now? Forswear it, sight! For I ne’er saw true beauty till this night.罗朱跳舞，跳完舞二人在台上小声说话，全场静止Tybalt This, by his voice, should be a Montague. （拔剑）Now, by the stock and honor of my kin, To strike him dead I hold it not a sin.Capulet Young Romeo is it?Tybalt ’ Tis he, that villain Romeo.Capulet He shall be endured. I say he shall.静止结束，罗朱继续说话Romeo If I profane with my unworthiest hand This holy shrine, the gentle sin is this; My lips, two blushing pilgrims, ready stand To smooth that rough touch with a tender kiss.Juliet Good pilgrim, you do wrong your hand too much, Which mannerly devotion shows in this; For saints have hands that pilgrims’ hands do touch, And palm to palm is holy palmers’ kiss.Romeo Have not saints lips, and holy palmers too?Juliet Ay, pilgrim, lips that they must use in prayer.Romeo O, then, dear saint, let lips do what hands do! They pray; grant thou, lest faith turn to despair.Juliet Saints do not move, though grant for prayers’ sake.Romeo Then move not while my prayer’s effect I take. Thus from my lips, by thine my sin is purged.Juliet You kiss by th’ book.Nurse Madam, your mother craves a word with you.Romeo Who is her mother?Nurse Marry, bachelor, her mother is the lady of the house.Romeo Is she a Capulet? O dear account! My life is my foe’s debt.（被benvolio勾肩搭背扯到一边）全场暗，juliet走到台前独白，romeo从旁边慢慢走上前，打追光，舞台道具逐渐清空Juliet ’ Tis but thy name that is my enemy. Thou art thyself, though not a Montague. What’s Montague? It is nor hand, nor foot, Nor arm, nor face, nor any other part Belonging to a man. O, be some other name! What’s in a name? That which we call a rose By any other word would smell as sweet. Romeo, doff thy name; And for thy name, which is no part of thee, Take all myself.Romeo I take thee at thy word. Call me but love, and I’ll be new baptized; Henceforth I never will be Romeo.Juliet If that thy bent of love be honorable, Thy purpose marriage, send me word to-morrow, By one that I’ll procure to come to thee, Where and what time thou wilt perform the rite; And all my fortunes at thy foot I’ll lay And follow thee my lord throughout the world.（Romeo伸出手，Juliet接过，二人走向神父的地方，最好仍然是追光）Friar So smile the heavens upon this holy act That after-hours with sorrow chide us not!Romeo Ah, Juliet, sweeten with thy breath This neighbor air, and let rich music’s tongue Unfold the imagined happiness that both Receive in either by this dear encounter.Juliet They are but beggars that can count their worth; But my true love is grown to such excess, I cannot sum up sum of half my wealth.Friar Come, come with me, and we will make short work; For, by your leaves, you shall not stay alone Till Holy Church incorporate two in one.Friar （转场词，可以提前出现，声音渐大）My Lord doth speak of love. What is Love? Love is patient, love is kind. Love does not delight in evil but rejoices with the truth. It always protects, always trusts, always hopes, always perseveres. Love never fails.scene 2 打架[Enter TYBALT and others] （注意tybalt有几个跟班）Benvolio By my head, here come the Capulets.Mercutio By my heel, I care not.Tybalt Gentlemen, good-den. A word with one of you.Mercutio Couple it with something; make it a word and a blow.Tybalt You shall find me apt enough to that, sir, and you will give me occasion.Benvolio （劝架，挡在中间）We talk here in the public haunt of men. Here all eyes gaze on us.Mercutio Men’s eyes were made to look, and let them gaze. I will not budge for no man’s pleasure, I.[Enter ROMEO]Tybalt Romeo, the love I bear thee can afford No better term than this: thou art a villain.Romeo Tybalt, the reason that I have to love thee Doth much excuse the appertaining rage To such a greeting. Villain am I none.Tybalt Boy, this shall not excuse the injuries That thou hast done me; therefore turn and draw.Mercutio O calm, dishonorable, vile submission![Draws] Tybalt, you ratcatcher, will you walk?Tybalt What wouldst thou have with me?Mercutio Will you pluck your sword out of his pilcher by the ears? Make haste, lest mine be about your ears ere it be out.Tybalt I am for you. [Draws]Romeo Gentle Mercutio, put thy rapier up.（mercutio被刺伤，倒地）Mercutio I am hurt. Ask for me tomorrow, and you shall find me a grave man. I am peppered, I warrant, for this world. A plague o’ both your houses! Why the devil came you between us? I was hurt under your arm. （死）Benvolio Here comes the furious Tybalt back again.Romeo Alive in triumph, and Mercutio slain? Mercutio’s soul Is but a little way above our heads, Staying for thine to keep him company.Tybalt Thou, wretched boy, that didst consort him here, Shalt with him hence. （二人拔剑打架，tybalt倒下）Romeo This shall determine that.Benvolio Romeo, away, be gone! The citizens are up, and Tybalt slain.Romeo O, I am fortune’s fool! （跑着下台）scene 0Friar I married them; and their stol’n marriage day Was Tybalt’s doomsday, whose untimely death Banished the new-made bridegroom from this city. Capulet, to remove that siege of grief from her, Betrothed and would have married her perforce To County Paris.scene 3 逼婚Juliet卧室Romeo And trust me, love, in my eye so do you. Dry sorrow drinks our blood. Adieu, adieu! （跑下台）Juliet O Fortune, Fortune! Be fickle, Fortune, For then I hope thou wilt not keep him long But send him back.Lady Ho, daughter! are you up? now I’ll tell thee joyful tidings, girl.Juliet And joy comes well in such a needy time. What are they, beseech your ladyship?Lady Well, well, thou hast a careful father, child; One who, to put thee from thy heaviness, Hath sorted out a sudden day of joy That thou expects not nor I looked not for.Juliet Madam, in happy time! What day is that?Lady Marry, my child, early next Thursday morn The gallant, young, and noble gentleman, The County Paris, at Saint Peter’s Church, Shall happily make thee there a joyful bride.Juliet I pray you tell my lord and father, madam, I will not marry yet; and when I do, I swear It shall be Romeo, whom you know I hate, Rather than Paris. These are news indeed!Capulet （上台）How now? a conduit, girl? What, still in tears? How now, wife? Have you delivered to her our decree?Lady Ay, sir; but she will none, she gives you thanks. I would the fool were married to her grave!Capulet Soft! take me with you, take me with you, wife. How? Will she none? Doth she not give us thanks? Is she not proud? Doth she not count her blest, Unworthy as she is, that we have wrought So worthy a gentleman to be her bride?Juliet Not proud you have, but thankful that you have. Proud can I never be of what I hate, But thankful even for hate that is meant love.Capulet How, how, how, how, chopped-logic? What is this? ‘Proud’ — and ‘I thank you’ — and ‘I thank you not’ — And yet ‘not proud’? Mistress minion you, Thank me no thankings, nor proud me no prouds, But fettle your fine joints ’gainst Thursday next To go with Paris to Saint Peter’s Church, Or I will drag thee on a hurdle thither.Lady Fie, fie! what, are you mad?Juliet Good father, I beseech you on my knees, Hear me with patience but to speak a word.Capulet. Hang thee, young baggage! disobedient wretch! I tell thee what — get thee to church a Thursday Or never after look me in the face. Speak not, reply not, do not answer me! Out on her, hilding!Nurse God in heaven bless her! You are to blame, my lord, to rate her so.Capulet And why, my Lady Wisdom? Hold your tongue, Good Prudence. Smatter with your gossips, go!Nurse I speak no treason.Capulet O, God-i-god-en!Nurse May not one speak?Capulet Peace, you mumbling fool! Utter your gravity o’er a gossip’s bowl, 175 For here we need it not.Lady You are too harsh.Capulet God’s bread! it makes me mad.（下台）Juliet O sweet my mother, cast me not away! Delay this marriage for a month, a week; Or if you do not, make the bridal bed In that dim monument where Tybalt lies.Lady Talk not to me, for I’ll not speak a word. Do as thou wilt, for I have done with thee. （下台）scene 4 赐药Juliet O, past hope, past cure, past help! （走到神父面前）Friar Ah, Juliet, I already know thy grief; It strains me past the compass of my wits. I hear thou must, and nothing may prorogue it, On Thursday next be married to this County.Juliet Tell me not, friar, that thou hearest of this, Unless thou tell me how I may prevent it. Be not so long to speak. I long to die If what thou speak’st speak not of remedy.Friar Hold, daughter. I do spy a kind of hope, Which craves as desperate an execution As that is desperate which we would prevent.scene 0Friar Then comes she to me And with wild looks bid me devise some mean To rid her from this second marriage, Or in my cell there would she kill herself. Then gave I her (so tutored by my art) A sleeping potion; which so took effect As I intended, for it wrought on her The form of death. Meantime I writ to Romeo That he should hither come as this dire night To help to take her from her borrowed grave, Being the time the potion’s force should cease.scene 4 continuedJuliet Give me, give me! O, tell not me of fear!Friar Hold! Get you gone, be strong and prosperous In this resolve.Juliet Love give me strength! and strength shall help afford.125 Farewell, dear father.scene 0Friar But he which bore my letter, Friar John, Was stayed by accident, and yesternight Returned my letter back. Then all alone At the prefixed hour of her waking Came I to take her from her kindred’s vault; Meaning to keep her closely at my cell Till I conveniently could send to Romeo.scene 5 殉情Romeo急匆匆上台Romeo How oft when men are at the point of death Have they been merry! which their keepers call A lightning before death. O, how may I Call this a lightning? O my love! my wife! Death, that hath sucked the honey of thy breath, Hath had no power yet upon thy beauty. Thou art not conquered. Beauty’s ensign yet Is crimson in thy lips and in thy cheeks, And death’s pale flag is not advanced there.O, here Will I set up my everlasting rest And shake the yoke of inauspicious stars From this world-wearied flesh. Eyes, look your last! Arms, take your last embrace! and lips, O you The doors of breath, seal with a righteous kiss A dateless bargain to engrossing death! Come, bitter conduct; come, unsavory guide! Here’s to my love! [Drinks]O true apothecary! Thy drugs are quick. Thus with a kiss I die. [Falls]Juliet What’s here? A cup, closed in my true love’s hand? Poison, I see, hath been his timeless end. O churl! drunk all, and left no friendly drop To help me after? I will kiss thy lips. Haply some poison yet doth hang on them To make me die with a restorative. [Kisses him] Thy lips are warm! （看到匕首)O happy dagger! [Snatches Romeo’s dagger] This is thy sheath; there rust, and let me die.（神父匆匆上台，看到尸体，暗场）scene 0friar if aught in this Miscarried by my fault, let my old life Be sacrificed, some hour before his time, Unto the rigor of severest law.Prince We still have known thee for a holy man. Where’s Romeo’s man? What can he say in this?Balthasar I brought my master news of Juliet’s death; And then in post he came from Mantua To this same place, to this same monument. This letter he early bid me give his father, And threat’ned me with death, going in the vault, If I departed not and left him there.Prince Give me the letter. （看信）This letter doth make good the friar’s words, Their course of love, the tidings of her death; And here he writes that he did buy a poison Of a poor pothecary, and therewithal Came to this vault to die, and lie with Juliet. （召唤两家庭的人）Where be these enemies? Capulet, Montague, See what a scourge is laid upon your hate, That heaven finds means to kill your joys with love. And I, for winking at your discords too, Have lost a brace of kinsmen. All are punished.（两大家族所有人上场）Capulet O brother Montague, give me thy hand This is my daughter’s jointure, for no more Can I demand.Montague But I can give thee more; For I will raise her statue in pure gold, That whiles Verona by that name is known, There shall no figure at such rate be set As that of true and faithful Juliet.Capulet As rich shall Romeo’s by his lady lie — Poor sacrifices of our enmity!Prince A glooming peace this morning with it brings The sun for sorrow will not show his head. Go hence, to have more talk of these sad things; Some shall be pardoned, and some punished; For never was a story of more woe Than this of Juliet and her Romeo." }, { "title": "使用k8s部署一个简单的微服务", "url": "/posts/k8s-demo/", "categories": "笔记, 技术", "tags": "技术, 踩坑, 笔记", "date": "2021-11-14 00:00:00 +0800", "snippet": "环境配置环境说明使用了两台ubuntu系统的机器，一个用作master node+control plane，一个用作worker node。安装基本的linux工具sudo apt updatesudo apt -y install htop vim git curl gnupg apt-transport-https ca-certificates net-toolssudo cat &amp;lt;&amp;lt;EOF &amp;gt;&amp;gt; .bashrcexport EDITOR=vimEOFinstall dockercurl -fsSL get.docker.com -o get-docker.shsudo sh get-docker.sh --mirror Aliyunsudo cat &amp;lt;&amp;lt;EOF &amp;gt;&amp;gt;/etc/docker/daemon.json{ &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;], &quot;log-driver&quot;: &quot;json-file&quot;, &quot;log-opts&quot;: { &quot;max-size&quot;: &quot;100m&quot; }, &quot;storage-driver&quot;: &quot;overlay2&quot;}EOFsudo vim /etc/group # authorize docker# 仅作示例# docker:x:998:usersudo systemctl restart docker.servicedocker login # 登录账号，方便包的管理，需要到hub.docker.com注册一个账号vim的使用：输入i开始编辑，完成编辑后，点击esc，然后输入:wq即可退出。install kubeadmsudo apt-get updatesudo apt-get install -y apt-transport-https curlsudo curl -s https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpgsudo tee /etc/apt/sources.list.d/kubernetes.list &amp;lt;&amp;lt;-&#39;EOF&#39;deb https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial mainEOFsudo apt-get update最后一步会出现报错，显示Err:6 https://mirrors.aliyun.com/kubernetes/apt kubernetes-xenial InRelease. The following signatures couldn&#39;t be verified because the public key is not available: NO_PUBKEY xxxxxx NO_PUBKEY xxxxxx。继续执行以下指令。sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys xxxxxxsudo apt-get install -y kubeadm启动k8smaster节点需要进行如下操作。sudo kubeadm init --config kubelet-cfg.yaml mkdir -p &quot;$HOME&quot;/.kubesudo cp -i /etc/kubernetes/admin.conf &quot;$HOME&quot;/.kube/configsudo chown $(id -u):$(id -g) &quot;$HOME&quot;/.kube/configkubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.ymlsudo kubeadm token create --print-join-command上述kubelet-cfg.yaml的内容见GitHub。执行到这里可以看到命令行给出了一串提示。另一个节点(worker node)执行master结点给出的操作指示即可，注意要加sudo权限。sudo kubeadm join 10.x.x.x:xx --token xxxxxxxx --discovery-token-ca-cert-hash sha256:xxxxx成功后可以看到worker node的输出。This node has joined the cluster:* Certificate signing request was sent to apiserver and a response was received.* The Kubelet was informed of the new secure connection details.Run &#39;kubectl get nodes&#39; on the control-plane to see this node join the cluster.之后，worker node会自动受到master node的调度。使用kubectl get nodes，看到两个节点均处于READY状态即证明配置成功。至此，k8s基本搭建完毕了，下面准备运行微服务的环境。微服务调整microservices-demo出于项目需求，选择了Google开源的microservices-demo。为了收集各个service的latency数据，额外引入了如influxDB等内容，代码见这里。如果仅使用原版的demo，只需要进行install skaffold一步。git clone https://github.com/GoogleCloudPlatform/microservices-demo.git # 原版demogit clone https://github.com/Sapio-S/microservices-demo.git # 用作收集数据的demogit checkout localinstall skaffoldcurl -Lo skaffold https://storage.googleapis.com/skaffold/releases/latest/skaffold-linux-amd64 &amp;amp;&amp;amp; sudo install skaffold /usr/local/bin/setup influxDBwget https://dl.influxdata.com/influxdb/releases/influxdb2-2.0.7-linux-amd64.tar.gztar xvzf influxdb2-2.0.7-linux-amd64.tar.gzsudo cp influxdb2-2.0.7-linux-amd64/{influx,influxd} /usr/local/bin/pip install &#39;influxdb-client[ciso]&#39;docker run -d --name my_influxdb -p 8086:8086 -v /home/influxdb/data:/var/lib/influxdb2 influxdb # -v 宿主机路径:容器内路径。将容器内指定路径挂载出来到宿主机中，这里是把数据库本地存储的目录挂出来，保证容器销毁以后数据还在port-forward 8086端口到localhost上，在网页上完成注册操作，获得token，填回源码。install wrk2sudo apt install openssl libssl-dev make gcc libz-devgit clone https://github.com/giltene/wrk2.gitcd wrk2makek8s allow unsafe sysctls如果严格按照上面的步骤进行的话，这一步应该可以跳过。笔者第一次忘记添加正确的k8s config，因此需要在这里重新设置一遍。当然，推荐删掉全部pod后直接重启k8s，反正也需要重启。sudo kubelet --allowed-unsafe-sysctls &#39;net.ipv4.*&#39; trouble shooting因为之前先跑了一次skaffold，产生了超大量废弃的pods，状态显示为SysctlForbidden。先清除这些。skaffold deletekubectl get namespace# NAME STATUS AGE# default Active 9h# kube-node-lease Active 9h# kube-public Active 9h# kube-system Active 9hkubectl delete --all pods --namespace=defaultreset k8ssudo kubeadm resetsudo rm -rf ~/.kube/sudo rm -rf /etc/cni/net.d/sudo rm -rf /var/lib/cni/sudo ifconfig cni0 down # 未必可以成功执行sudo ip link delete cni0 sudo ifconfig flannel.1 down # 未必可以成功执行sudo ip link delete flannel.1run microservices-demosudo apt install python3-pippip install influxdb-clientpip install -r requirements.txt # 遇到版本不兼容的问题，忽略指定版本即可python3 run_wrk.pytrouble shooting 根据influxDB的具体情况，需要重新设置token, org, bucket。注意，创建influxDB client的URL要替换成本机的inner ip(用ifconfig下eth0暴露的IP)。 修改部分过时的dockerfile这样环境就搭好了，可以尽情收集数据了！" }, { "title": "《艺术、科学与哲学》之绘画艺术（待填坑）", "url": "/posts/arts-notes-1/", "categories": "笔记, 艺术", "tags": "杂谈, 艺术, 笔记", "date": "2021-11-13 00:00:00 +0800", "snippet": "你是否独立地面对过艺术作品默默的面对 与艺术作品的瞬间/持续的交流，寻找与自己有关的事物/普遍性；形态；与自己的故事/经验/梦境有关； 不确定性（欣赏的时机不同，个人背景不同，感受不同）哪怕只有一次触摸梵高的向日葵 视觉的触感（油画笔触的凹凸感，高低不平的肌理）-&amp;gt;观看方式发生变化，看到触觉相关的感受。 触觉意义上的视觉体验。打破了绘画二维的认知，可以叫“雕塑”。笔触有独立意义，有感情。梵高对光线的理解-&amp;gt;与欧洲传统绘画方法（他自己的之前的画作，注重技法，明暗etc）不同 独立面对艺术？如何算是独立？艺术是个体的，作为个体面对艺术，如何独立？忘掉艺术史-&amp;gt;不是不重要，但不是理解的框架。 从日常生活内容/习见中独立/解放出来。油腻-&amp;gt;过于功利，无法欣赏艺术。需要从油腻中简化出来。 孩童的艺术体验？是否需要有一些经验性的东西才能欣赏，否则只是天性（进化论上）的感受。需要理性参与，去思考/再创造。自己创造与欣赏他人的创造有什么区别？ 空+艺术 vs 个体经验+艺术？必须基于历史欣赏艺术？ 个人的知识会影响艺术审美，永远无法摆脱个人知识或偏见。“技术性”和“艺术性”的对立。“艺术性”是什么？（一种没有任何世俗的空白的快乐状态？） 欣赏中国画，不知道如何评价、如何判断、如何欣赏。需要技术背景才能有审美上的标准。标准如何制定？为何需要这样的标准？ 在绘画中保留笔触，甚至是有意为之，这是从印象派绘画开始出现的事情，但梵高却将笔触的效果发挥到了极致。从此，笔触具有了独立的意义，我们不只可以观看，还可以触摸绘画中的事物，这是一种触觉意义上的视觉体验，我们看到了粗燥或光滑，看到了凹凸不平的空间起伏，我们还可能看到更多。感受艺术比分析艺术更重要忘记艺术史把经验留在脑后走近莫奈的睡莲艺术史有局限性，忘记已知的部分、经验性的部分，甚至逆向思维。每个人需要有独特的观感。“艺术需要用无知去面对。” 莫奈的睡莲：颜色关系是湿润的，一种氛围。主观表现。其早期作品更为客观。心中的色彩。 印象派：旧绘画的范畴，客观、真实、公正、准确再现自然界的色彩，是进步。之前是宗教绘画，多半靠想象。 莫奈的睡莲系列，是晚年的作品，不同于他早期作品的客观描绘，睡莲系列作品更注重主观表现，绘画的手法上更趋向写意、更追求酣畅淋漓。当我们靠近莫奈的睡莲时候，更像是走近了处在睡莲中的莫奈，睡莲就是莫奈，莫奈也是睡莲。而观者则正好处于他们中间，我们的感受是湿润的。 潜意识感受艺术品的本身，音乐同理。观者是否真的可以超脱技术的限制或恐慌？对熟悉的东西会有安全感；门外汉对于杂乱的音响是否可以展开独立的音乐体验？ 不要急于寻找答案解惑容易添惑难让纠结之美留在心里分享塞尚的烦恼科学：需要结论性的结果。艺术：答案不是目的，在个性的答案中寻找共性的答案。达芬奇是未完成作品最多的艺术家：结果已知，没有兴趣画下去，需要有惊喜和未知。纠结之美。是否需要先验的技术基础？塞尚：反印象派画家，不按常规观察事物。动机为直觉和本能。突破点：视角perspective（个人看问题的角度，从一个焦点变成多个焦点/视点，把最好看的表面展现出来，思维创造现实中不可能的事物，多视角的画面展现在同一个画布上）。寻找获得问题答案的过程，在其中寻找生命的答案。最不成功的画作收获最多。可以观察绘画的过程而不是结论性的假象。人在没有办法的时候才会在帅、好看上下文章。美和漂亮混为一谈。艺术没有答案 != 艺术没有真理。需要有完备性，需要有真理。艺术的感染力=艺术有真理性（不是科学上的真理）。跳出真实油腻世界的可能性。艺术比照片在主观上更真实。 塞尚的烦恼始于他的独立思考，描绘那些想到的，而不仅仅是看到的事物。这既是一种折磨，也是一种升华。因为他很快就陷入了纠结、困惑、矛盾的陷阱之中，而且不可思议的在其中寻找到了美感，痛苦和幸福在此进行了转换。也许塞尚的本意就是寻找获得问题答案的过程，并且尽可能的在其中体验生命的意义。 在展厅中寻找自己绘画中有你的身影其实是自我的吸引挥之不去的夏加尔共性的因素；共鸣。一个好的作品有非常多的答案，有唯一答案的是广告/宣传。夏加尔：绘画自己童年的经验，在绘画中寻找自己。艺术是与生命平行的世界。看到的是完完全全的自己。没有目的、真诚地欣赏。个人感受不同。是否有统一的标准？艺术家是否具有解释作品的权威性？解释哲学：打破了文本作者的权威性，作者不具有解释作品的权威性；但不是所有人对于作品的理解都有同等重要性/权威性。 我们总是以为自己看到的是作品，看到的是作品中的故事。其实，我们看到的是完完全全的自己，只要我们是在没有目的、真诚的状态中观看绘画，就能够感受到自己的呼吸和心跳，这是从绘画作品中得来的感受。夏加尔在自己的绘画中寻找自己，我们是在夏加尔的绘画中寻找自己。 博物馆里的感受聆听色彩的共鸣观看寂静的声音戳戳点点的毕沙罗毕沙罗：用小点绘画。一个人在切（塞尚），一个人戳（毕沙罗）。琐碎连接成整体；瞬间构成时间和空间；喧闹成为宁静。 相对于塞尚风格的“切”，毕沙罗运用的则是“戳”。他戳出了琐碎，也戳出了整体，一种由“琐碎”连接起来的“整体”。“琐碎”是瞬间，许多的瞬间又构成了时间，构成了空间，并最终达到了永恒。“琐碎”中夹杂着喧闹，当“喧闹”连接成为“整体”，宁静就伴随在其中了。此时的绘画，既是用来听，也是用来看，更是用来品的。 灵感也许会来找你被触动的瞬间不是因为故事莫兰迪的瓶瓶罐罐内容属性。人们容易陷入表面：偏向于看故事，是绘画载体不是绘画本身。但是也应该关注形式属性。能否获得触动？艺术具有感染力（与人有关？人需要有感受力）。艺术应该有“美”和“力”。灵感来自于敞开的心扉&amp;amp;接纳。莫兰迪：一生在画瓶瓶罐罐。不仅是瓶罐，有黑白、大小、空间排列、间隔的节奏感、形式逻辑etc。不是在讲故事，那是摄影做的事情。表达一种状态，一种关系。绘画有自己的语言与情感表达。康定斯基：《论艺术的精神》。 事物的内容属性很容易成为绘画的目的，从而让我们忽略了它们的形式属性。我们只看到了罐子，却没有看到罐子与罐子，罐子与空间，空间与空间，空间与我们，我们与罐子之间存在着的相互关系，这种关系同样有感情、有意志、有思想，也同样会楚楚动人。 很少人能理解“艺术生活化”从生活中分离艺术将艺术融入生活马蒂斯只是在画画 “艺术生活化” 艺术不高于生活甚至低于生活。价值不在于美化生活，而在于与生活剥离，从剥离出来的部分中提取自己的部分，再返还于生活。 马蒂斯：深受东方艺术哲学影响的法国画家。野兽派，太丑、太随意、没有比例透视。“我不在画人，我在画画”。绘画与绘画的载体有区别。从画人开始，到画画结束。去除意义、内涵、功利，让绘画成为单纯的视觉体验。绘画语言从此具有独立的艺术价值。 人人都在美化。处理过的照片才有安全感。是否与自我存在的状态有关？现代人与艺术关系？ 舆论：艺术品高于生活，为艺术而艺术。是否有可能因为其他原因而艺术？艺术是否过于真空，与现实相距太远？是否毫无作用、毫无价值？ 去除意义、去除内涵、去除功利，让绘画成为一种单纯的视觉体验，体验空间、形态、色彩、韵律、节奏，这是否就是绘画在我们生活中的意义。 马蒂斯从画人开始，至画画结束。 其他（全部为讨论内容） 美和审美：美的感受在知性介入之前；带着知识的判断可以展开，可以看到更多的可能性、更多的样貌。油腻：人工介入。美：事物本身变换的过程。感性的美-&amp;gt;理性的好-&amp;gt;感性&amp;amp;理性结合导致不同的美的表现，审美的增长，欣赏更美的事物。 技术性限制审美的角度？ 结合未必是好的，但是会发生。 好奇驱动？为什么有好奇？是认识你自己。 艺术为了目的服务，折射出的思想。 我们在欣赏艺术的时候究竟在欣赏什么？ 回应了什么（常识和知识） 打破了什么（重新认识） 创作者和观赏者的交流？ 艺术品：有共性的观点赋予其价值。 艺术品？艺术家？艺术？懂得艺术？（现代艺术打破懂和不懂的界线） 人看不到自己经验之外的内容。欣赏&amp;amp;鉴定的区别？ 再现意义上的表现林风眠：“中国的塞尚”，黄色的梨。中国现代艺术教育先驱。绘画是对自然“美景”的“真实”描绘绘画的三个阶段：再现（很长）– 表现（印象派后期开始，100-200年）– 实现。我认识了真实 – 我创造了真实 – 我等同了真实。描绘？真实？类似的情绪、形态、颜色？对客观的追求：再现意义上的绘画。表现绘画，源于艺术家的不满足。莫奈开始，注意画家和人之间的关系，是否需要遵循观察到的规律？否。将情绪融入客观中。主观绘画，与自然的距离越来越远。我们与自然的关系究竟是什么？对立or包含or相等？中国绘画不想表现（对立关系），而是天人合一。西方逐渐受到日本绘画的影响。绘画创造了“美景”或“美景”创造了绘画如何理解“好看”。美国社会的审美偏印象派，东方审美与之不同。 再现、表现、实现，是对绘画不同发展阶段的概括。“我再现你” “我表现你” “我就是你“。“我认识了真实” “我创造了真实” “我等同了真实”…… 认知意义上的思考吴冠中：西方绘画建立在明暗上，中国绘画建立在黑白上。强调黑白的节奏韵律。艺术的标准是如何灵活运用使用共性的标准。黑白的运用产生意识的流动。不再“摆事实”和“讲道理”的绘画再现故事。像报告文学，接近描述。“三突出”：典型事件典型场景中的典型人物。现实主义文学。不是艺术手段的唯一。让我的意识“随波逐流”换一个角度思考问题，从意识的角度思考问题。不再寻找故事、题目、答案。与绘画的桥梁是否是故事？用绘画描述事物（文学语言）与用绘画表达事物（视觉语言）是不同的。结合在一起会产生意识的流动。 用绘画描述事物与用绘画表达事物是不同的，“描述”倾向文学语言，“表达”倾向视觉语言。将描述和表达结合在一起，意识会产生流动，或许也可以叫做意识流…… 客观意义上的主观贾科梅蒂《行走的人》：法国艺术家。很多艺术家一生都伴随着纠结，艺术家必须要享受这种纠结。减掉解剖的部分（客观），融入自己的认知（主观），纠结的结果。与科学研究的思维是否存在相关性？人如何面对事物、如何面对人-&amp;gt;人如何面对自己。绘画中是否真的存在“客观”主观认知也是一种客观。认为绘画是客观，是否是一种主观？中世纪认为光线一定从上面来，与宗教有关，是一种主观。莫奈追求科学，描绘色彩，开拓了颜色的冷暖（此前只有明暗深浅），否定了固有色的存在。“主观”的认知也是一种“客观”面对绘画作品时，我们的反应是相对的主观和相对的客观。 当所有的人都认为，绘画是一种客观事物时，认知反而成为了一种主观。在绘画作品面前，我们的反应应该是相对的主观和相对的客观…… 绝对意义上的相对赵无极：没有颜色，没有形式，没有内容，但是不代表没有意义。中国的艺术是抽象的艺术。在“相对”的状态中才会被感动绝对化的状态：重复了多次，没有偶然性，没有感动，没有好奇，没有忘我。“绝对”的颜料和“相对”的色彩颜料在被调配前是绝对的，只是颜料不是色彩；调配后是相对的，才是色彩，产生了偶然。在博物馆中不是为了寻找答案，而是为了添惑（不是结论）。创作中是否也需要忘我？ 相对的因素是不确定的，也是非结论性的，因此会有好奇、会有偶然性、会有由此而来的忘乎所以，那种状态就是正在发生的感动…… 理性意义上的非理性康定斯基：《论艺术的精神》。本来学习法律，但是转投绘画。阳光照射在倒放的绘画上，发现新大陆。认为绘画的语言可以有自己的表现方式，与绘画的内容等分离。绘画学习没有“正确”的方式艺术创作没有标准。比例透视可以有，但是从不准确出发或许也可以，描绘会更有特点。恢复“不正确”的感受力并永远保持下去标准无法感受，可能性可以感受，因此不正确意味着开拓和可能性。 如果“正确”意味着标准和习惯，那么“不正确”就意味着开拓和可能，我们感受不到标准，却可以感受到可能…… 单纯意义上的复杂封塔纳：《概念空间，等待》。在纸上切、戳，回归到极简。“简单的事实”与“复杂的美感”“不确定性”中产生的“画面感”轻描淡写中的错综复杂转换。用尽全身力气轻轻画上一笔数学的运算，用复杂的方式证明简单的结论。对立的因素可以互现转换，以至于先前的常识认知不起作用，我们开始记起自己是谁。体会自己，知道自己有多好或者多糟糕。 相对应的因素之间是可以相互转换的，转换到措手不及，以至于我们开始记起自己是谁，记起自己到底有多好…… 审美意义上的本能米罗。画完再想，而非想好再画。怀疑什么也别怀疑自己的本能自己并列于他人。本能是什么？直觉反应。本能是在理性之上“涂抹”的思考怀疑基于经验（依赖）or本能（自信）。基于本能的判断更接近审美思考。审美：将不相关的事物放在一起的比较，得到完全不相关的结论。 怀疑本身是基于经验，还是基于本能，前者是依赖，后者才是自信。那些基于本能的判断，才更接近审美思考…… 附录实践活动 跟随音乐绘画 写生 木版画 形体探究 拓印" }, { "title": "乐理之和声学(1)", "url": "/posts/music-theory-1/", "categories": "笔记, 音乐", "tags": "杂谈, 音乐, 笔记", "date": "2021-11-12 00:00:00 +0800", "snippet": "第一讲钢琴即兴伴奏的能力 瞬间感知音乐的能力 钢琴演奏能力 运用和声技术能力 钢琴织体运用能力 音乐想象的能力三和弦T为主和弦功能；S为下属和弦功能；D为属七和弦功能。这个图要熟练掌握。换一种划分方式，根据大小三和弦来划分，可以得到这种分法。顺带一提，七和弦的划分如下。第二讲柱式和弦织体的运用平铺用于叙事部分。这个re其实是大三和弦+下移八度的九音。运动用于起伏/情绪变化部分。推动推进高潮。注意震音的练习，目标1min。其他 可以把钢琴视作整个乐队，低音为bass声线、鼓、铜管乐器等，高音为木管、弦乐等。 和弦之间的衔接，需要考虑就近原则，大部分音需要保持（内声部保持，和弦之间有一定的一致性）。第三讲这一讲内容贼多，从此之后我就听不懂了。注意，这里举的例子部分涉及到了转位。三和弦变体：大调三和弦大三和弦一定要加大二度外音。第二小节的D7是属七和弦（大三和弦+小七）。另：大三和弦示例中，T-D-S是铁三角关系。三和弦变体：小调三和弦小调分三种。自然小调要升7；和声小调要升6；旋律小调要升6升7。注意这里还原7与升7交叉运用。基本和声功能进行即：和弦与和弦之间的逻辑关系。 T-S-D7-T（这个D7是说D的属七和弦，不是TVII，更没有DVII），即1-4-5-1 T-DTIII-S-D7-T，即1-3-4-5-1 T-SII-S-D7-T，即1-2-4-5-1 T-TSVI-S-SII-D7-T，即1-6-4-2-5-1一些伴奏的提示（后文有更详细的说明）两手相距两个八度内，声音才可以集中。左手弹奏的低音范围不能超过中央c往下两个八度的mi。强拍一定要换和弦，4/4即为每小节换一次。第四讲和声功能配置法则 旋律音存在于和弦内。太显然了，不放例子了。 旋律中，外音不参与旋律与和弦之间的选择。以作业为例简单说明。作业为《同一首歌》的旋律，F大调。本讲中，配的简单和声为（两拍一个和声，即一小节两个和声）：T – T – S – T ——— T – T – SII – D ——— T – S – D – T(可以改用DTIII，和声的意外进行) ——— SII – D – D – T一开始我把起的部分配成了T – TSVI – SII – T，但是这样一来，中间两个和声功能相同，属于和声切分。第五讲六和弦的运用六和弦只能重复根音与五音，重复三音听上去会很浑浊。六和弦常用在两个三和弦之间，“嫁接两个三和弦”。以《同一首歌》做示例。蓝框中这个小节用了两个T和弦，第二个和弦即为六和弦。注意伴奏时二度外音的使用、节奏的流动变化。弹奏和声原则 振动原则：中央C以下必须弹奏开放和弦（就是低音要叉得开一点，不要太密集了）。 两个八度之内：右手柱式和弦和左手根音在两个八度内。 保持或级进：只能做2-3度内的变化。第六讲离调（本调离调） 原理：由属到主，和声的“解决”关系，影响旋律的色彩。适用于长音的多种色彩处理。 找离调：和弦的第二转位，从根音起做大小七和弦（即属七和弦）。 何时使用：找S功能的和弦，看S的属七与S前面的一小节的音有无冲突。仍然用《同一首歌》举例。在接到S和弦前，可以加入一个属七和弦。(fa la do ♭mi -&amp;gt; fa ♭si re)副歌部分也可以加入属七和弦。(fa la do ♭mi -&amp;gt; fa ♭si re)(so si re fa -&amp;gt; so do mi)第七讲减七和弦离调减七和弦全部为小三组成，必须解决到T。由紧张、待解决变为松弛，离调时发生更多的色彩变化。 找减七离调：给定一个和弦，用它根音上的小二度为基础，做减七和弦（四个音都是小三度关系）。 何时使用：离调是需要计算&amp;amp;尝试的，需要看减七和弦与旋律是否有冲突。这次举的例子是《夜半歌声》。13小节的离调与上面一致，是TSVI的减七和弦离调。第二个则是SII的本调离调。第八讲和声切分是一种错误的和声配法。强拍不能重复弱拍和声，尤其在跨小节的情况下，必须换和声。辅助功能这种变化可以使主和弦和声更稳固。 第一种较为原始，常见于古典音乐。 第二种有所升级，用了一个小小七和弦，听起来更为温柔，浪漫派音乐常用。注意，SII2一定要以fa为根音（F大调的情况下），更好听。 第三种为重属2和弦，有一种“想挣脱但是被拽回来”的感觉，更有色彩，常见于晚期浪漫派，爵士，流行乐之中。在《夜半歌声》的伴奏中，就使用了类似的技术，如下图，是一个T-SII2-T。作业&amp;amp;讲评 织体的选择：旋律本身有较强的律动，因此配伴奏的时候，两小节一个和弦，不要太复杂，否则会影响旋律的流动。伴奏需要有鼓的功能，不要太分解。 和弦的选择：见下图。附录：几段旋律的完整伴奏谱《同一首歌》副歌的部分没有给全，此处没有放，请自行脑补。用了一些诸如对位之类的技巧。《夜半歌声》老师自己写的一段旋律。" }, { "title": "hello world!", "url": "/posts/hello-world/", "categories": "笔记, 技术", "tags": "踩坑, 碎碎念", "date": "2021-11-05 00:00:00 +0800", "snippet": "人到底会在《网络安全工程》课上做出什么啊。大致的框架挺眼熟的，20年暑假给开源社区提交过blog就是这个格式，当时不太理解，现在看一眼就差不多明白了，进步了～第一步自然是找模版，自己搭是不可能的，这辈子都不可能的，因为我懒。找了半天，好不容易锁定了一个还不错的样式Flex，兴高采烈就搬到自己的仓库里了，很快啊。无师自通装了个github action，美滋儿滋儿地自我陶醉了一下，换上头像，发个blog庆祝。然而简单测试了一下其他的功能，发现这个排版……有一点点一言难尽。（虽然现在看起来也还可以…呃）于是继续踏上寻找模版之旅。一个彩蛋：persephone模版的demo链接是作者的个人网站，里面有她写的小说。说起来这个模版还真不错，我为什么不用呢（突然陷入沉思）。最终挑了半天决定用chalk，另外几个觉得还不错的包括jekyllDecent，monochrome，simple-texture，都是偏简约的设计。考虑到chalk的功能比较多，还支持视频播放，最终决定用它。万万没想到chalk的水还挺深，我直接挖了个坑给自己跳。github action不通过仔细阅读了README，发现Chalk does not support the standard way of Jekyll hosting on GitHub Pages这句话，泪目。不死心的我试图在github actions中加入ruby，仍然直接用github跑，往里面一点点加命令，未果，在npm run publish的时候会报错，让我set github account。然后再次仔细阅读README，灵光一闪，意识到我应该在本地运行这一切。windows装Ruby别装，深坑。在这一步我把github的仓库迁移到了gitee上。私心觉得gitee确实挺好用的。Mac装Homebrewgithub的响应速度太慢，请使用以下命令：/bin/zsh -c &quot;$(curl -fsSL https://gitee.com/cunkai/HomebrewCN/raw/master/Homebrew.sh)&quot;。Mac build web pages之后按照README的方式运行即可。血的教训：先commit再deploy！！发现改动没了，整个人都麻了。trouble shooting (11.08 update)在本机启动了另一份jekyll后ruby开始疯狂报错，用了stackoverflow上的各种方法都没用，几个gem的版本换了好几个也于事无补。切到windows上也是同样的问题，抓狂。终于，重新装了个2.7的ruby，好了。brew reinstall ruby@2.7之后的改造计划需要一个放个人简历的页面（暂定用about页面）。边距等等的设定也需要调整。希望有个侧边栏、有检索功能，并且有瀑布流式的portfolio展示页面。需要研究一下纯音频的播放和文件的下载。" } ]
