---
title: Reinforcement Learning Part 1
intro: David Silver的RL课程学习笔记。大量参考了https://www.zhihu.com/column/reinforce中的内容。
author: Sapio-S
date: 2021-11-21
categories: [笔记, RL]
tags: [RL,笔记,技术]
math: true
---

## 第一讲：基本概念

### 个体和环境

t时刻，个体observation $$O_t$$，action $$A_t$$，收到环境的reward $$R_{t+1}$$。环境接收个体的$$A_t$$，给出$$O_{t+1}$$与$$R_{t+1}$$。

### History & State

- History: $$H_t=O_1,R_1,R_1,...,O_t,R_t,A_t$$
- State: $$S_t=f(H_t)$$。决定将来的已知信息，与历史有关。

- Markov Property：A state $$S_t$$ is Markov if and only if $$P[S_{t+1}\|S_t]=P[S_{t+1}\|S_1,S_2,...,S_t]$$。历史信息不起作用，仅当前状态的信息足够推断将来。

- Fully Observable Environments：即Markov Decision Process（MDP）。个体状态与环境状态一致。
- Partially Observable Environments：个体状态 != 环境状态。个体需要考虑如何给出状态。
  - 记住完整历史, $$S^a_t=H_t$$。
  - Beliefs of environment state, $$S^a_t=(P[S^e_t=s^1],...,P[S^e_t=s^n])$$。用已知状态的概率分布表示当前时刻的状态。
  - RNN自动给出状态，$$S^a_t= \sigma (S^a_{t-1}W_s+O_tW_o)$$。

### 个体的三个组成部分

- policy：$$\pi$$，从状态到行为的概率分布。给定状态s，$$\pi(a\|s)$$为个体采取行为a的概率，即$$\pi(a\|s)=P[A_t=a\|S_t=s]$$。策略仅和当前的状态有关，与历史信息无关；某一确定的Policy是静态的，与时间无关，但是个体可以随着时间更新策略。
- Value Function：对未来将来的预测，评价当前**状态**（不是策略）的好坏。注意，价值函数基于特定的策略，不同策略下同一状态的价值可能不同。某一策略下的价值函数为$$v_\pi(s)=E_\pi[R_{t+1}+\gamma R_{t+2}+\gamma ^2 R_{t+3}+...\|S_t=s]$$。
- model：个体对环境的建模，试图预测环境给出的状态和奖励。做出决策后，抵达下一个特定状态的概率为$$P^a_{ss'} = P[S_{t+1}=s'\|S_t=s,A_t=a]$$，可能收获的奖励为$$R^a_s=E[R_{t+1}\|S_t=s,A_t=a]$$。

### 个体的分类

第一种划分方式：

- value based：无policy，有value function。
- policy based：有policy，无value function。
- Actor-Critic：有policy，有value function。

第二种划分方式：

- model-based：个体尝试建立一个描述环境运作过程的模型，以此来指导价值或策略函数的更新。
- 非model-based：个体不试图了解环境如何工作。

### learning & planning

- learning：环境未知。
- planning：环境近似已知或已知，个体不与环境发生交互，而是利用其构建的模型进行计算，在此基础上改善行为策略。

一个常用的强化学习问题解决思路是，先学习环境如何工作，即学习得到一个模型，然后利用这个模型进行规划。

### prediction & control

- prediction：给定一个策略，评价未来。可以看成求解给定策略下的value function。
- control：找到一个好的策略，使得奖励最大化。



## 第二讲：MDP

### Markov Property / Markov Chain

无记忆的随机过程，以$$<S,P>$$表示，其中S是有限数量的状态集，P是状态转移概率矩阵（定义了所有状态的转移概率）。

$$
P=
 \left[
 \begin{matrix}
   P_{11} & ...& P_{1n} \\
   ... & & ... \\
   P_{n1} & ...& P_{nn}
  \end{matrix}
  \right]
$$

### Markov Reward Process

在上一基础上增加了奖励函数与衰减系数，以$$<S,P,R,\gamma>$$表示。

- 奖励函数$$R$$：给定状态(s)，时间(t)，下一时刻(t+1)可以获得的奖励期望，$$R_s=E[R_{t+1}\|S_t=s]$$。
- 衰减系数 Discount Factor $$\gamma$$：在0到1之间。
- 收获/收益/回报 return $$G_t$$：在Markov Reward Process上，从t时刻开始往后所有的奖励的总和，$$G_t=R_{t+1}+\gamma R_{t+2}+\gamma ^2 R_{t+3}+...$$。$$γ$$接近0，则表明趋向于“近视”性评估；$$γ$$接近1则表明偏重考虑远期的利益。
- value function：给定状态(s)，时间(t)，从该状态开始return的期望，即$$v(s)=E[G_t\|S_t=s]$$。给出某一状态的长期价值。

各状态价值的确定是很重要的，RL的许多问题可以归结为求状态的价值问题。因此如何求解各状态的价值，也就是寻找一个价值函数（从状态到价值的映射）就变得很重要了。

### 价值函数的求解 / Bellman方程

针对MRP的价值函数方程：$$v(s)=E[R_{t+1}+\gamma v(S_{t+1})\|S_t=s]$$。进一步地，根据t+1时刻状态的概率分布，可以得到其期望，其中，$$s'$$表示t+1时刻，s状态后继可能的状态：
$$
v(s)=R_s+\gamma \sum_{s'\in S}{P_{ss'}v(s')}
$$
这就是针对MRP的Bellman方程。写成矩阵形式，为$$v=R+\gamma Pv$$。这个方程显然可以直接求解，$$v=(1-\gamma P)^{-1}R$$，但是复杂度为$$O(n^3)$$，不适用于大规模的问题。

### Markov Decision Process

在MRP基础上引入行为集合$$A$$，以$$<S,P,R,\gamma,A>$$表示。注意，这里的$$P$$和$$R$$需要进一步与具体的**行为**$$a$$对应，而不像MRP那样仅对应于某个状态。修正后，得到第一讲中出现的公式。

$$
P^a_{ss'} = P[S_{t+1}=s'|S_t=s,A_t=a] \\
R^a_s=E[R_{t+1}|S_t=s,A_t=a]
$$

### MDP + policy

给定一个MDP $$M=<S,P,R,\gamma,A>$$与一个策略$$\pi$$，那么：状态序列$$S_1,S_2,...$$是一个马尔科夫过程$$<S,P^\pi>$$；状态和奖励序列$$S_1,R_2,S_2,R_3,S_3,...$$ 是一个马尔科夫奖励过程$$<S,P^\pi,R^\pi,\gamma>$$。其中，转移状态满足$$P^\pi_{s,s'}=\sum_{a\in A}{\pi(a\|s)P^a_{ss'}}$$，奖励函数满足$$R^\pi_{s}=\sum_{a\in A}{\pi(a\|s)R^a_{s}}$$。

> policy描述了agent采取各行为的概率。policy使得agent“做出选择”，进而形成Markov Chain与MRP。agent选择的不同行为产生了不同过程，其差别在于有后续状态不同，对应奖励也不同。

> 策略是静态的、关于整体的概念，不随状态改变而改变；变化的是在某一个状态时，依据策略可能产生的具体行为。具体的行为是有一定的概率的，策略就是用来描述各个不同状态下执行各个不同行为的概率。

- 状态价值函数：给定状态$$s$$与policy $$\pi$$，此时的价值函数为**状态价值函数**，记为$$v_\pi(s)$$，表示从状态$$s$$开始，遵循当前策略可获得的return的期望。$$v_\pi(s)=E_\pi[G_t\|S_t=s]$$。
- 行为价值函数：给定状态$$s$$与policy $$\pi$$，执行具体行为$$a$$时可以得到的return的期望，一般使用状态行为对进行描述，$$q_\pi(s,a)=E_\pi[G_t\|S_t=s,A_t=a]$$。

### Bellman Expectation Equation

在上述定义的基础上，可以得到MDP的两种价值函数方程，基本与MRP的价值函数方程相同，仅下标不同。

$$
v_\pi(s)=E_\pi[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s]\\
q_\pi(s,a)=E_\pi[R_{t+1}+\gamma q_\pi(S_{t+1},A_{t+1})|S_t=s,A_t=a]
$$

$$v_\pi(s)$$，$$q_\pi(s,a)$$的关系如下。式（1）表明，给定状态$$s$$与policy $$\pi$$，则$$s$$的价值为遵循$$\pi$$采取的全部可能行为的期望（用行为的价值与行为的概率相乘再求和）。式（2）表明，给定状态$$s$$与policy $$\pi$$，行为$$a$$的价值可以分为两部分：其一是离开这个状态的价值，其二是下一步状态价值的期望（用状态的价值与状态的概率相乘再求和）。

$$
v_\pi(s)=\sum_{a\in A}{\pi(a|s)q_\pi(s,a)} \tag{1}
$$

$$
q_\pi(s,a)=R^a_s+\gamma \sum_{s'\in S}{P^a_{ss'}v_\pi(s')} \tag{2}
$$

综合上述公式，简单代入，就可以推出MDP的Bellman Expectation Equation。这个方程可以用来求解$$v_\pi(s)$$，$$q_\pi(s,a)$$。

$$
v_\pi(s)=\sum_{a\in A}{\pi(a|s){(R^a_s+\gamma \sum_{s'\in S}{P^a_{ss'}v_\pi(s')})}} \\
q_\pi(s,a)=R^a_s+\gamma \sum_{s'\in S}{P^a_{ss'} \sum_{a'\in A}{\pi(a'|s')q_\pi(s',a')} }
$$

### Bellman Optimality Equation

1. 最优价值函数

   - 最优状态价值函数：$$v_*(s)=max_\pi v_\pi(s)$$，给定状态s，在所有可能的策略中，选择最大的状态价值函数。（reminder：一个策略对应一种行为的概率分布。）

   - 最优行为价值函数：$$q_*(s,a)=max_\pi q_\pi(s,a)$$，给定状态s和行为a，在所有可能的策略中，选择最大的行为价值函数。

   - 当我们知道了最优价值函数，也就知道了每个状态的最优价值，这时便认为这个MDP获得了解决。

   - 根据定义和式(1)(2)，我们可以建立起$$v_*(s)$$与$$q_*(s,a)$$的关系。直观上理解$$v_*(s)$$，给定状态s，其最优状态价值等于所有可能的后续行为a中，行为价值最大的值；$$q_*(s,a)$$可以理解为，给定状态s和行为a，此时最优行为价值由两个部分组成，一部分是离开状态 s 的即刻奖励，另一部分则是所有可能的后续状态(s')构成的期望。

     $$
     v_*(s)=max_a q_*(s,a) \\
     q_*(s,a)=R^a_s+\gamma \sum_{s'\in S}{P^a_{ss'} v_*(s')}
     $$

2. Bellman Optimality Equation

   - 迭代上面出现的两个式子，即可得到这组方程。它有两种表述方式。这组方程可以用来求解$$v_*(s)$$，$$q_*(s,a)$$，进一步地，可以计算出最优策略。
     
     $$
     v_*(s)=max_a (R^a_s+\gamma \sum_{s'\in S}{P^a_{ss'}v_*(s')}) \\
     q_*(s,a)=R^a_s+\gamma \sum_{s'\in S}{P^a_{ss'} max_{a'} q_*(s',a')}
     $$

   - 此方程是非线性的，没有快速便捷的解决方案，一般通过迭代解决。

3. 最优策略

   - policy $$\pi$$ 优于policy $$\pi'$$ ：$$\pi \geq \pi'$$，若$$v_\pi(s) \geq v_{\pi'}(s), \forall s$$

     > 定理：对于任何MDP，下面几点成立：
     >
     > 1. 存在一个最优策略，比任何其他策略更好或至少相等；
     > 2. 所有的最优策略有相同的最优价值函数；
     > 3. 所有的最优策略具有相同的行为价值函数。

4. 寻找最优策略

   - 通过最大化$$q_*(s,a) $$找最优策略。仅当$$a=argmax_{a\in A}{q_*(s,a)}$$（即，当前行为a是最优的行为）时，$$\pi_*{(a\|s)}=1$$，其他情况下，$$\pi_*{(a\|s)}=0$$。

   - 对于任何MDP问题，总存在一个确定性的最优策略；如果我们知道最优行为价值函数，那么我们就找到了最优策略。



## 第三讲：使用动态规划寻找最优策略

本讲中，我们假定模型已知。整体感觉像是用牛顿法求解方程组，方程就是上一讲推导得到的Bellman Expectation Equation与Bellman Optimality Equation。

### DP与MDP的可比性

- DP：动态规划算法是解决复杂问题的一个方法，算法通过把复杂问题分解为子问题，通过求解子问题进而得到整个问题的解。在解决子问题的时候，其结果通常需要存储起来被用来解决后续复杂问题。当问题具有下列特性时，通常可以考虑使用动态规划来求解：第一个特性是一个复杂问题的最优解由数个小问题的最优解构成，可以通过寻找子问题的最优解来得到复杂问题的最优解；子问题在复杂问题内重复出现，使得子问题的解可以被存储起来重复利用。
- MDP：Bellman方程把问题递归为求解子问题，价值函数就相当于存储了一些子问题的解，可以复用。因此可以使用动态规划来求解MDP。

### 两种应用

- 预测：给定一个MDP$$<S,P,R,\gamma,A>$$与一个策略$$\pi$$，或者给定一个MRP  ，$$<S,P^\pi,R^\pi,\gamma>$$，求价值函数$$v_\pi$$。
- 控制：给定一个MDP$$<S,P,R,\gamma,A>$$，求最优价值函数$$v_*$$和最优策略$$\pi_*$$。

### Iterative Policy Evaluation

通过反向迭代Bellman Expectation Equation，解决预测问题。有点像是用牛顿迭代法解Bellman Expectation Equation（参考《数值分析》一课）。

矩阵形式为$$v^{k+1}=R^\pi+\gamma P^\pi v^k$$，k代表第k次迭代。展开后具体公式为$$v_{k+1}(s)=\sum_{a\in A}{\pi(a\|s)(R^a_s+\gamma \sum_{s' \in S}{P^a_{ss'}{v_k(s')}})}$$。

上式迭代至收敛后，可以得到价值函数。接下来，我们可以据此优化策略，在当前策略的基础上，贪婪地选取行为，使得后继状态价值增加，$$\pi'=greedy(v_\pi)$$。这样一来，我们便得到了Policy Iteration。

### Policy Iteration

分为policy evalution & policy improvement两步。为当前策略迭代计算$$v_\pi$$，之后根据$$v_\pi$$贪婪地更新策略，反复操作，即可得到最优价值函数$$v_*$$和最优策略$$\pi_*$$。

贪婪：仅采取那个/那些使得状态价值得到最大的行为。$$\pi'(s)=argmax_{a \in A}{q_\pi(s,a)}$$。

可以证明，这样迭代得到的最优价值函数$$v_*$$和最优策略$$\pi_*$$满足Bellman Optimality Equation，证明过程略。

- Modified Policy Iteration：有时候不需要持续迭代至解出最优价值函数，可以设置一些条件提前终止迭代，比如设定一个Ɛ，比较两次迭代的价值函数平方差；直接设置迭代次数；以及每迭代一次更新一次策略等。

### Value Iteration

- Principle of Optimality：状态s下的最优策略可以被分解为两部分：从状态s到下一个状态s’采取了最优行为；之后，在状态s’时，遵循s’状态下的最优策略。即，$$v_*(s) \leftarrow max_a (R^a_s+\gamma \sum_{s'\in S}{P^a_{ss'}v_*(s')})$$。
- Deterministic Value Iteration：已知最终状态与状态之间的关系，可以直接反推。
- Value Iteration：不确定最终状态在哪里，根据**每一个**状态后续的最优状态价值来更新该状态的最佳状态价值。具体方法为，迭代求解Bellman Optimality Equation。这个方法可以找到最优策略$$\pi_*$$，但是迭代过程中不给出明确的策略，迭代期间得到的价值函数与策略无关。

用一张PPT总结。



## 第四讲：不基于模型的预测

Agent需要与环境交互，得到一个estimate 最优价值函数和最优策略。本讲主要解决策略评估（即预测）的问题。

新的概念：**episode**。从某一个状态开始，Agent与Environment交互直到**终止状态**，环境给出终止状态的即时收获为止。完整的Episode不要求起始状态一定是某一个特定的状态，但是要求个体最终进入环境认可的某一个终止状态。

### Monte-Carlo Reinforcement Learning

在不清楚MDP状态转移（即模型，上文的$$P^a_{ss'}$$）与即时奖励（即$$R^a_s$$）的情况下，直接从经历过的完整Episode中学习状态价值。通常情况下，某状态的价值等于在多个Episode中，以该状态算得到的所有收获的平均。注：收获不是针对Episode的，它存在于Episode内，针对Episode中某一个状态。从这个状态开始经历完Episode时得到的有衰减的即时奖励的总和。从一个Episode中，我们可以得到该Episode内所有状态的收获。当一个状态在Episode内出现多次，该状态的收获有不同的计算方法，下文会讲到。

蒙特卡洛强化学习有如下特点：不基于模型本身，直接从经历过的Episode中学习，必须是**完整的Episode**，使用的思想就是用平均收获值代替价值。理论上Episode越多，结果越准确。

### Monte-Carlo Policy Evaluation

在给定策略下，从一系列的完整Episode经历中学习得到该策略下的状态价值函数。在解决问题过程中主要使用的信息是一系列完整Episode。其包含的信息有：状态的转移、使用的行为序列、中间状态获得的即时奖励以及到达终止状态时获得的即时奖励。其特点是使用有限的、完整Episode产生的这些经验性信息经验性地推导出每个状态的平均收获，以此来替代收获的期望（即状态价值）。通常需要掌握完整的MDP信息才能准确计算得到。

在状态转移过程中，可能发生一个状态经过一定的转移后又一次或多次返回该状态，此时在一个Episode里如何计算这个状态发生的次数和计算该Episode的收获呢？可以有如下两种方法：

- 首次访问蒙特卡洛策略评估

  在给定一个策略，使用一系列完整Episode评估某一个状态s时，对于每一个Episode，仅当该状态**第一次**出现时列入计算。

- 每次访问蒙特卡洛策略评估

  在给定一个策略，使用一系列完整Episode评估某一个状态s时，对于每一个Episode，状态s**每次**出现在状态转移链时均列入计算。

对状态s的更新计算如下。当$$N(s) \rightarrow \infty$$时，$$V(s) \rightarrow v_\pi(s)$$。

- 状态出现次数+1：$$N(s) \leftarrow N(s)+1$$
- 总收获值更新：$$S(s) \leftarrow S(s)+G_t$$
- 状态s的价值更新：$$V(s)=S(s)/N(s)$$

某种角度上来说和蒙特卡洛树搜索有点类似。

一种优化方式为蒙特卡洛累进更新，它利用了Incremental Mean（即$$\mu_k=\mu_{k-1}+1/k*(x_l-\mu_{k-1})$$这一公式）。这样一来，状态s的更新计算可以丢掉过去的episode信息，节省空间。新的价值更新方式为$$V(S_t)\leftarrow V(S_t)+1/{N(S_t)}*(G_t-V(S_t))$$。

### Temporal-Difference Learning / TD学习

可以学习**不完整**的Episode，通过bootstrapping(TD目标值代替收获$$G_t$$)，猜测Episode的结果，同时持续更新这个猜测。算法在估计某一个状态的价值时，用的是离开该状态的即刻奖励$$R_{t+1}$$ 与下一状态$$S_{t+1}$$的预估状态价值乘以衰减系数$\gamma$组成，与Bellman方程相符。$$V(S_t)\leftarrow V(S_t)+\alpha(R_{t+1}+\gamma V(S_{t+1})-V(S_t))$$。其中，$$R_{t+1}+\gamma V(S_{t+1})$$为**目标值/TD目标**，$$\delta _t=R_{t+1}+\gamma V(S_{t+1})-V(S_t)$$为**TD误差**。True TD target为$$R_{t+1}+\gamma v_\pi(S_{t+1})$$。

| MC                                           | TD                                                  |
| -------------------------------------------- | --------------------------------------------------- |
| episode结束后才可以学习                      | episode中间可以学习，不需要等到结果                 |
| 使用$$G_t$$ ，对状态价值的估计无偏，variance高 | TD target有偏，True TD target无偏                   |
| 在非Markov环境下更有效                       | 利用了MDP的Markov性                                 |
| result：状态价值与实际收获的均方差最小       | 估计状态间的转移概率&状态的即时奖励，再计算状态函数 |

### TD(λ)

上述算法为TD(0)，在当前状态下往前多看1步。下面考虑多看n步的情况。（MC属于∞-步预测）

- **n-步预测**：在当前状态往前行动n步，计算n步的return。同样TD target 也由2部分组成，已走的步数使用确定的即时reward，剩下的使用估计的状态价值替代。n-步收获为$$G^{(n)}_t=R_{t+1}+\gamma R_{t+2}+...+\gamma ^n V(S_{t+n})$$，状态价值函数的更新为$$V(S_t) \leftarrow V(S_t)+\alpha (G^{(n)}_t-V(S_t))$$。

- **λ-收获**：为了找到合适的n，我们可以通过λ-收获，在不增加计算复杂度的情况下，综合考虑1到无穷步数的预测。其基本思路为，给n-步收获增加一个权重$$(1-\lambda)\lambda^{n-1}$$。这样，收获变为$$G^\lambda_t=(1-\lambda) \sum \lambda^{n-1}G^{(n)}_t$$，对应的TD(λ)预测下，状态价值函数的更新为$$V(S_t) \leftarrow V(S_t)+\alpha (G^\lambda_t-V(S_t))$$。

几个提示：

- 必须走完整个Episode，获得每一个状态的即时奖励以及最终状态的即时奖励，才可以更新一个状态的状态价值。
- 当λ=1时，off-line时对应MC算法。（online时，状态价值差每一步都会有积累，不完全等同）
- 当λ=0时，只有当前状态得到更新，等同于TD(0)算法。

### 总结

本讲的三个算法可以对比如下。



## 第五讲：不基于模型的控制

通过本讲的学习，我们将会学习到如何训练一个Agent，使其能够在完全未知的环境下较好地完成任务，得到尽可能多的奖励。

优化控制分两类：

- **on-policy learning**：个体已有一个策略，并且遵循这个策略进行采样，或者说采取一系列该策略下产生的行为，根据这一系列行为得到的奖励，更新状态函数，最后根据该更新的价值函数来优化策略得到较优的策略。要优化的策略就是当前遵循的策略。
- **off-policy learning**：虽然个体有一个自己的策略，但是个体并不针对这个策略进行采样，而是基于另一个策略进行采样，这另一个策略可以是先前学习到的策略，也可以是人类的策略等一些较为优化成熟的策略，通过观察基于这类策略的行为，或者说通过对这类策略进行采样，得到这类策略下的各种行为，继而得到一些奖励，然后更新价值函数，即**在自己的策略形成的价值函数的基础上观察别的策略产生的行为**，以此达到学习的目的。这种学习方式类似于“站在别人的肩膀上可以看得更远”。这些策略是已有的策略。

### On-Policy Monte-Carlo Control

使用Ｑ函数进行策略评估，使用Ɛ-贪婪探索来改善策略。该方法最终可以收敛至最优策略。我们一般在经历了多个Episode之后才进行依次Ｑ函数更新或策略改善。

- 由于模型未知，我们使用状态行为对下的价值$$Q(s,a)$$代替状态价值，即$$\pi'(s)=argmax_{a \in A}{Q(s,a)}$$。这样，我们可以改善策略而不用知道整个模型，只需要知道在某个状态下采取什么行为价值最大即可。

- 为达到足够的exploration，我们需要引入一个随机机制，以一定的概率选择当前最好的策略，同时给以其它可能的行为一定的几率，这就是**Ɛ-贪婪探索**。$$1-\epsilon$$的概率选择当前认为最好的行为，$$\epsilon$$在所有可能的行为中选择（也包括那个当前最好的行为）。

  > 定理：使用Ɛ-贪婪探索策略，对于任意一个给定的策略$$\pi$$，我们在评估这个策略的同时也总在改善它。

- **GLIE**(Greedy in the Limit with Infinite Exploration)，在有限的时间内进行无限可能的探索。具体表现为：所有已经经历的状态行为对（state-action pair）会被无限次探索；另外随着探索的无限延伸，贪婪算法中Ɛ值趋向于０。例如如果我们取$$\epsilon =1/k$$（$$k$$为探索的Episode数目)，那么该Ɛ贪婪蒙特卡洛控制就具备GLIE特性。

  > 定理：GLIE MC control 能收敛至最优的状态行为价值函数。

算法的具体流程如下：

1. 对于给定策略$$\pi$$，采样第$$k$$个episode，$${S_1,A_1,R_2,...,S_T}$$。
2. 对于该episode中出现的每一个状态行为对$$S_t, A_t$$，更新计数和Q函数。$$N(S_t,A_t) \leftarrow N(S_t,A_t) +1$$，$$Q(S_t,A_t) +=1/N(S_t,A_t) *(G_t-Q(S_t,A_t))  $$。
3. 基于新的Ｑ函数改善策略。$$\epsilon \leftarrow 1/k$$，$$\pi \leftarrow \epsilon - greedy(Q)$$。

### On-Policy Temporal-Difference Control / SARSA

类似上一讲中的TD算法，在单个episode内，每一次agent采取一个行为后都要更新Q值。给定$$S$$，$$A$$，与环境交互，得到及时奖励$$R$$，进入后续状态$$S'$$，agent遵循现有策略产生行为$$A'$$（但是不执行$$A'$$，留到下一个循环再执行），根据当前状态行为价值函数得到$$(S', A')$$的价值$$Q$$，利用$$Q$$更新$$(S,A)$$的价值。

SARSA同样使用Ɛ-贪婪探索改善策略。具体的算法流程为：

for each episode:

1. initialize $$S$$
2. choose $$ A$$ from $$S$$ using policy derived from $$Q$$ (e.g. Ɛ-greedy)
3. for each step of episode:
   1. take action $$A$$, observe $$R$$, $$S'$$
   2. choose $$A'$$ from $$S'$$ using policy derived from $$Q$$  (e.g. Ɛ-greedy)
   3. $$Q(S,A)\leftarrow Q(S,A)+\alpha (R+\gamma Q(S',A')-Q(S,A))$$;
   4. $$S \leftarrow S'$$; $$A \leftarrow A'$$;

注：算法中的$$Q(S,A)$$是以一张大表存储的，这不适用于解决规模很大的问题；

> 定理：满足如下两个条件时，SARSA算法将收敛至最优行为价值函数。
>
> 条件1：任何时候的policy $$\pi_t(a\|s)$$符合GLIE。
>
> 条件2：步长满足$$\sum a_t=\infty$$且$$\sum a_t^2 < \infty$$。

### SARSA(λ)

- **n-step Q-return**：$$q^{(n)}_t=R_{t+1}+\gamma R_{t+2}+...+\gamma^{n-1}R_{t+n}+\gamma^nQ(S_{t+n})$$。注意，这里的$$q_t$$对应一个状态行为对$$(s_t,a_t)$$，表示在状态$$s_t$$下采取行为$$a_t$$的价值大小。如果$$n=\infty$$，则表示一直用即时奖励计算$$Q$$，直至Episode结束，个体进入终止状态，获得终止状态的即时奖励。
- **SARSA(λ)**：给每一个n-step Q-return分配一个权重，得到$$q^\lambda_t=(1-\lambda)\sum \lambda^{n-1}q^{(n)}_t$$。更新Q的公式变为$$Q(S_t,A_t)\leftarrow Q(S_t,A_t)+\alpha (q^\lambda_t-Q(S_t,A_t))$$。
- **Eligibility Trace**：它体现的是一个结果与某一个状态行为对的因果关系。与得到结果最近的状态行为对 & 那些在此之前频繁发生的状态行为对，对得到这个结果的影响最大。计算公式为$$E_0(s,a)=0$$，$$E_t(s,a)= \gamma \lambda E_{t-1}(s,a)+1(S_t=s,A_t=a)$$。每浏览完一个Episode后，ET需要重新置0，因为ET仅在一个Episode中发挥作用。

具体的算法流程为：

for each episode:

1.  $$E(s,a)=0$$, for all $$s \in S, a \in A(s)$$
2. initialize $$S,A$$
3. for each step of episode:
   1. take action $$A$$, observe $$R$$, $$S'$$
   2. choose $$A'$$ from $$S'$$ using policy derived from $$Q$$  (e.g. Ɛ-greedy)
   3. $$\delta \leftarrow R+\gamma Q(S',A')-Q(S,A)$$;
   4. $$E(S,A) \leftarrow E(S,A)+1$$;
   5. for all $$s \in S, a \in A(s)$$:
      1. $$Q(S,A)\leftarrow Q(S,A)+\alpha \delta E(s,a)$$;
      2. $$E(s,a) \leftarrow \gamma \lambda E(s,a)$$;
   6. $$S \leftarrow S'$$; $$A \leftarrow A'$$;

注：算法更新Q和E的时候针对的不是某个Episode里的$$Q$$或$$E$$，而是针对个体掌握的整个状态空间和行为空间产生的$$Q$$和$$E$$。个体每得到一个即时奖励，同时会对所有历史事件（即状态行为对，在同一状态采取不同行为是不同的事件）的价值进行依次更新，当然那些与该事件关系紧密的事件价值改变的较为明显。

### Off-policy TD learning / Q-learning

使用TD方法在遵循一个策略$$\mu (a\|s)$$的同时评估另一个策略$$\pi (a\|s)$$。

定理：Q-learning将收敛至最优状态行为价值函数：$$Q(s,a) \rightarrow q_*(s,a)$$。
