---
title: RL基础（精简版）-part 1：介绍
intro: from David Silver's course. 根据自己的理解删掉了部分证明和案例，旨在保留基本脉络与逻辑。
author: Sapio-S
date: 2022-10-20
categories: [笔记, RL, RL基础]
tags: [RL,笔记,技术]
math: true
---


## Overview

RL并不要求一定有神经网络的参与，比如传统RL就可以视作一种纯粹的计算问题，至多是用了不同的近似方法去求解奖励，或者梯度式更新某一参数。但是随着问题越来越复杂，我们不得不开始考虑使用更强大的方式优化运算，于是出现了神经网络，出现了现在的Deep RL。

我按照自己的理解重新整理了David Silver在UCL讲授的RL内容。简单来说，前五讲解决的是小model的问题，后五讲解决的是大model的问题。对于小model来说，若model已知，那么就可以直接用规划的方式得到最优解；若model未知，那么我们就需要使用model-free的方式，从prediction和control两个角度切入求解。对于大model来说，我们已经不能遍历全部的动作/状态空间，因此只能近似求解RL问题中最重要的三个组件：value，policy和model。

## 基本概念

### agent & environment

t时刻，个体(agent)观测到observation $O_t$，执行action $A_t$，收到环境(environment)的reward $R_{t+1}$。环境接收个体的$A_t$，给出$O_{t+1}$与$R_{t+1}$。

### History & State

- History: $H_t=O_1,R_1,R_1,...,O_t,R_t,A_t$

- State: $S_t=f(H_t)$。决定将来的已知信息，与历史有关。

- Markov Property：A state $S_t$ is Markov if and only if $P[S_{t+1}|S_t]=P[S_{t+1}|S_1,S_2,...,S_t]$。历史信息不起作用，仅当前状态的信息足够推断将来。

- Fully Observable Environments：即Markov Decision Process（MDP）。个体状态与环境状态一致。

- Partially Observable Environments：个体状态 != 环境状态。个体需要考虑如何给出状态。

### 个体的三个组成部分

- policy：$\pi$，从状态到行为的概率分布。给定状态s，$\pi(a|s)$为个体采取行为a的概率，即$\pi(a|s)=P[A_t=a|S_t=s]$。策略仅和当前的状态有关，与历史信息无关；某一确定的Policy是静态的，与时间无关，但是个体可以随着时间更新策略。

- Value Function：对未来将来的预测，评价当前**状态**（不是策略）的好坏。注意，价值函数基于特定的策略，不同策略下同一状态的价值可能不同。某一策略下的价值函数为$v_\pi(s)=E_\pi[R_{t+1}+\gamma R_{t+2}+\gamma ^2 R_{t+3}+...|S_t=s]$。

- model：个体对环境的建模，试图预测环境给出的状态和奖励。做出决策后，抵达下一个特定状态的概率为$P^a_{ss'} = P[S_{t+1}=s'|S_t=s,A_t=a]$，可能收获的奖励为$R^a_s=E[R_{t+1}|S_t=s,A_t=a]$。

## Markov Decision Processes

MDP通常表示为 $M=<S,P,R,\gamma,A>$。其中各项的含义如下：

- 状态集S：所有可能状态的集合。

- 状态转移概率矩阵P：表示所有状态的转移概率。$P^a_{ss'} = P[S_{t+1}=s'|S_t=s,A_t=a]$

- 奖励函数$R$：给定状态(s)，时间(t)，下一时刻(t+1)可以获得的奖励期望，$R_s=E[R_{t+1}|S_t=s]$。

- 衰减系数 Discount Factor $\gamma$：在0到1之间。值越大，意味着agent越倾向于考虑长期的利益；值越小，agent越倾向于考虑短期的利益。

- 收获/收益/回报 return $G_t$：在Markov Reward Process上，从t时刻开始往后所有的奖励的总和，$G_t=R_{t+1}+\gamma R_{t+2}+\gamma ^2 R_{t+3}+...$。

- value function：给定状态(s)，时间(t)，从该状态开始return的期望，即$v(s)=E[G_t|S_t=s]$。给出某一状态的**长期价值**。

对于个体而言，我们可以定义策略policy和两种价值函数value function：

- policy：在特定状态下，agent采取各行为的概率。一般用$\pi(a|s)$表示。

- 状态价值函数V：给定状态$s$与policy $\pi$，此时的价值函数为**状态价值函数**，记为$v_\pi(s)$，表示从状态$s$开始，遵循当前策略可获得的return的期望。$v_\pi(s)=E_\pi[G_t|S_t=s]$。

- 行为价值函数Q：给定状态$s$与policy $\pi$，执行具体行为$a$时可以得到的return的期望，一般使用状态行为对进行描述，$q_\pi(s,a)=E_\pi[G_t|S_t=s,A_t=a]$。

## Bellman Equation

### Bellman Expectation Equation

在上述定义的基础上，可以得到MDP的两种价值函数方程，基本与MRP的价值函数方程相同，仅下标不同。
$$
v_\pi(s)=E_\pi[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s]\\
q_\pi(s,a)=E_\pi[R_{t+1}+\gamma q_\pi(S_{t+1},A_{t+1})|S_t=s,A_t=a]
$$

简单推导，就可以得到Bellman Expectation Equation。这个方程可以用来求解$v_\pi(s)$，$q_\pi(s,a)$。

$$
v_\pi(s)=\sum_{a\in A}{\pi(a|s){(R^a_s+\gamma \sum_{s'\in S}{P^a_{ss'}v_\pi(s')})}} \\
q_\pi(s,a)=R^a_s+\gamma \sum_{s'\in S}{P^a_{ss'} \sum_{a'\in A}{\pi(a'|s')q_\pi(s',a')} }
$$

简单来说，Bellman Expectation Equation可以记作$v=R+\gamma Pv$。

### Bellman Optimality Equation

#### 最优价值函数

- 最优状态价值函数：$v_*(s)=max_\pi v_\pi(s)$，给定状态s，在所有可能的策略中，选择最大的状态价值函数。（reminder：一个策略对应一种行为的概率分布。）

- 最优行为价值函数：$q_*(s,a)=max_\pi q_\pi(s,a)$，给定状态s和行为a，在所有可能的策略中，选择最大的行为价值函数。

- 根据定义和式(1)(2)，我们可以建立起$v_*(s)$与$q_*(s,a)$的关系。
  
  $$
  v_*(s)=max_a q_*(s,a) \\
q_*(s,a)=R^a_s+\gamma \sum_{s'\in S}{P^a_{ss'} v_*(s')}
  $$

#### Bellman Optimality Equation

- 迭代上面出现的两个式子，即可得到这组方程。它有两种表述方式。这组方程可以用来求解$v_*(s)$，$q_*(s,a)$，进一步地，可以计算出最优策略。
  
  $$
  v_*(s)=max_a (R^a_s+\gamma \sum_{s'\in S}{P^a_{ss'}v_*(s')}) \\
q_*(s,a)=R^a_s+\gamma \sum_{s'\in S}{P^a_{ss'} max_{a'} q_*(s',a')}
  $$

- 此方程是非线性的，没有快速便捷的解决方案，一般通过迭代解决。

## 小结

之后的全部内容基本都是在讲如何求解Bellman Equation。只要解出了Bellman Equation，我们就可以认为得到了最优策略。

- policy $\pi$ 优于policy $\pi'$ ：$\pi \geq \pi'$，若$v_\pi(s) \geq v_{\pi'}(s), \forall s$
  
  > 定理：对于任何MDP，下面几点成立：
  > 
  > 1. 存在一个最优策略；
  > 2. 所有的最优策略有相同的最优价值函数；
  > 3. 所有的最优策略具有相同的行为价值函数。

- 通过最大化$q_*(s,a)$找最优策略。仅当$a=argmax_{a\in A}{q_*(s,a)}$（即，当前行为a是最优的行为）时，$\pi_*{(a|s)}=1$，其他情况下，$\pi_*{(a|s)}=0$。

- 对于任何MDP问题，总存在一个确定性的最优策略；如果我们知道最优行为价值函数，那么我们就找到了最优策略。当我们知道了最优价值函数，也就知道了每个状态的最优价值，这时便认为这个MDP获得了解决。
