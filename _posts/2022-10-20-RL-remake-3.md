---
title: RL基础（精简版）part 3：大模型近似
intro: from David Silver's course. 根据自己的理解删掉了部分证明和案例，旨在保留基本脉络与逻辑。
author: Sapio-S
date: 2022-10-20
categories: [科研, RL基础]
tags: [RL,笔记,技术]
math: true
---

## Overview

当状态空间/动作空间非常大或者连续的时候，我们没有办法遍历所有的state/action，只能通过estimate近似求解。前面讲到的针对小model的RL方法都可以看作是通过lookup table实现的，即，对于每一个state或者action，我们都记录了一个对应的$V(s)$或者$Q(s,a)$；现在我们使用各种函数逼近function approximation（比如linear combination，神经网络，决策树等）替代之前的lookup table，预测没有见过的状态的价值。

## value-based: Value Function Approximation

我们希望agent可以从有限的经验中学习value function，预测没有见过的状态的价值。

普遍使用$w$表示价值函数的参数。

### Gradient Descent

#### Gradient Descent快速回顾

假设我们的目标函数是$J(w)$，其梯度是$\nabla_wJ(w)$。为了求$J(w)$的（局部）最小值，我们只需要沿着这个方向更新$J(w)$的值：$\Delta w=-frac{1}{2}\alpha \nabla_wJ(w)$。

#### Prediction: 逼近V

目标是$\hat v(s,w) \approx v_\pi(s)$。此时可知：

- 目标函数：$J(w)=E_\pi[(v_\pi(S)-\hat v(S,w))^2]$，即均方差MSE的期望。

- 梯度：$\Delta w=\alpha(v_\pi(S)-\hat v(S,w)) \nabla_w \hat v(S,w)$。这里使用单样本对gradient进行了采样，所以不用求期望。

使用一组线性函数(Linear Function Approximation)近似，即，令$\hat v(S,w)=x(S)^Tw=\sum x_j(S)w_j$。这种情况下，$\nabla_w \hat v(S,w)=x(S)$，因此梯度可表示为$\Delta w=\alpha(v_\pi(S)-\hat v(S,w)) x(S)$。但是，在训练时我们得不到价值函数的真值$v_\pi (S)$，需要近似表示：

- MC: $G_t$。此时梯度为$\Delta w=\alpha(G_t-\hat v(S_t,w)) x(S_t)$。收敛至局部最优。

- TD(0): $R_{t+1}+\gamma \hat v(S_{t+1},w)$。收敛至全局最优。

- TD(λ): $G^\lambda _t$。收敛至全局最优。

#### Control: 逼近Q

在policy evaluation时预估Q，之后使用$\epsilon$-greedy完成policy improvement。

拟合的思路与上一小节基本完全一致。目标是$\hat v(s,w) \approx v_\pi(s)$，当使用Linear Function近似时，令$\hat v(S,w)=x(S)^Tw=\sum x_j(S)w_j$，有：

- MC: $G_t$。此时梯度为$\Delta w=\alpha(G_t-\hat v(S_t,w)) x(S_t)$。

- TD(0): $R_{t+1}+\gamma \hat v(S_{t+1},w)$。此时梯度为$\Delta w=\alpha \delta x(S)$。

- TD(λ): $G^\lambda _t$。此时梯度为$\Delta w=\alpha \delta_t E_t$。

### batch RL

Gradient Descent使用的是流式的数据，更新参数后sample即废弃。更加sample efficient的方式是收集一段时间内的数据，直接在这个数据集上拟合。这时我们的优化目标转换为求Least Squares，即求$LS(w)=\sum (v_t^\pi-\hat v(s_t,w))^2$的最小值。令其导数为0，可以直接求出结果$w=(\sum x(s_t)x(s_t)^T)^{-1}\sum x(s_t)v_t^\pi$。同样的，这里的$v_t^\pi$在实际求解时未知，需要通过MC/TD等方式近似。

解决control问题时，我们使用least squares Q-learning进行policy evaluation，用greedy policy进行policy improvement。拟合Q时我们需要将上述的v换为Q。注意因为数据集包含多个历史policy的experience，所以必须off-policy式更新：

- 从旧policy的experience中取得sample$S_t, A_t, R_{t+1}, S_{t+1}$。

- 用新policy选择action$A'=\pi_{new}(S_{t+1})$。

- 更新：$\hat q(S_t,A_t,w) \leftarrow R_{t+1}+\gamma \hat q(S_{t+1},A',w)$。

【和Q-learning的关系？】

## policy-based: Policy Gradient

我们希望agent可以从有限的经验中学习policy。

直接学习policy的好处在于：更好的收敛性；可以有效用于高维/连续空间；可以学到随机策略（比如石头剪刀布场景下，随机策略才是最优解）。坏处在于：通常只能达到局部最优；很难直接评估策略。

普遍使用$\theta$表示policy的参数。

### 优化目标

我们可以用不同的方式衡量策略$\pi_\theta$的好坏，比如：

- start value: $J_1(\theta)=V^{\pi_theta}(s_1)$.

- average value: $J_{avV}(\theta)=\sum_{s} d^{\pi_theta}(s) V^{\pi_theta}(s)$.

- average reward per time-step: $J_{avV}(\theta)=\sum_{s} d^{\pi_theta}(s) \sum_a \pi_theta(s,a)R^a_s$.

我们希望最大化$J(\theta)$。有很多可选的方式，这里我们使用Gradient Descent，此时$\Delta \theta=\alpha \nabla_\theta J(\theta)$。

### policy gradient

#### Finite Differences

使用导数/微分的定义。对于$\theta$的每一维k，计算$J(\theta)$的偏导，从而得到近似的梯度：$\frac {\partial J(\theta)}{\partial \theta_k} \approx \frac {J(\theta+\epsilon u_k)-J(\theta)}{\epsilon}$。

#### score function

根据公式$d \ln(y)=\frac {dy}y$，可以得到$\nabla _\theta \pi_\theta(s,a)=\pi_\theta(s,a) \nabla_\theta \ln \pi_\theta(s,a)$。我们定义score function为$\nabla _\theta log\pi_\theta(s,a)$。score function其实就是gradient的估计器

score function是可以根据policy的类别直接计算的，比如：

- softmax policy：针对离散行为，将行为表示为$\phi(s,a)^T \theta$，则采取行为的概率为$\pi_\theta(s,a) \propto e^{\phi(s,a)^T\theta}$，此时score function为$\nabla _\theta \log \pi_\theta(s,a)=\phi(s,a)-E_{\pi\theta}[\phi(s,·)]$。

- Gauss policy：针对连续行为，将行为表示为$\phi(s)^T \theta$，采取行为的概率为$a \sim \mathcal{N}(\mu(s),\sigma^2)$，score function为$\nabla _\theta \log \pi_\theta(s,a)=\frac{(a-\mu(s))\phi(s)}{\sigma^2}$。

#### policy gradient theorem

For any differentiable policy $\pi_\theta(s,a)$, for any of the policy objective functions $J = J_1, J_{avR},\frac1{1-\gamma}J_{avV}$, the policy gradient is $\nabla_\theta J(\theta)=E_{\pi\theta}[\nabla_\theta \log \pi_\theta(s,a)Q^{\pi_\theta}(s,a)]$.

和之前一样，我们遇到了这个问题：$Q^{\pi_\theta}(s,a)]$未知，需要近似。想必大家都熟悉这个近似方式了。比如，用MC近似，则 可以视$v_t$为无偏样本，此时更新方式为$\theta \leftarrow \theta + \alpha \nabla_\theta \log  \pi_\theta(s_t,a_t)v_t$。

### value + policy: Actor-Critic

这一类算法需要同时预估policy和value，维护两组参数。其中，actor为policy，critic为value，用$Q_w(s,a)$估算$Q^{\pi_\theta}(s,a)$。此时，policy gradient的梯度可以写作$\nabla_\theta J(\theta)=E_{\pi\theta}[\nabla_\theta \log \pi_\theta(s,a)Q_w(s,a)]$。这样一来，我们可以降低policy gradient的variance。

critic的任务就是之前的policy evaluation。因此，我们可以用之前提过的任何方式更新critic和actor，把比如用TD(0)更新critic，用policy gradient更新actor。

#### advantage function

在不改变期望的情况下，我们可以引入baseline来降低算法的方差。常用的baseline就是state value function$B(s)=V^{\pi_\theta}(s)$。

我们定义advantage function为$A^{\pi_\theta}(s,a)=Q^{\pi_\theta}(s,a)-V^{\pi_\theta}(s)$，此时梯度变为$\nabla_\theta J(\theta)=E_{\pi\theta}[\nabla_\theta \log \pi_\theta(s,a)A^{\pi_\theta}(s,a)]$。

当然，$A^{\pi_\theta}(s,a)$的值我们得不到，所以同样使用近似：$A(s,a)=Q_w(s,a)-V_v(s)$。两个近似量可以用之前的任意方式求解。注意，由于$Q(s,a)=r+\gamma V(s)$，因此我们实际可以用一套参数更新，比如TD error是$\delta_v=r+\gamma V_v(s')-V_v(s)$。

### 总结

除了MC是policy-based之外，其余的算法都是Actor-Critic结构。

【TBC】

## model-based

agent从经验中学习model，自行预测环境可能给出的reward和状态转移方式，进而以此构建value function/policy。

具体来说，我们使用$\eta$表示model的参数，要预测的model即为$<P_\eta,R_\eta>$，从$(s,a)\rightarrow r$数据集中学习$R_\eta$是regression问题，从$(s,a)\rightarrow s'$数据集中学习$P_\eta$是density estimation问题，使用MSE，KL divergence等loss求解即可。model可以有table lookup（记录全部(s,a)对），Gaussian Process等多种形式。

有了这个model之后，我们就可以得到下一步的状态分布$S_{t+1}~P_\eta(S_{t+1}|S_t,A_t)$以及奖励分布$R_{t+1}~R_\eta(R_{t+1}|S_t,A_t)$。之后，我们根据这个model求解MDP。

一个典型的求解思路是sample-based planning。这意味着学习到的model只负责产生samples，得到samples之后使用model-free的方式求解MDP（可以选用MC control, SARSA, Q-learning等方式）。

我们也可以将model与其他方式结合。一个典型的例子就是Dyna算法，agent学到model之后，会混合真实的和模拟的数据，在此基础上learn+plan。

另外，model也可以用在搜索中，辅助构建搜索树。在model-based search中，一般会用到simulation policy $\pi$，model负责simulation，确定节点的下一个状态，并估计某一状态的价值。这类算法在更新Q函数的时候可以用MC control或者SARSA，其对应的应用即为Monte Carlo Tree Search和TD Tree Search。MCTS在另一篇blog中已经有详细介绍了，这里就不展开了。

## Exploration & Exploitation



## Self-Play
