---
title: 从围棋到矩阵乘
intro: 本篇blog主要梳理AlphaGo系列的文章线索，总结Monte-Carlo Tree Search，AlphaGo，AlphaZero，MuZero的算法脉络，并描述由AlphaZero拓展出的三个应用。部分材料来自于UC Berkeley CS294-190第二讲课件以及deepmind博客。
author: Sapio-S
date: 2022-10-20
categories: [笔记, RL]
tags: [RL,笔记,技术]
math: true
---

## 算法发展

### Overview

本部分涉及到的主要算法发展流程。来自DeepMind。

<img title="" src="file:///D:/Desktop/文献/CS294-190/w2/pic/1.jpg" alt="1.jpg" width="534" data-align="center">

几种算法的表现如下。

### Monte-Carlo Tree Search (MCTS)

在《人工智能导论》一课我们就学过这一算法，并且用它训练了一个玩四子棋的AI。让我们简单回顾一下这个算法。

<img src="file:///C:/Users/XYQ/AppData/Roaming/marktext/images/2022-10-18-17-16-09-image.png" title="" alt="" data-align="center">

算法主要分为四个部分：通过一定的方式选择一个可扩展节点$n_0$；执行行动，拓展$n_0$得到$n_1$；使用蒙特卡洛的方式（即随机落子）模拟$n_1$状态的棋局，得到$n_1$对应的奖励；回溯更新各祖先节点的价值。多次重复上述过程后，AI选择胜率（或者其他指标）最大的节点落子即可。

上述算法中，最重要的是如何**选择**节点扩展，以及如何**更新**节点的价值。使用UCB (Upper Confidence Bound)算法的UCT (Upper Confidence Bounds for Trees)即是目前最常见的MCTS变体。在UCT中，根据$\argmax_{v'\in children}(\frac{Q(v')}{N(v')}+c\sqrt{\frac{2\ln(N(v))}{N(v')}})$选择要拓展的子节点；更新时，从子节点向父节点回溯，$N(v)\leftarrow N(v)+1$， $Q(v)\leftarrow Q(v)+\Delta$，其中$\Delta\leftarrow 1-\Delta$。简单来说，$N(v)$代表节点被访问的次数，$Q(v)$表示己方胜利的次数（这就是为什么要用交替变换的$\Delta$更新父节点），选择子节点扩展时，既要考虑到这些节点被访问的次数，尽量访问未被充分探索的子节点，同时也要考虑到子节点价值的高低，尽量寻找价值高的子节点。UCB的推导原理可以用regret、多臂老虎机模型等方式解释，此处不多做介绍。

### AlphaGo

#### RL训练

在前期训练阶段，AlphaGo的主要流程如下。<img title="" src="file:///C:/Users/XYQ/AppData/Roaming/marktext/images/2022-10-18-17-46-22-image.png" alt="" width="595" data-align="center">

1. 从人类专家数据集(30 million!)中，使用supervised learning学出两种策略（即，给定棋局，选择下一步落子的位置）：一个是简单的fast rollout policy $\pi_\delta$，一个是复杂的SL policy $\pi_\sigma$。在SL时，输入除了棋局之外，还有围棋相关的指标(liberties, ladder status等)。rollout policy $\pi_\rho$的准确度低于SL policy $\pi_\sigma$，但是速度非常快。

2. $\pi_\sigma$通过self-play（即让模型自己同自己过去的任一历史策略对弈）和policy gradient，进化为一个更好的策略，即RL policy $\pi_\rho$。

3. 用$\pi_\rho$self-play，可以得到多组棋局及其对应的胜率数据(30 million!)。用神经网络$V_\theta$拟合这些数据，即可快速评估某一棋局的价值。针对统一棋局，用$V_\theta$直接预测的结果，与执行100次$\pi_\rho$的结果基本相当，但是$V_\theta$更快。

其中，后两步合在一起可以视作一次approximate policy iteration（即一次policy improvement与一次policy evaluation）的RL。

#### MCTS对弈

在对弈阶段，AlphaGo的使用一种改进版的MCTS选择落子策略。

1. selection: 在选择节点扩展时，选择的标准改为$\argmax_{a}(Q(s,a)+u(s,a))$，其中$u(s,a) \propto \frac{\pi_\sigma(a|s)}{1+N(s,a)}$，$\pi_\sigma(a|s)$即为$\pi_\sigma$给出的先验概率。

2. expansion & simulation: 计算子节点的价值时，结合了两种方式：使用$V_\theta$直接估算价值，以及使用$\pi_\delta$运行一次rollout。子节点的价值为$V(s)=(1-\lambda)V_\theta(s)+\lambda z$。

3. backpropagation: 更新每个祖先节点，重新计算子节点的价值均值即可。

4. 落子时，选择访问次数最高的行为。

### AlphaZero

AlphaZero不需要围棋相关的知识与评判标准作为状态输入，也不需要专家数据，因此可以迁移至象棋与Shogi等其他棋盘游戏上。

#### RL训练

policy网络和value网络使用了同一个残差网络，只是加了不同的head（即额外几层简单网络），使得网络可以同时给出策略和价值。

AlphaZero没有专家数据，使用iterative self-play training scheme直接从0学起，进行了多次approximate policy iteration。

#### MCTS对弈

在MCTS时，AlphaZero只使用$V_\theta$判断棋局的价值，不再使用fast rollout policy$\pi_\delta$进行rollout。

### MuZero

不用是棋局，是啥都行！

#### RL训练

#### MCTS对弈

### 问题与思考

#### 计算开销

AlphaGo用了1202 CPUs和176 GPUs执行MCTS；AlphaZero在RL阶段用5000 TPUs训了14天。

#### self-play

- 好处：self-play构成了一组natural curriculum，每次都可以和自己水平相当的对手（即自己近期的历史模型）切磋。

- 坏处：策略未必收敛到纳什均衡，存在很多漏洞。固定AlphaZero的策略，可以用标准RL训练出一个能（且几乎只能）打败AlphaZero的AI。

## AlphaGo应用

### AlphaTensor

用AlphaZero发现了矩阵乘的快速计算方式，优化了矩阵乘法的计算速度。Nature 2022.10的封面文章。

#### 背景

以2x2的矩阵乘为例，它的计算可以由以下方式表示。例如，用Strass's方式计算C=AB的矩阵乘，可以用图b的方式算得。将图b的计算方式转换为向量表示，则可以得到c。

图a中的三位tensor记为$\Tau_n$，对于一个特定的维度n是确定的；c中的三个矩阵分别记为$U$, $V$, $W$。寻找矩阵乘的算法，即寻找一组$U,V,W$，使得$\Tau_n=\Sigma_{r=1}^R u^{(r)}\otimes v^{(r)}\otimes w^{(r)}$，其中$R$大于等于张量的rank，代表着计算矩阵乘法时运算的次数。

![](C:\Users\XYQ\AppData\Roaming\marktext\images\2022-10-18-19-08-41-image.png)

#### 问题建模

基于上述原理，可以将矩阵乘的过程抽象为一个TensorGame。

- 初始“棋局”：$\Tau_n$。

- 终止状态：$\Tau_t = 0$。

- “下棋”action：选择一组$(u^{(t)},v^{(t)},w^{(t)})$，其中每个元素只能在人为设定的集合$F=\{-2,-1,0,1,2\}$中选择。

- 状态转移：$\Tau_t \leftarrow \Tau_{t-1} -u^{(t)}\otimes v^{(t)}\otimes w^{(t)}$。

- 奖励：每个时间步给-1的惩罚；规定时间内没有完成棋局，则根据最后状态的rank计算惩罚。

#### AlphaZero的应用

AlphaTensor基于AlphaZero进行了一些修改：

- MCST扩展节点时，因为子节点的可能性过大，因此不枚举全部子节点比较其UCB（或者UCB相关的值），而是**抽取**部分子节点比较后扩展。

- RL过程中，policy和value网络的底层框架改为Transformer式结构。价值函数$V_\theta$ 给出的结果是对步数的预期（对张量的rank的预期）。

- RL训练的数据包括两部分：AI玩过的游戏回放，以及提前合成好的数据（随机生成$(u^{(t)},v^{(t)},w^{(t)})$，得到$\Tau=\Sigma_{r=1}^R u^{(r)}\otimes v^{(r)}\otimes w^{(r)}$，不管$\Tau$是不是目标张量）。

一些任务相关的trick：

- 数据增强：同一场棋局中，action的出现次序并不影响结果，所以可以交换action的次序，构建更多数据。

- 对张量进行三次cyclic transposition卷积，因为矩阵交换行或列后rank不变。

#### 结果

- 发现了更多种矩阵乘组合方式，其中部分方式的乘法运算次数比已知的任何算法都少（意味着在硬件上实现时可以更快，因为乘法器运算的时间远大于加法器）。

- 发现了更高维度的矩阵乘法分解方式。

- 针对特定的硬件，在reward中引入该硬件执行矩阵乘的时间作为惩罚，可以得到独特的矩阵乘法方式。

### chemical syntheses

发表在nature上。使用AlphaGo的变体寻找分子合成的方式。

#### 问题建模

图a为逆向合成的反应思路，图b为对应的搜索树。terminal state中的分子都是工业上可用的材料。每一个状态下，可能action分为两种：在rollout阶段（即通过MC随机采样/rollout policy的方式估算子节点价值）使用的是小范围的rules，在MCTS节点扩展阶段使用的是大范围的rules（包括出现频率较少的rules，只关注反应中心）。

![](C:\Users\XYQ\AppData\Roaming\marktext\images\2022-10-18-21-50-15-image.png)

#### AlphaGo应用

基于AlphaGo的实现。从专家数据（这里是已知的化学反应原理）中学习fast rollout policy与expansion policy；用value网络预测反应的可行度。

注意这里没有self-play的参与，几乎全程使用MCST。（AlphaTensor应该也没有）

### quantum physics

发表在nature子刊npjqi上。使用AlphaZero优化动态量子代价函数(dynamical quantum cost functionals)。

这篇文章结合了Quantum Optimal Control Theory (QOCT) 与AlphaZero，但是因为笔者对于相应领域实在是不太理解，文章里也没有介绍，所以就不详细展开了。

## 拓展阅读



## Quiz

- In AlphaGo, how is the RL policy network  $p_{\rho}$ trained and what role does it play in the final program?

- What is the RL method used in AlphaZero?

- Does the “emergent complexity/tool use” in the Bansal et al and Baker et al papers really require an adversarial setting?  If so, why? If not, explain how it could arise otherwise.

- Explain the primary difference between the ASP and PAIRED approaches to auto-curriculum generation.

## References

课件参考：[CS294-190 -- Fa21](https://sites.google.com/view/berkeley-cs294-190-fa21)

### Papers

### Blogs
