---
title: RL基础（精简重置版）
intro: from David Silver's course. 根据自己的理解删掉了部分证明和案例，旨在保留基本脉络与逻辑。
author: Sapio-S
date: 2022-10-20
categories: [笔记, RL]
tags: [RL,笔记,技术]
math: true
---


## Introduction

### Overview

RL并不要求一定有神经网络的参与，比如传统RL就可以视作一种纯粹的计算问题，至多是用了不同的近似方法去求解奖励，或者梯度式更新某一参数。但是随着问题越来越复杂，我们不得不开始考虑使用更强大的方式优化运算，于是出现了神经网络，出现了现在的Deep RL。

我按照自己的理解重新整理了David Silver在UCL讲授的RL内容。简单来说，前五讲解决的是小model的问题，后五讲解决的是大model的问题。对于小model来说，若model已知，那么就可以直接用规划的方式得到最优解；若model未知，那么我们就需要使用model-free的方式，从prediction和control两个角度切入求解。对于大model来说，我们已经不能遍历全部的动作/状态空间，因此只能近似求解RL问题中最重要的三个组件：value，policy和model。

### 基本概念

#### agent & environment

t时刻，个体(agent)观测到observation $O_t$，执行action $A_t$，收到环境(environment)的reward $R_{t+1}$。环境接收个体的$A_t$，给出$O_{t+1}$与$R_{t+1}$。

#### History & State

- History: $H_t=O_1,R_1,R_1,...,O_t,R_t,A_t$

- State: $S_t=f(H_t)$。决定将来的已知信息，与历史有关。

- Markov Property：A state $S_t$ is Markov if and only if $P[S_{t+1}|S_t]=P[S_{t+1}|S_1,S_2,...,S_t]$。历史信息不起作用，仅当前状态的信息足够推断将来。

- Fully Observable Environments：即Markov Decision Process（MDP）。个体状态与环境状态一致。

- Partially Observable Environments：个体状态 != 环境状态。个体需要考虑如何给出状态。

#### 个体的三个组成部分

- policy：$\pi$，从状态到行为的概率分布。给定状态s，$\pi(a|s)$为个体采取行为a的概率，即$\pi(a|s)=P[A_t=a|S_t=s]$。策略仅和当前的状态有关，与历史信息无关；某一确定的Policy是静态的，与时间无关，但是个体可以随着时间更新策略。

- Value Function：对未来将来的预测，评价当前**状态**（不是策略）的好坏。注意，价值函数基于特定的策略，不同策略下同一状态的价值可能不同。某一策略下的价值函数为$v_\pi(s)=E_\pi[R_{t+1}+\gamma R_{t+2}+\gamma ^2 R_{t+3}+...|S_t=s]$。

- model：个体对环境的建模，试图预测环境给出的状态和奖励。做出决策后，抵达下一个特定状态的概率为$P^a_{ss'} = P[S_{t+1}=s'|S_t=s,A_t=a]$，可能收获的奖励为$R^a_s=E[R_{t+1}|S_t=s,A_t=a]$。

### Markov Decision Processes

MDP通常表示为 $M=<S,P,R,\gamma,A>$。其中各项的含义如下：

- 状态集S：所有可能状态的集合。

- 状态转移概率矩阵P：表示所有状态的转移概率。$P^a_{ss'} = P[S_{t+1}=s'|S_t=s,A_t=a]$

- 奖励函数$R$：给定状态(s)，时间(t)，下一时刻(t+1)可以获得的奖励期望，$R_s=E[R_{t+1}|S_t=s]$。

- 衰减系数 Discount Factor $\gamma$：在0到1之间。值越大，意味着agent越倾向于考虑长期的利益；值越小，agent越倾向于考虑短期的利益。

- 收获/收益/回报 return $G_t$：在Markov Reward Process上，从t时刻开始往后所有的奖励的总和，$G_t=R_{t+1}+\gamma R_{t+2}+\gamma ^2 R_{t+3}+...$。

- value function：给定状态(s)，时间(t)，从该状态开始return的期望，即$v(s)=E[G_t|S_t=s]$。给出某一状态的**长期价值**。

对于个体而言，我们可以定义策略policy和两种价值函数value function：

- policy：在特定状态下，agent采取各行为的概率。一般用$\pi(a|s)$表示。

- 状态价值函数V：给定状态$s$与policy $\pi$，此时的价值函数为**状态价值函数**，记为$v_\pi(s)$，表示从状态$s$开始，遵循当前策略可获得的return的期望。$v_\pi(s)=E_\pi[G_t|S_t=s]$。

- 行为价值函数Q：给定状态$s$与policy $\pi$，执行具体行为$a$时可以得到的return的期望，一般使用状态行为对进行描述，$q_\pi(s,a)=E_\pi[G_t|S_t=s,A_t=a]$。

### Bellman Equation

#### Bellman Expectation Equation

在上述定义的基础上，可以得到MDP的两种价值函数方程，基本与MRP的价值函数方程相同，仅下标不同。
$$
v_\pi(s)=E_\pi[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s]\\
q_\pi(s,a)=E_\pi[R_{t+1}+\gamma q_\pi(S_{t+1},A_{t+1})|S_t=s,A_t=a]
$$

简单推导，就可以得到Bellman Expectation Equation。这个方程可以用来求解$v_\pi(s)$，$q_\pi(s,a)$。

$$
v_\pi(s)=\sum_{a\in A}{\pi(a|s){(R^a_s+\gamma \sum_{s'\in S}{P^a_{ss'}v_\pi(s')})}} \\
q_\pi(s,a)=R^a_s+\gamma \sum_{s'\in S}{P^a_{ss'} \sum_{a'\in A}{\pi(a'|s')q_\pi(s',a')} }
$$

简单来说，Bellman Expectation Equation可以记作$v=R+\gamma Pv$。

#### Bellman Optimality Equation

##### 最优价值函数

- 最优状态价值函数：$v_*(s)=max_\pi v_\pi(s)$，给定状态s，在所有可能的策略中，选择最大的状态价值函数。（reminder：一个策略对应一种行为的概率分布。）

- 最优行为价值函数：$q_*(s,a)=max_\pi q_\pi(s,a)$，给定状态s和行为a，在所有可能的策略中，选择最大的行为价值函数。

- 根据定义和式(1)(2)，我们可以建立起$v_*(s)$与$q_*(s,a)$的关系。
  
  $$
  v_*(s)=max_a q_*(s,a) \\
q_*(s,a)=R^a_s+\gamma \sum_{s'\in S}{P^a_{ss'} v_*(s')}
  $$

##### Bellman Optimality Equation

- 迭代上面出现的两个式子，即可得到这组方程。它有两种表述方式。这组方程可以用来求解$v_*(s)$，$q_*(s,a)$，进一步地，可以计算出最优策略。
  
  $$
  v_*(s)=max_a (R^a_s+\gamma \sum_{s'\in S}{P^a_{ss'}v_*(s')}) \\
q_*(s,a)=R^a_s+\gamma \sum_{s'\in S}{P^a_{ss'} max_{a'} q_*(s',a')}
  $$

- 此方程是非线性的，没有快速便捷的解决方案，一般通过迭代解决。

### 小结

之后的全部内容基本都是在讲如何求解Bellman Equation。只要解出了Bellman Equation，我们就可以认为得到了最优策略。

- policy $\pi$ 优于policy $\pi'$ ：$\pi \geq \pi'$，若$v_\pi(s) \geq v_{\pi'}(s), \forall s$
  
  > 定理：对于任何MDP，下面几点成立：
  > 
  > 1. 存在一个最优策略；
  > 2. 所有的最优策略有相同的最优价值函数；
  > 3. 所有的最优策略具有相同的行为价值函数。

- 通过最大化$q_*(s,a)$找最优策略。仅当$a=argmax_{a\in A}{q_*(s,a)}$（即，当前行为a是最优的行为）时，$\pi_*{(a|s)}=1$，其他情况下，$\pi_*{(a|s)}=0$。

- 对于任何MDP问题，总存在一个确定性的最优策略；如果我们知道最优行为价值函数，那么我们就找到了最优策略。当我们知道了最优价值函数，也就知道了每个状态的最优价值，这时便认为这个MDP获得了解决。

## 小model：穷举

### 已知model：planning

用动态规划的思路对已知的MDP求解，主要有两种方式：1）policy evaluation+policy iteration；2）value iteration。

#### policy evaluation

本质上来说，每一个状态的价值是下一个状态的价值的期望: $V(S_t)\leftarrow E_\pi [R_{t+1}+\gamma V(S_{t+1})]$。用$v$表示价值矩阵，可以得到Bellman Expectation Equation $v=R+\gamma Pv$。

现在我们要迭代求解Bellman Expectation Equation（当然也可以直接矩阵求逆，但是维数大的时候计算量太大了）。这里循环迭代$v^{k+1}=R+\gamma Pv^k$即可求得policy \pi 的value，注意，这里每次迭代都会更新全部状态的价值。可以证明这样的迭代最后一定会收敛得到真正的$v$。

#### policy improvement

根据policy evaluation的结果，贪心求解即可。每次选择收益最大的行为$\pi'(s)=\argmax _a q_\pi (s,a)$即为最优策略。一般来讲，我们先进行policy evaluation再进行policy improvement，二者交替进行。

#### value iteration

##### 思路1：Principle of Optimality

将v*(s)逐步拆解，可以从末态出发逐步计算。状态s下的最优策略可以被分解为两部分：从状态s到下一个状态s’采取了最优行为；之后，在状态s’时，遵循s’状态下的最优策略。即，$v_*(s) \leftarrow max_a (R^a_s+\gamma \sum_{s'\in S}{P^a_{ss'}v_*(s')})$。

##### 思路2：迭代求解

对Bellman Optimality Equation求解。循环迭代$v^{k+1}=\max _a (R^a+\gamma P^a v^k)$。可以证明最终一定会收敛到$v*$。

#### DP优化(asynchronous DP)

每次更新只对部分state有效（这些state提前backup），可以降低computation。

in-place dynamic programming: 真·动态规划

prioritised sweeping：对Bellman误差最大的state backup

real-time DP：只backup S_t

sample backup：随机抽样一些state

#### 收敛性证明

构造operator，使用Contraction mapping定理即可证明。

### 未知model：model-free

从这里起我们引入一个新的概念：**episode**。从某一个状态开始，Agent与Environment交互直到进入**终止状态**。完整的Episode对起始状态不做要求，但是要求个体必须达到某一个终止状态。

提示：model指MDP的transition probability和reward。

#### prediction

当model未知时，估算value function，对应于planning中的第一步，policy evalution。

##### 迭代求解框架

我们仍然想使用迭代的方式求出value function，但是现在$v^{k+1}=R+\gamma Pv^k$式中的$R$，$P$未知，需要换个思路：用“牛顿下山法”等数值分析技巧直接迭代求解。

最终，我们可以得到这样的迭代框架：$V(S_t)\leftarrow V(S_t)+\alpha ( X -V(S_t))$。其中，$X$代表着当前状态价值的真值（或估计的真值），有以下几种常见的形式。

##### MC

Monte Carlo方式中，我们认为value的估计值可以用样本中的return均值替代。此时，$X=G_t$，即有$V(S_t)\leftarrow V(S_t)+\alpha ( G_t -V(S_t))$。

由于必须等到episode结束才能计算出$G_t$，所以MC方式必须在完整episode结束之后再进行更新。

##### TD(0)

全称为Temporal-Difference。每次更新时仅考虑下一个状态的reward，因此可以使用不完整的episode即时更新，更好地体现了Markov property（即无后效性）。

此时的$X$称作TD target, $X=R_{t+1}+\gamma V(S_{t+1})$，更新公式为$V(S_t)\leftarrow V(S_t)+\alpha ( R_{t+1}+\gamma V(S_{t+1}) -V(S_t))$。其中$\delta_t = R_{t+1}+\gamma V(S_{t+1}) -V(S_t)$称为TD error。

由于迭代时使用的$V(S_{t+1})$并不是准确的值（也是一个估计值），所以进行TD运算时，对$X$的估计是有偏的。如果我们能在迭代时得到准确真值，即$X=R_{t+1}+\gamma V_\pi(S_{t+1})$，那就可以得到无偏估计。

##### TD(λ): Forward-View

在TD(0)中，我们每次仅考虑了下一个状态的reward。我们当然可以综合更多步的价值再更新！

从当前时刻起，第n步的reward可以表示为$G^{(n)}_t=R_{t+1}+\gamma R_{t+2}+...+\gamma ^n V(S_{t+n})$。加权结合n步内所有的reward，给n-步收获增加一个权重$(1-\lambda)\lambda^{n-1}$，我们就得到了$G^\lambda_t=(1-\lambda) \sum \lambda^{n-1}G^{(n)}_t$。

令$X=G^\lambda_t$，更新公式即为$V(S_t)\leftarrow V(S_t)+\alpha (G^\lambda_t -V(S_t))$。

显然，标准的TD(λ)需要完整的episode才可以计算出$X$并更新价值。

##### Backward-View TD(λ)

另一种思路是从后向前迭代更新，当第n步的reward出现之后，更新这一步之前的全部状态的价值。这样一来，更新方式是online的，但是结果与forward-view一样。

我们仍然使用TD error的定义$\delta_t = R_{t+1}+\gamma V(S_{t+1}) -V(S_t)$，每次更新的时候，同时更新前面出现过的全部state。这样一来，迭代公式为$v(s) \leftarrow v(s)+\alpha \delta_t E_t(s)$，其中$E_t(s)$ eligibility trace定义为$E_0(s)=0, E_t(s)=\gamma \lambda E_{t-1}(s)+1(s_t=s)$。注意这里的更新公式是矩阵形式的。

##### 对比

【TBC】

#### control

这部分需要解决的问题是：当model未知时，如何选择最优策略。

类似于planning，我们可以使用policy iteration（交替进行policy evaluation与policy improvement），或者直接进行value iteration。但是，与planning不同的是：

- 使用$Q(s,a)$而不是$v(s)$进行policy evaluation或者value iteration。这样我们可以直接通过$\pi(s)=\argmax_a Q(s,a)$得到最优策略，跳过model中$P$的参与。

- 进行policy improvement的时候，对greedy贪心算法进行轻微的调整，改为使用$\epsilon$-greedy选择动作。即，以$1-\epsilon$的概率选择当前认为最好的行为，以$\epsilon$的概率随机选择一个行为。这主要是为了保证算法可以充分地探索环境。

这里引入了两个新的概念，on-policy和off-policy。简单而言，on-policy用策略$\pi$产生的行为学习、优化$\pi$，off-policy用别的策略$\mu$产生的行为学习、优化$\pi$，类似于“站在别人的肩膀上可以看得更远”。

##### on-policy policy iteration

下面简单说明如何得到$Q(s,a)$。

与prediction一样，我们可以得到迭代更新的框架$Q(S_t, A_t)\leftarrow Q(S_t, A_t)+\alpha ( X -Q(S_t,A_t))$。其中，$X$有几种常见的表示形式：

- MC: $X=G_t$.

- TD(0), or SARSA: $X=R+\gamma Q(S',A')$. 这个方法称为SARSA的原因是更新公式中含有SARSA: $Q(S,A)\leftarrow Q(S,A)+\alpha (R+\gamma Q(S',A')-Q(S,A))$.

- Forward-View TD(λ), or SARSA(λ): $X=q_t^\lambda$.
  
  - 第n步的Q-return: $q_t^{(n)}=R_{t+1}+...+\gamma^{n-1}R_{t+n}+\gamma^nQ(S_{t+n},A_{t+n})$.
  
  - $q_t^\lambda=(1-\lambda)\Sigma_{n=1}^\inf \lambda^{n-1}q_t^{(n)}$, $q_t^{(n)}$.

- Backward-View SARSA(λ): $Q(s,a) \leftarrow Q(s,a)+\alpha \delta_t E_t(s,a)$.
  
  - $\delta_t=R_{t+1}+\gamma Q(S_{t+1}, A_{t+1})-Q(S_t,A_t)$.
  
  - $E_0(s,a)=0, E_t(s,a)= \gamma \lambda E_{t-1}(s,a)+1(S_t=s,A_t=a)$.

##### off-policy policy iteration

我们希望用$\mu$的行为优化$\pi$，但是二者的策略分布并不相同，因此需要先通过**importance sampling**转换不同分布下的期望：$E_{X~P}[f(x)]=E_{X~Q}[\frac{P(X)}{Q(X)}f(x)]$。之后，我们就可以直接将on-policy下推导的结果挪用过来继续使用了。

- MC: $X=G_t^{\pi / \mu}=frac{\pi(A_{t}|S_{t})}{\mu(A_{t}|S_{t})} frac{\pi(A_{t+1}|S_{t+1})}{\mu(A_{t+1}|S_{t+1})} ... frac{\pi(A_{T}|S_{T})}{\mu(A_{T}|S_{T})} G_t$.

- TD(0): $X=frac{\pi(A_{t}|S_{t})}{\mu(A_{t}|S_{t})}(R_{t+1}+\gamma Q(S',A'))$.

##### Q-learning: value iteration

$Q(S,A)\leftarrow Q(S,A)+\alpha (R+\gamma \max_{a'}Q(S',a')-Q(S,A))$.

可以证明收敛到$q*(s,a)$。

##### 对比

【TBC】

#### 总结

【TBC】

## 大model：近似

当状态空间/动作空间非常大或者连续的时候，我们没有办法遍历所有的state/action，只能通过estimate近似求解。前面讲到的针对小model的RL方法都可以看作是通过lookup table实现的，即，对于每一个state或者action，我们都记录了一个对应的$V(s)$或者$Q(s,a)$；现在我们使用各种函数逼近function approximation（比如linear combination，神经网络，决策树等）替代之前的lookup table，预测没有见过的状态的价值。

### value-based: Value Function Approximation

我们希望agent可以从有限的经验中学习value function，预测没有见过的状态的价值。

普遍使用$w$表示价值函数的参数。

#### Gradient Descent

##### Gradient Descent快速回顾

假设我们的目标函数是$J(w)$，其梯度是$\nabla_wJ(w)$。为了求$J(w)$的（局部）最小值，我们只需要沿着这个方向更新$J(w)$的值：$\Delta w=-frac{1}{2}\alpha \nabla_wJ(w)$。

##### Prediction: 逼近V

目标是$\hat v(s,w) \approx v_\pi(s)$。此时可知：

- 目标函数：$J(w)=E_\pi[(v_\pi(S)-\hat v(S,w))^2]$，即均方差MSE的期望。

- 梯度：$\Delta w=\alpha(v_\pi(S)-\hat v(S,w)) \nabla_w \hat v(S,w)$。这里使用单样本对gradient进行了采样，所以不用求期望。

使用一组线性函数(Linear Function Approximation)近似，即，令$\hat v(S,w)=x(S)^Tw=\sum x_j(S)w_j$。这种情况下，$\nabla_w \hat v(S,w)=x(S)$，因此梯度可表示为$\Delta w=\alpha(v_\pi(S)-\hat v(S,w)) x(S)$。但是，在训练时我们得不到价值函数的真值$v_\pi (S)$，需要近似表示：

- MC: $G_t$。此时梯度为$\Delta w=\alpha(G_t-\hat v(S_t,w)) x(S_t)$。收敛至局部最优。

- TD(0): $R_{t+1}+\gamma \hat v(S_{t+1},w)$。收敛至全局最优。

- TD(λ): $G^\lambda _t$。收敛至全局最优。

##### Control: 逼近Q

在policy evaluation时预估Q，之后使用$\epsilon$-greedy完成policy improvement。

拟合的思路与上一小节基本完全一致。目标是$\hat v(s,w) \approx v_\pi(s)$，当使用Linear Function近似时，令$\hat v(S,w)=x(S)^Tw=\sum x_j(S)w_j$，有：

- MC: $G_t$。此时梯度为$\Delta w=\alpha(G_t-\hat v(S_t,w)) x(S_t)$。

- TD(0): $R_{t+1}+\gamma \hat v(S_{t+1},w)$。此时梯度为$\Delta w=\alpha \delta x(S)$。

- TD(λ): $G^\lambda _t$。此时梯度为$\Delta w=\alpha \delta_t E_t$。

#### batch RL

Gradient Descent使用的是流式的数据，更新参数后sample即废弃。更加sample efficient的方式是收集一段时间内的数据，直接在这个数据集上拟合。这时我们的优化目标转换为求Least Squares，即求$LS(w)=\sum (v_t^\pi-\hat v(s_t,w))^2$的最小值。令其导数为0，可以直接求出结果$w=(\sum x(s_t)x(s_t)^T)^{-1}\sum x(s_t)v_t^\pi$。同样的，这里的$v_t^\pi$在实际求解时未知，需要通过MC/TD等方式近似。

解决control问题时，我们使用least squares Q-learning进行policy evaluation，用greedy policy进行policy improvement。拟合Q时我们需要将上述的v换为Q。注意因为数据集包含多个历史policy的experience，所以必须off-policy式更新：

- 从旧policy的experience中取得sample$S_t, A_t, R_{t+1}, S_{t+1}$。

- 用新policy选择action$A'=\pi_{new}(S_{t+1})$。

- 更新：$\hat q(S_t,A_t,w) \leftarrow R_{t+1}+\gamma \hat q(S_{t+1},A',w)$。

【和Q-learning的关系？】

### policy-based: Policy Gradient

我们希望agent可以从有限的经验中学习policy。

直接学习policy的好处在于：更好的收敛性；可以有效用于高维/连续空间；可以学到随机策略（比如石头剪刀布场景下，随机策略才是最优解）。坏处在于：通常只能达到局部最优；很难直接评估策略。

普遍使用$\theta$表示policy的参数。

#### 优化目标

我们可以用不同的方式衡量策略$\pi_\theta$的好坏，比如：

- start value: $J_1(\theta)=V^{\pi_theta}(s_1)$.

- average value: $J_{avV}(\theta)=\sum_{s} d^{\pi_theta}(s) V^{\pi_theta}(s)$.

- average reward per time-step: $J_{avV}(\theta)=\sum_{s} d^{\pi_theta}(s) \sum_a \pi_theta(s,a)R^a_s$.

我们希望最大化$J(\theta)$。有很多可选的方式，这里我们使用Gradient Descent，此时$\Delta \theta=\alpha \nabla_\theta J(\theta)$。

#### policy gradient

##### Finite Differences

使用导数/微分的定义。对于$\theta$的每一维k，计算$J(\theta)$的偏导，从而得到近似的梯度：$\frac {\partial J(\theta)}{\partial \theta_k} \approx \frac {J(\theta+\epsilon u_k)-J(\theta)}{\epsilon}$。

##### score function

根据公式$d \ln(y)=\frac {dy}y$，可以得到$\nabla _\theta \pi_\theta(s,a)=\pi_\theta(s,a) \nabla_\theta \ln \pi_\theta(s,a)$。我们定义score function为$\nabla _\theta log\pi_\theta(s,a)$。score function其实就是gradient的估计器

score function是可以根据policy的类别直接计算的，比如：

- softmax policy：针对离散行为，将行为表示为$\phi(s,a)^T \theta$，则采取行为的概率为$\pi_\theta(s,a) \propto e^{\phi(s,a)^T\theta}$，此时score function为$\nabla _\theta \log \pi_\theta(s,a)=\phi(s,a)-E_{\pi\theta}[\phi(s,·)]$。

- Gauss policy：针对连续行为，将行为表示为$\phi(s)^T \theta$，采取行为的概率为$a \sim \mathcal{N}(\mu(s),\sigma^2)$，score function为$\nabla _\theta \log \pi_\theta(s,a)=\frac{(a-\mu(s))\phi(s)}{\sigma^2}$。

##### policy gradient theorem

For any differentiable policy $\pi_\theta(s,a)$, for any of the policy objective functions $J = J_1, J_{avR},\frac1{1-\gamma}J_{avV}$, the policy gradient is $\nabla_\theta J(\theta)=E_{\pi\theta}[\nabla_\theta \log \pi_\theta(s,a)Q^{\pi_\theta}(s,a)]$.

和之前一样，我们遇到了这个问题：$Q^{\pi_\theta}(s,a)]$未知，需要近似。想必大家都熟悉这个近似方式了。比如，用MC近似，则 可以视$v_t$为无偏样本，此时更新方式为$\theta \leftarrow \theta + \alpha \nabla_\theta \log  \pi_\theta(s_t,a_t)v_t$。

#### value + policy: Actor-Critic

这一类算法需要同时预估policy和value，维护两组参数。其中，actor为policy，critic为value，用$Q_w(s,a)$估算$Q^{\pi_\theta}(s,a)$。此时，policy gradient的梯度可以写作$\nabla_\theta J(\theta)=E_{\pi\theta}[\nabla_\theta \log \pi_\theta(s,a)Q_w(s,a)]$。这样一来，我们可以降低policy gradient的variance。

critic的任务就是之前的policy evaluation。因此，我们可以用之前提过的任何方式更新critic和actor，把比如用TD(0)更新critic，用policy gradient更新actor。

##### advantage function

在不改变期望的情况下，我们可以引入baseline来降低算法的方差。常用的baseline就是state value function$B(s)=V^{\pi_\theta}(s)$。

我们定义advantage function为$A^{\pi_\theta}(s,a)=Q^{\pi_\theta}(s,a)-V^{\pi_\theta}(s)$，此时梯度变为$\nabla_\theta J(\theta)=E_{\pi\theta}[\nabla_\theta \log \pi_\theta(s,a)A^{\pi_\theta}(s,a)]$。

当然，$A^{\pi_\theta}(s,a)$的值我们得不到，所以同样使用近似：$A(s,a)=Q_w(s,a)-V_v(s)$。两个近似量可以用之前的任意方式求解。注意，由于$Q(s,a)=r+\gamma V(s)$，因此我们实际可以用一套参数更新，比如TD error是$\delta_v=r+\gamma V_v(s')-V_v(s)$。

#### 总结

除了MC是policy-based之外，其余的算法都是Actor-Critic结构。

【TBC】

### model-based

agent从经验中学习model，自行预测环境可能给出的reward和状态转移方式，进而以此构建value function/policy。

具体来说，我们使用$\eta$表示model的参数，要预测的model即为$<P_\eta,R_\eta>$，从$(s,a)\rightarrow r$数据集中学习$R_\eta$是regression问题，从$(s,a)\rightarrow s'$数据集中学习$P_\eta$是density estimation问题，使用MSE，KL divergence等loss求解即可。model可以有table lookup（记录全部(s,a)对），Gaussian Process等多种形式。

有了这个model之后，我们就可以得到下一步的状态分布$S_{t+1}~P_\eta(S_{t+1}|S_t,A_t)$以及奖励分布$R_{t+1}~R_\eta(R_{t+1}|S_t,A_t)$。之后，我们根据这个model求解MDP。

一个典型的求解思路是sample-based planning。这意味着学习到的model只负责产生samples，得到samples之后使用model-free的方式求解MDP（可以选用MC control, SARSA, Q-learning等方式）。

我们也可以将model与其他方式结合。一个典型的例子就是Dyna算法，agent学到model之后，会混合真实的和模拟的数据，在此基础上learn+plan。

另外，model也可以用在搜索中，辅助构建搜索树。在model-based search中，一般会用到simulation policy $\pi$，model负责simulation，确定节点的下一个状态，并估计某一状态的价值。这类算法在更新Q函数的时候可以用MC control或者SARSA，其对应的应用即为Monte Carlo Tree Search和TD Tree Search。MCTS在另一篇blog中已经有详细介绍了，这里就不展开了。

## Relevant Topics

### Exploration & Exploitation

### Self-Play
