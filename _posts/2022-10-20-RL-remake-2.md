---
title: RL基础（精简版）-part 2：穷举小模型
intro: from David Silver's course. 根据自己的理解删掉了部分证明和案例，旨在保留基本脉络与逻辑。
author: Sapio-S
date: 2022-10-20
categories: [笔记, RL, RL基础]
tags: [RL,笔记,技术]
math: true
---



## 已知model：planning

用动态规划的思路对已知的MDP求解，主要有两种方式：1）policy evaluation+policy iteration；2）value iteration。

### policy evaluation

本质上来说，每一个状态的价值是下一个状态的价值的期望: $V(S_t)\leftarrow E_\pi [R_{t+1}+\gamma V(S_{t+1})]$。用$v$表示价值矩阵，可以得到Bellman Expectation Equation $v=R+\gamma Pv$。

现在我们要迭代求解Bellman Expectation Equation（当然也可以直接矩阵求逆，但是维数大的时候计算量太大了）。这里循环迭代$v^{k+1}=R+\gamma Pv^k$即可求得policy \pi 的value，注意，这里每次迭代都会更新全部状态的价值。可以证明这样的迭代最后一定会收敛得到真正的$v$。

### policy improvement

根据policy evaluation的结果，贪心求解即可。每次选择收益最大的行为$\pi'(s)=\argmax _a q_\pi (s,a)$即为最优策略。一般来讲，我们先进行policy evaluation再进行policy improvement，二者交替进行。

### value iteration

#### 思路1：Principle of Optimality

将v*(s)逐步拆解，可以从末态出发逐步计算。状态s下的最优策略可以被分解为两部分：从状态s到下一个状态s’采取了最优行为；之后，在状态s’时，遵循s’状态下的最优策略。即，$v_*(s) \leftarrow max_a (R^a_s+\gamma \sum_{s'\in S}{P^a_{ss'}v_*(s')})$。

#### 思路2：迭代求解

对Bellman Optimality Equation求解。循环迭代$v^{k+1}=\max _a (R^a+\gamma P^a v^k)$。可以证明最终一定会收敛到$v*$。

### DP优化(asynchronous DP)

每次更新只对部分state有效（这些state提前backup），可以降低computation。

in-place dynamic programming: 真·动态规划

prioritised sweeping：对Bellman误差最大的state backup

real-time DP：只backup S_t

sample backup：随机抽样一些state

### 收敛性证明

构造operator，使用Contraction mapping定理即可证明。

## 未知model：model-free

从这里起我们引入一个新的概念：**episode**。从某一个状态开始，Agent与Environment交互直到进入**终止状态**。完整的Episode对起始状态不做要求，但是要求个体必须达到某一个终止状态。

提示：model指MDP的transition probability和reward。

### prediction

当model未知时，估算value function，对应于planning中的第一步，policy evalution。

#### 迭代求解框架

我们仍然想使用迭代的方式求出value function，但是现在$v^{k+1}=R+\gamma Pv^k$式中的$R$，$P$未知，需要换个思路：用“牛顿下山法”等数值分析技巧直接迭代求解。

最终，我们可以得到这样的迭代框架：$V(S_t)\leftarrow V(S_t)+\alpha ( X -V(S_t))$。其中，$X$代表着当前状态价值的真值（或估计的真值），有以下几种常见的形式。

#### MC

Monte Carlo方式中，我们认为value的估计值可以用样本中的return均值替代。此时，$X=G_t$，即有$V(S_t)\leftarrow V(S_t)+\alpha ( G_t -V(S_t))$。

由于必须等到episode结束才能计算出$G_t$，所以MC方式必须在完整episode结束之后再进行更新。

#### TD(0)

全称为Temporal-Difference。每次更新时仅考虑下一个状态的reward，因此可以使用不完整的episode即时更新，更好地体现了Markov property（即无后效性）。

此时的$X$称作TD target, $X=R_{t+1}+\gamma V(S_{t+1})$，更新公式为$V(S_t)\leftarrow V(S_t)+\alpha ( R_{t+1}+\gamma V(S_{t+1}) -V(S_t))$。其中$\delta_t = R_{t+1}+\gamma V(S_{t+1}) -V(S_t)$称为TD error。

由于迭代时使用的$V(S_{t+1})$并不是准确的值（也是一个估计值），所以进行TD运算时，对$X$的估计是有偏的。如果我们能在迭代时得到准确真值，即$X=R_{t+1}+\gamma V_\pi(S_{t+1})$，那就可以得到无偏估计。

#### TD(λ): Forward-View

在TD(0)中，我们每次仅考虑了下一个状态的reward。我们当然可以综合更多步的价值再更新！

从当前时刻起，第n步的reward可以表示为$G^{(n)}_t=R_{t+1}+\gamma R_{t+2}+...+\gamma ^n V(S_{t+n})$。加权结合n步内所有的reward，给n-步收获增加一个权重$(1-\lambda)\lambda^{n-1}$，我们就得到了$G^\lambda_t=(1-\lambda) \sum \lambda^{n-1}G^{(n)}_t$。

令$X=G^\lambda_t$，更新公式即为$V(S_t)\leftarrow V(S_t)+\alpha (G^\lambda_t -V(S_t))$。

显然，标准的TD(λ)需要完整的episode才可以计算出$X$并更新价值。

#### Backward-View TD(λ)

另一种思路是从后向前迭代更新，当第n步的reward出现之后，更新这一步之前的全部状态的价值。这样一来，更新方式是online的，但是结果与forward-view一样。

我们仍然使用TD error的定义$\delta_t = R_{t+1}+\gamma V(S_{t+1}) -V(S_t)$，每次更新的时候，同时更新前面出现过的全部state。这样一来，迭代公式为$v(s) \leftarrow v(s)+\alpha \delta_t E_t(s)$，其中$E_t(s)$ eligibility trace定义为$E_0(s)=0, E_t(s)=\gamma \lambda E_{t-1}(s)+1(s_t=s)$。注意这里的更新公式是矩阵形式的。

#### 对比

【TBC】

### control

这部分需要解决的问题是：当model未知时，如何选择最优策略。

类似于planning，我们可以使用policy iteration（交替进行policy evaluation与policy improvement），或者直接进行value iteration。但是，与planning不同的是：

- 使用$Q(s,a)$而不是$v(s)$进行policy evaluation或者value iteration。这样我们可以直接通过$\pi(s)=\argmax_a Q(s,a)$得到最优策略，跳过model中$P$的参与。

- 进行policy improvement的时候，对greedy贪心算法进行轻微的调整，改为使用$\epsilon$-greedy选择动作。即，以$1-\epsilon$的概率选择当前认为最好的行为，以$\epsilon$的概率随机选择一个行为。这主要是为了保证算法可以充分地探索环境。

这里引入了两个新的概念，on-policy和off-policy。简单而言，on-policy用策略$\pi$产生的行为学习、优化$\pi$，off-policy用别的策略$\mu$产生的行为学习、优化$\pi$，类似于“站在别人的肩膀上可以看得更远”。

#### on-policy policy iteration

下面简单说明如何得到$Q(s,a)$。

与prediction一样，我们可以得到迭代更新的框架$Q(S_t, A_t)\leftarrow Q(S_t, A_t)+\alpha ( X -Q(S_t,A_t))$。其中，$X$有几种常见的表示形式：

- MC: $X=G_t$.

- TD(0), or SARSA: $X=R+\gamma Q(S',A')$. 这个方法称为SARSA的原因是更新公式中含有SARSA: $Q(S,A)\leftarrow Q(S,A)+\alpha (R+\gamma Q(S',A')-Q(S,A))$.

- Forward-View TD(λ), or SARSA(λ): $X=q_t^\lambda$.
  
  - 第n步的Q-return: $q_t^{(n)}=R_{t+1}+...+\gamma^{n-1}R_{t+n}+\gamma^nQ(S_{t+n},A_{t+n})$.
  
  - $q_t^\lambda=(1-\lambda)\Sigma_{n=1}^\inf \lambda^{n-1}q_t^{(n)}$, $q_t^{(n)}$.

- Backward-View SARSA(λ): $Q(s,a) \leftarrow Q(s,a)+\alpha \delta_t E_t(s,a)$.
  
  - $\delta_t=R_{t+1}+\gamma Q(S_{t+1}, A_{t+1})-Q(S_t,A_t)$.
  
  - $E_0(s,a)=0, E_t(s,a)= \gamma \lambda E_{t-1}(s,a)+1(S_t=s,A_t=a)$.

#### off-policy policy iteration

我们希望用$\mu$的行为优化$\pi$，但是二者的策略分布并不相同，因此需要先通过**importance sampling**转换不同分布下的期望：$E_{X~P}[f(x)]=E_{X~Q}[\frac{P(X)}{Q(X)}f(x)]$。之后，我们就可以直接将on-policy下推导的结果挪用过来继续使用了。

- MC: $X=G_t^{\pi / \mu}=frac{\pi(A_{t}|S_{t})}{\mu(A_{t}|S_{t})} frac{\pi(A_{t+1}|S_{t+1})}{\mu(A_{t+1}|S_{t+1})} ... frac{\pi(A_{T}|S_{T})}{\mu(A_{T}|S_{T})} G_t$.

- TD(0): $X=frac{\pi(A_{t}|S_{t})}{\mu(A_{t}|S_{t})}(R_{t+1}+\gamma Q(S',A'))$.

#### Q-learning: value iteration

$Q(S,A)\leftarrow Q(S,A)+\alpha (R+\gamma \max_{a'}Q(S',a')-Q(S,A))$.

可以证明收敛到$q*(s,a)$。

#### 对比

【TBC】

### 总结

【TBC】

