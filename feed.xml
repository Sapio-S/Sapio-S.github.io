<feed xmlns="http://www.w3.org/2005/Atom"> <id>https://sapio-s.github.io/</id><title>Sapio-S</title><subtitle>NaN. </subtitle> <updated>2022-10-20T04:36:02+08:00</updated> <author> <name>Sapio-S</name> <uri>https://sapio-s.github.io/</uri> </author><link rel="self" type="application/atom+xml" href="https://sapio-s.github.io/feed.xml"/><link rel="alternate" type="text/html" hreflang="en" href="https://sapio-s.github.io/"/> <generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator> <rights> © 2022 Sapio-S </rights> <icon>/assets/img/favicons/favicon.ico</icon> <logo>/assets/img/favicons/favicon-96x96.png</logo> <entry><title>RL基础（精简版）part 3：大模型近似</title><link href="https://sapio-s.github.io/posts/RL-remake-3/" rel="alternate" type="text/html" title="RL基础（精简版）part 3：大模型近似" /><published>2022-10-20T00:00:00+08:00</published> <updated>2022-10-20T04:31:36+08:00</updated> <id>https://sapio-s.github.io/posts/RL-remake-3/</id> <content src="https://sapio-s.github.io/posts/RL-remake-3/" /> <author> <name>Sapio-S</name> </author> <category term="科研" /> <category term="RL基础" /> <summary> Overview 当状态空间/动作空间非常大或者连续的时候，我们没有办法遍历所有的state/action，只能通过estimate近似求解。前面讲到的针对小model的RL方法都可以看作是通过lookup table实现的，即，对于每一个state或者action，我们都记录了一个对应的$V(s)$或者$Q(s,a)$；现在我们使用各种函数逼近function approximation（比如linear combination，神经网络，决策树等）替代之前的lookup table，预测没有见过的状态的价值。 value-based: Value Function Approximation 我们希望agent可以从有限的经验中学习value function，预测没有见过的状态的价值。 普遍使用$w$表示价值函数的参数。 Gradient Descent Gradi... </summary> </entry> <entry><title>RL基础（精简版）part 2：穷举小模型</title><link href="https://sapio-s.github.io/posts/RL-remake-2/" rel="alternate" type="text/html" title="RL基础（精简版）part 2：穷举小模型" /><published>2022-10-20T00:00:00+08:00</published> <updated>2022-10-20T04:31:36+08:00</updated> <id>https://sapio-s.github.io/posts/RL-remake-2/</id> <content src="https://sapio-s.github.io/posts/RL-remake-2/" /> <author> <name>Sapio-S</name> </author> <category term="科研" /> <category term="RL基础" /> <summary> 已知model：planning 用动态规划的思路对已知的MDP求解，主要有两种方式：1）policy evaluation+policy iteration；2）value iteration。 policy evaluation 本质上来说，每一个状态的价值是下一个状态的价值的期望: $V(S_t)\leftarrow E_\pi [R_{t+1}+\gamma V(S_{t+1})]$。用$v$表示价值矩阵，可以得到Bellman Expectation Equation $v=R+\gamma Pv$。 现在我们要迭代求解Bellman Expectation Equation（当然也可以直接矩阵求逆，但是维数大的时候计算量太大了）。这里循环迭代$v^{k+1}=R+\gamma Pv^k$即可求得policy \pi 的value，注意，这里每次迭代都会更新全部状态... </summary> </entry> <entry><title>RL基础（精简版）part 1：介绍</title><link href="https://sapio-s.github.io/posts/RL-remake-1/" rel="alternate" type="text/html" title="RL基础（精简版）part 1：介绍" /><published>2022-10-20T00:00:00+08:00</published> <updated>2022-10-20T04:31:36+08:00</updated> <id>https://sapio-s.github.io/posts/RL-remake-1/</id> <content src="https://sapio-s.github.io/posts/RL-remake-1/" /> <author> <name>Sapio-S</name> </author> <category term="科研" /> <category term="RL基础" /> <summary> Overview RL并不要求一定有神经网络的参与，比如传统RL就可以视作一种纯粹的计算问题，至多是用了不同的近似方法去求解奖励，或者梯度式更新某一参数。但是随着问题越来越复杂，我们不得不开始考虑使用更强大的方式优化运算，于是出现了神经网络，出现了现在的Deep RL。 我按照自己的理解重新整理了David Silver在UCL讲授的RL内容。简单来说，前五讲解决的是小model的问题，后五讲解决的是大model的问题。对于小model来说，若model已知，那么就可以直接用规划的方式得到最优解；若model未知，那么我们就需要使用model-free的方式，从prediction和control两个角度切入求解。对于大model来说，我们已经不能遍历全部的动作/状态空间，因此只能近似求解RL问题中最重要的三个组件：value，policy和model。 基本概念 agent... </summary> </entry> <entry><title>从围棋到矩阵乘</title><link href="https://sapio-s.github.io/posts/AlphaGo/" rel="alternate" type="text/html" title="从围棋到矩阵乘" /><published>2022-10-20T00:00:00+08:00</published> <updated>2022-10-20T04:31:36+08:00</updated> <id>https://sapio-s.github.io/posts/AlphaGo/</id> <content src="https://sapio-s.github.io/posts/AlphaGo/" /> <author> <name>Sapio-S</name> </author> <category term="科研" /> <category term="paper" /> <summary> 算法发展 Overview 本部分涉及到的主要算法发展流程。来自DeepMind。 几种算法的表现如下。 Monte-Carlo Tree Search (MCTS) 在《人工智能导论》一课我们就学过这一算法，并且用它训练了一个玩四子棋的AI。让我们简单回顾一下这个算法。 算法主要分为四个部分：通过一定的方式选择一个可扩展节点$n_0$；执行行动，拓展$n_0$得到$n_1$；使用蒙特卡洛的方式（即随机落子）模拟$n_1$状态的棋局，得到$n_1$对应的奖励；回溯更新各祖先节点的价值。多次重复上述过程后，AI选择胜率（或者其他指标）最大的节点落子即可。 上述算法中，最重要的是如何选择节点扩展，以及如何更新节点的价值。使用UCB (Upper Confidence Bound)算法的UCT (Upper Confidence Bounds for Trees)即是目前最常... </summary> </entry> <entry><title>MARL之sequential update</title><link href="https://sapio-s.github.io/posts/MARL-sequential-update/" rel="alternate" type="text/html" title="MARL之sequential update" /><published>2022-09-07T00:00:00+08:00</published> <updated>2022-09-07T00:00:00+08:00</updated> <id>https://sapio-s.github.io/posts/MARL-sequential-update/</id> <content src="https://sapio-s.github.io/posts/MARL-sequential-update/" /> <author> <name>Sapio-S</name> </author> <category term="科研" /> <category term="paper" /> <summary> 合作的MARL 问题描述 多机的POMDP（Partially Observable Markov Decision Process）可以用$&amp;lt;N, S, A, P, r, \gamma&amp;gt;$ 描述。其中，$N$是智能体个数，$S$是智能体的联合状态State，$A$是智能体的联合行为Action，$P$是状态转移函数Transition Probability function，$r$是多机共享的joint reward function，$\gamma$是折扣因子discount factor。 联合策略joint policy由各个agent的独立策略组成， 我们需要找到一个最优的policy从而maximize the expected total reward: 常见处理方式 同单机一样，多机RL也可以大致分为value-based和policy-ba... </summary> </entry> </feed>
